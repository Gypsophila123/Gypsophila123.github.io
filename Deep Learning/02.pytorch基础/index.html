<!DOCTYPE html>
<html lang="zh-CN">
    <head>
    <meta charset="utf-8">

    

    <!-- 渲染优化 -->
    <meta name="renderer" content="webkit">
    <meta name="force-rendering" content="webkit">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="HandheldFriendly" content="True" >
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <!--icon-->

    
        <link rel="shortcut icon" href="/github.ico">
    
    
        <link rel="icon" type="image/png" sizes="16x16" href="/github_small.png">
    
    
        <link rel="icon" type="image/png" sizes="32x32" href="/github_medium.png">
    
    
    


    <!-- meta -->


<title>02.pytorch基础 | My Learning Notes</title>





    <!-- OpenGraph -->
 
    <meta name="description" content="环境搭建推荐使用Anaconda安装（可选）进入Anaconda官网下载安装即可，安装完成后搜索 Anaconda Prompt 以打开命令行。 路径前括号包裹的名字即为当前环境名，如 (base) C:\Users\Administrator&gt;即为base环境，该环境在Anaconda根目录下，不可删除。如果创建新环境，则存放于根目录下的envs文件夹内。 开始： 123456789101">
<meta property="og:type" content="article">
<meta property="og:title" content="02.pytorch基础">
<meta property="og:url" content="http://example.com/Deep%20Learning/02.pytorch%E5%9F%BA%E7%A1%80/">
<meta property="og:site_name" content="My Learning Notes">
<meta property="og:description" content="环境搭建推荐使用Anaconda安装（可选）进入Anaconda官网下载安装即可，安装完成后搜索 Anaconda Prompt 以打开命令行。 路径前括号包裹的名字即为当前环境名，如 (base) C:\Users\Administrator&gt;即为base环境，该环境在Anaconda根目录下，不可删除。如果创建新环境，则存放于根目录下的envs文件夹内。 开始： 123456789101">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/pictures/02.Pytorch%E5%9F%BA%E7%A1%80/%E6%8D%9F%E5%A4%B1%E5%80%BC%E6%9B%B2%E7%BA%BF.png">
<meta property="article:published_time" content="2024-04-07T05:06:32.506Z">
<meta property="article:modified_time" content="2024-04-06T09:14:20.966Z">
<meta property="article:author" content="yuanjie xiang">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/pictures/02.Pytorch%E5%9F%BA%E7%A1%80/%E6%8D%9F%E5%A4%B1%E5%80%BC%E6%9B%B2%E7%BA%BF.png">


    
<link rel="stylesheet" href="/css/style/main.css">
 

    
    
    

    

     

    <!-- custom head -->

<meta name="generator" content="Hexo 7.1.1"></head>

    <body>
        <div id="app" tabindex="-1">
            <header class="header">
    <div class="header__left">
        <a href="/" class="button">
            <span class="logo__text">My Learning Notes</span>
        </a>
    </div>
    <div class="header__right">
        
            <div class="navbar__menus">
                
                    <a href="/" class="navbar-menu button">首页</a>
                
                    <a href="/archives/" class="navbar-menu button">归档</a>
                
            </div>
        
        
        

        
        

        
            <a class="dropdown-icon button" id="btn-dropdown" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width='24' height='24' fill="none" stroke="currentColor" stroke-width="0.7" stroke-linecap="round" stroke-linejoin="round"><path fill="currentColor" d="M3.314,4.8h13.372c0.41,0,0.743-0.333,0.743-0.743c0-0.41-0.333-0.743-0.743-0.743H3.314c-0.41,0-0.743,0.333-0.743,0.743C2.571,4.467,2.904,4.8,3.314,4.8z M16.686,15.2H3.314c-0.41,0-0.743,0.333-0.743,0.743s0.333,0.743,0.743,0.743h13.372c0.41,0,0.743-0.333,0.743-0.743S17.096,15.2,16.686,15.2z M16.686,9.257H3.314c-0.41,0-0.743,0.333-0.743,0.743s0.333,0.743,0.743,0.743h13.372c0.41,0,0.743-0.333,0.743-0.743S17.096,9.257,16.686,9.257z"></path></svg></a>
            <div class="dropdown-menus" id="dropdown-menus">
                
                    <a href="/" class="dropdown-menu button">首页</a>
                
                    <a href="/archives/" class="dropdown-menu button">归档</a>
                
            </div>
        
    </div>
</header>


            <main class="main">
    

<div class="post-title">
    <h1 class="post-title__text">
        02.pytorch基础
    </h1>
    <div class="post-title__meta">
        <a href="/archives/2024/04/" class="post-meta__date button">2024-04-07</a>
        
 
        
    
    


 

 
    </div>
</div>


    <aside class="post-side">
        <div class="post-side__toc">
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A8%E8%8D%90%E4%BD%BF%E7%94%A8Anaconda%E5%AE%89%E8%A3%85%EF%BC%88%E5%8F%AF%E9%80%89%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">推荐使用Anaconda安装（可选）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#pycharm-%E9%85%8D%E7%BD%AE-Anaconda%E4%B8%BAPython%E8%A7%A3%E9%87%8A%E5%99%A8"><span class="toc-number">1.1.</span> <span class="toc-text">pycharm 配置 Anaconda为Python解释器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85Pytorch%EF%BC%88%E5%BF%85%E9%A1%BB%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">安装Pytorch（必须）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Jupyter%EF%BC%88%E5%8F%AF%E9%80%89%EF%BC%89"><span class="toc-number">3.</span> <span class="toc-text">使用Jupyter（可选）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95"><span class="toc-number">4.</span> <span class="toc-text">问题记录</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8"><span class="toc-number">5.</span> <span class="toc-text">基础使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-nn-%E5%BA%93"><span class="toc-number">6.</span> <span class="toc-text">torch.nn 库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%AF%B9%E8%B1%A1%E7%9A%84%E8%87%AA%E5%8A%A8%E6%A2%AF%E5%BA%A6"><span class="toc-number">7.</span> <span class="toc-text">张量对象的自动梯度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tensor-%E5%AF%B9-ndarray-%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="toc-number">8.</span> <span class="toc-text">tensor 对 ndarray 的优势</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%81%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%99%A8%E5%92%8C%E6%89%B9%E5%A4%A7%E5%B0%8F"><span class="toc-number">9.</span> <span class="toc-text">数据集、数据加载器和批大小</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E8%AE%AD%E7%BB%83%E8%BF%87%E7%9A%84%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E6%96%B0%E5%80%BC"><span class="toc-number">10.</span> <span class="toc-text">使用训练过的模型预测新值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">11.</span> <span class="toc-text">实现自定义损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E4%B8%AD%E9%97%B4%E5%B1%82%E7%9A%84%E5%80%BC"><span class="toc-number">12.</span> <span class="toc-text">获取中间层的值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Sequential%E7%B1%BB%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">13.</span> <span class="toc-text">使用Sequential类构建神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E5%B9%B6%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="toc-number">14.</span> <span class="toc-text">保存并加载模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#state-dict"><span class="toc-number">14.1.</span> <span class="toc-text">state dict</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98"><span class="toc-number">14.2.</span> <span class="toc-text">保存</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD"><span class="toc-number">14.3.</span> <span class="toc-text">加载</span></a></li></ol></li></ol>
        </div>
    </aside>
    <a class="btn-toc button" id="btn-toc" tabindex="0">
        <svg viewBox="0 0 1024 1024" width="32" height="32" xmlns="http://www.w3.org/2000/svg">
            <path d="M128 256h64V192H128zM320 256h576V192H320zM128 544h64v-64H128zM320 544h576v-64H320zM128 832h64v-64H128zM320 832h576v-64H320z" fill="currentColor"></path>
        </svg>
    </a>
    <div class="toc-menus" id="toc-menus">
        <div class="toc-title">文章目录</div>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A8%E8%8D%90%E4%BD%BF%E7%94%A8Anaconda%E5%AE%89%E8%A3%85%EF%BC%88%E5%8F%AF%E9%80%89%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">推荐使用Anaconda安装（可选）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#pycharm-%E9%85%8D%E7%BD%AE-Anaconda%E4%B8%BAPython%E8%A7%A3%E9%87%8A%E5%99%A8"><span class="toc-number">1.1.</span> <span class="toc-text">pycharm 配置 Anaconda为Python解释器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85Pytorch%EF%BC%88%E5%BF%85%E9%A1%BB%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">安装Pytorch（必须）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Jupyter%EF%BC%88%E5%8F%AF%E9%80%89%EF%BC%89"><span class="toc-number">3.</span> <span class="toc-text">使用Jupyter（可选）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95"><span class="toc-number">4.</span> <span class="toc-text">问题记录</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8"><span class="toc-number">5.</span> <span class="toc-text">基础使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-nn-%E5%BA%93"><span class="toc-number">6.</span> <span class="toc-text">torch.nn 库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%AF%B9%E8%B1%A1%E7%9A%84%E8%87%AA%E5%8A%A8%E6%A2%AF%E5%BA%A6"><span class="toc-number">7.</span> <span class="toc-text">张量对象的自动梯度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tensor-%E5%AF%B9-ndarray-%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="toc-number">8.</span> <span class="toc-text">tensor 对 ndarray 的优势</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%81%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%99%A8%E5%92%8C%E6%89%B9%E5%A4%A7%E5%B0%8F"><span class="toc-number">9.</span> <span class="toc-text">数据集、数据加载器和批大小</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E8%AE%AD%E7%BB%83%E8%BF%87%E7%9A%84%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E6%96%B0%E5%80%BC"><span class="toc-number">10.</span> <span class="toc-text">使用训练过的模型预测新值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">11.</span> <span class="toc-text">实现自定义损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E4%B8%AD%E9%97%B4%E5%B1%82%E7%9A%84%E5%80%BC"><span class="toc-number">12.</span> <span class="toc-text">获取中间层的值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Sequential%E7%B1%BB%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">13.</span> <span class="toc-text">使用Sequential类构建神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E5%B9%B6%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="toc-number">14.</span> <span class="toc-text">保存并加载模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#state-dict"><span class="toc-number">14.1.</span> <span class="toc-text">state dict</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98"><span class="toc-number">14.2.</span> <span class="toc-text">保存</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD"><span class="toc-number">14.3.</span> <span class="toc-text">加载</span></a></li></ol></li></ol>
    </div>


<article class="post post__with-toc content-card">
    <div class="post__header"></div>
    <div class="post__content">
        <h1 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h1><h2 id="推荐使用Anaconda安装（可选）"><a href="#推荐使用Anaconda安装（可选）" class="headerlink" title="推荐使用Anaconda安装（可选）"></a>推荐使用Anaconda安装（可选）</h2><p>进入Anaconda官网下载安装即可，安装完成后搜索 Anaconda Prompt 以打开命令行。</p>
<p>路径前括号包裹的名字即为当前环境名，如 <code>(base) C:\Users\Administrator&gt;</code>即为base环境，该环境在Anaconda根目录下，不可删除。如果创建新环境，则存放于根目录下的<code>envs</code>文件夹内。</p>
<p><strong>开始：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置清华镜像</span></span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/</span><br><span class="line"><span class="comment"># 设置bioconda</span></span><br><span class="line">conda config --add channels bioconda</span><br><span class="line">conda config --add channels conda-forge</span><br><span class="line"><span class="comment"># 设置搜索时显示通道地址</span></span><br><span class="line">conda config --<span class="built_in">set</span> show_channel_urls yes</span><br><span class="line"><span class="comment"># 更新</span></span><br><span class="line">conda update conda</span><br><span class="line">conda update Anaconda</span><br></pre></td></tr></table></figure>

<p><strong>基本使用</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建虚拟环境</span></span><br><span class="line">conda create -n env_name python=<span class="number">3.8</span></span><br><span class="line">conda create -n env_name numpy matplotlib python=<span class="number">3.8</span></span><br><span class="line"><span class="comment"># 查看有哪些虚拟环境</span></span><br><span class="line">conda env <span class="built_in">list</span></span><br><span class="line">conda info -e</span><br><span class="line">conda info --envs</span><br><span class="line"><span class="comment"># 启用虚拟环境</span></span><br><span class="line">conda activate env_name</span><br><span class="line"><span class="comment"># 查看当前python版本</span></span><br><span class="line">python --version</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导出特定环境</span></span><br><span class="line"><span class="comment">#获得环境中的所有配置</span></span><br><span class="line">conda env export --name myenv &gt; myenv.yml</span><br><span class="line"><span class="comment">#重新还原环境</span></span><br><span class="line">conda env create -f  myenv.yml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看环境安装了哪些包</span></span><br><span class="line">conda <span class="built_in">list</span></span><br><span class="line"><span class="comment"># 安装和更新包</span></span><br><span class="line">conda install &lt;package&gt;</span><br><span class="line"><span class="comment"># 卸载包</span></span><br><span class="line">conda uninstall package_name</span><br><span class="line"><span class="comment"># 卸载包及依赖该包的包（不建议）</span></span><br><span class="line">conda uninstall package_name --force</span><br><span class="line"><span class="comment"># 变更Python版本</span></span><br><span class="line">conda install python=<span class="number">3.6</span></span><br><span class="line">conda update python</span><br></pre></td></tr></table></figure>

<h3 id="pycharm-配置-Anaconda为Python解释器"><a href="#pycharm-配置-Anaconda为Python解释器" class="headerlink" title="pycharm 配置 Anaconda为Python解释器"></a>pycharm 配置 Anaconda为Python解释器</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 设置-项目-python解释器</span><br><span class="line"># 指定路径为安装目录，或者指定安装目录下的 python.exe 文件</span><br></pre></td></tr></table></figure>



<h2 id="安装Pytorch（必须）"><a href="#安装Pytorch（必须）" class="headerlink" title="安装Pytorch（必须）"></a>安装Pytorch（必须）</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">进入官网：https://pytorch.org</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">选择快速开始</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">按你的环境选择操作系统、安装方式等</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果需要使用cuda，进入cmd输入nvidia-smi查看自己支持的cuda版本</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">最后复制命令行，进入conda命令行开始安装</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如下是在 windows 下使用 conda 安装pytorch的一个例子</span></span><br><span class="line">conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia</span><br></pre></td></tr></table></figure>

<p>安装完成后，进入Python命令行以查看是否成功。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)</span><br></pre></td></tr></table></figure>

<h2 id="使用Jupyter（可选）"><a href="#使用Jupyter（可选）" class="headerlink" title="使用Jupyter（可选）"></a>使用Jupyter（可选）</h2><blockquote>
<p>安装Anaconda后在Anaconda内安装Jupyter或使用命令行安装</p>
<p>如果使用pycharm，配置好Anaconda为解释器后，直接在pycahrm中使用即可</p>
</blockquote>
<p>使用Jupyter notebook以便捷的演示代码效果。jupyter 的目录在 Anaconda 的 scripts 下，将其加入环境变量。输入：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">命令行进入自己的文档目录</span></span><br><span class="line">jupyter notebook  ./</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">指定路径</span></span><br><span class="line">jupyter notebook  [文件夹路径]</span><br></pre></td></tr></table></figure>

<p>上述命令会打开浏览器，进入Jupyter界面，右键新建 <code>New Notebook</code> 即可开始使用。</p>
<p>在单元中我们可以编辑文字、编写代码、绘制图片等等。</p>
<p>单元中包含<strong>两种模式与快捷键</strong>，蓝色为命令模式，绿色为编辑模式，使用Enter和Esc在两种模式下切换。</p>
<ul>
<li>通用<ul>
<li><code>Shift+Enter</code>，执行本单元代码，并跳转到下一单元</li>
<li><code>Ctrl+Enter</code>，执行本单元代码，留在本单元</li>
</ul>
</li>
<li>命令模式<ul>
<li><code>Y</code>：cell切换到Code模式</li>
<li><code>M</code>：cell切换到Markdown模式</li>
<li><code>A</code>：在当前cell的上面添加cell</li>
<li><code>B</code>：在当前cell的下面添加cell</li>
<li><code>双击D</code>：删除当前cell</li>
<li><code>Z</code>：回退</li>
<li><code>Ctrl+Shift+减号</code>：分隔cell，在光标处</li>
<li><code>L</code>：为当前cell加上行号</li>
</ul>
</li>
<li>编辑模式(和大多数编辑器一样)<ul>
<li><code>Ctrl+Z(Mac:CMD+Z)</code>：回退</li>
<li><code>Ctrl+Y</code>：重做</li>
<li><code>Tab键</code>：代码补全</li>
<li><code>Ctrl+/</code>：注释多行代码</li>
</ul>
</li>
</ul>
<h2 id="问题记录"><a href="#问题记录" class="headerlink" title="问题记录"></a>问题记录</h2><ul>
<li>opencv安装 <code>conda install opencv</code> 后提示 <code>ImportError: DLL load failed while importing cv2: 找不到指定的模块</code></li>
<li>发现如果是未安装提示为：<code> ModuleNotFoundError: No module named &#39;xx&#39;</code></li>
<li>所以这是安装成功，但环境没配置好导致出现问题</li>
<li>使用 <code>conda remove opencv</code> 再 <code>pip install opencv-python</code>，问题解决</li>
<li>猜测原因是某些包使用 pip 安装，导致与conda环境不兼容，以后安装包要与最开始保持一致</li>
</ul>
<blockquote>
<p>参考 <a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/1124357">opencv-python 最新4.6.0.66版安装及介绍翻译-阿里云开发者社区 (aliyun.com)</a></p>
</blockquote>
<h1 id="张量-Tensor"><a href="#张量-Tensor" class="headerlink" title="张量 Tensor"></a>张量 Tensor</h1><blockquote>
<p>涉及到可以演示的代码放在同名的 <code>jupyter notebook</code> 文件中</p>
</blockquote>
<h2 id="基础使用"><a href="#基础使用" class="headerlink" title="基础使用"></a>基础使用</h2><p>张量 Tensor 是PyTorch中的基本数据类型。张量是一个多维矩阵，类似于NumPy中的ndarrays：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">y = torch.tensor([[<span class="number">1</span>], [<span class="number">2</span>]])</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="comment"># 如果初始数组中元素类型不同，会转化为能兼容全部值的元素类型</span></span><br><span class="line">z = torch.tensor([<span class="literal">False</span>, <span class="number">0.1</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>

<ul>
<li>张量实现了一些和numpy一样的基础函数，这里就不多赘述了</li>
<li>下面介绍一些有别于numpy的地方<ul>
<li>使用 matmul 或者 @ 进行矩阵乘法</li>
<li>使用permute交换维度</li>
<li>使用cat实现张量的连接</li>
</ul>
</li>
</ul>
<h2 id="torch-nn-库"><a href="#torch-nn-库" class="headerlink" title="torch.nn 库"></a>torch.nn 库</h2><p><code>torch.nn</code>是PyTorch中用于构建神经网络的模块。<code>nn</code>模块提供了一系列用于构建神经网络模型的类和函数，包括各种层（如全连接层、卷积层、循环神经网络层等）、损失函数、优化器等。</p>
<p><code>torch.nn</code>中的主要组件包括：</p>
<ol>
<li><strong>各种神经网络层</strong>：如全连接层（<code>nn.Linear</code>）、卷积层（<code>nn.Conv2d</code>）、池化层（<code>nn.MaxPool2d</code>）、循环神经网络层（<code>nn.RNN</code>、<code>nn.LSTM</code>、<code>nn.GRU</code>）等。</li>
<li><strong>激活函数</strong>：如ReLU激活函数（<code>nn.ReLU</code>）、Sigmoid激活函数（<code>nn.Sigmoid</code>）、Tanh激活函数（<code>nn.Tanh</code>）等。</li>
<li><strong>损失函数</strong>：如均方误差损失（<code>nn.MSELoss</code>）、交叉熵损失（<code>nn.CrossEntropyLoss</code>）等。</li>
<li><strong>优化器</strong>：如随机梯度下降优化器（<code>torch.optim.SGD</code>）、Adam优化器（<code>torch.optim.Adam</code>）等。</li>
</ol>
<p>使用<code>torch.nn</code>模块可以简化神经网络的构建过程，提供了丰富的工具和组件来搭建和训练神经网络模型。通常，您可以通过继承<code>nn.Module</code>类来定义自己的神经网络模型，并在其中组合和使用<code>nn</code>模块中提供的各种层和函数。</p>
<h2 id="张量对象的自动梯度"><a href="#张量对象的自动梯度" class="headerlink" title="张量对象的自动梯度"></a>张量对象的自动梯度</h2><p>微分和计算梯度在更新权重中起着关键作用。PyTorch的张量对象自带计算梯度的内置功能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定requires_grad为True表示要为张量对象计算梯度</span></span><br><span class="line">x = torch.tensor([[<span class="number">2.</span>, -<span class="number">1</span>], [<span class="number">1.</span>, <span class="number">1</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 定义输出方式为 x 平方和，这里返回的实际是一个Tensor对象</span></span><br><span class="line">out = x.<span class="built_in">pow</span>(<span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment"># 为 out 函数计算梯度，实际上我们知道 x^2 的微分结果为 2x</span></span><br><span class="line">out.backward()</span><br><span class="line"><span class="comment"># 如此可以直接得到梯度结果</span></span><br><span class="line">x.grad</span><br><span class="line"><span class="comment"># tensor([[ 4., -2.], [ 2.,  2.]])</span></span><br></pre></td></tr></table></figure>

<h2 id="tensor-对-ndarray-的优势"><a href="#tensor-对-ndarray-的优势" class="headerlink" title="tensor 对 ndarray 的优势"></a>tensor 对 ndarray 的优势</h2><ul>
<li><p>torch优化了张量能与GPU一起工作，如果每个权重由不同的内核并行完成，则可以优化权重的计算过程，大幅度提高计算效率</p>
</li>
<li><pre><code class="Python"># 随机生成两个torch对象
x = torch.rand(1, 6400)
y = torch.rand(6400, 5000)
# print(x, y)
# 定义使用 gpu 进行计算，如果没有gpu则使用cpu
device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;

# 执行矩阵乘法并计时
x, y = x.to(device), y.to(device)
%timeit z = x@y

x, y = x.cpu(), y.cpu()
%timeit z = x@y
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
412 µs ± 5.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
3.79 ms ± 14.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 使用pytorch构建神经网络模型</span><br><span class="line"></span><br><span class="line">下面通过tensor实现一个神经网络，解决两个数相加的问题</span><br><span class="line"></span><br><span class="line">在每轮训练中，我们需要做的步骤有：</span><br><span class="line"></span><br><span class="line">1. 计算给定输入输出所对应的损失值</span><br><span class="line">2. 计算每个参数对应的梯度</span><br><span class="line">3. 根据梯度更新权重</span><br><span class="line"></span><br><span class="line">```Python</span><br><span class="line">import torch</span><br><span class="line"># torch中用于构建神经网络的模块</span><br><span class="line">import torch.nn as nn</span><br><span class="line"># torch中的梯度下降算法模块</span><br><span class="line">import torch.optim as optim</span><br><span class="line"></span><br><span class="line"># 两个数相加问题</span><br><span class="line">x = [[1,2],[3,4],[5,6],[7,8]]</span><br><span class="line">y = [[3],[7],[11],[15]]</span><br><span class="line"></span><br><span class="line"># 将数转化为浮点数，避免精度损失</span><br><span class="line">X = torch.tensor(x).float()</span><br><span class="line">Y = torch.tensor(y).float()</span><br><span class="line"></span><br><span class="line"># 使用 GPU 进行计算</span><br><span class="line">device = &#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;</span><br><span class="line">X = X.to(device)</span><br><span class="line">Y = Y.to(device)</span><br><span class="line"></span><br><span class="line"># 一个简单的神经网络模型，继承于nn.Module类</span><br><span class="line">class MyNeuralNet(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        # 输入大小为2，输出大小为8的全连接层</span><br><span class="line">        self.input_to_hidden_layer = nn.Linear(2,8)</span><br><span class="line">        # ReLU 作激活函数</span><br><span class="line">        self.hidden_layer_activation = nn.ReLU()</span><br><span class="line">        # 输入大小为8，输出大小为1的全连接层</span><br><span class="line">        self.hidden_to_output_layer = nn.Linear(8,1)</span><br><span class="line">        </span><br><span class="line">    # 覆写前向传播方法，当调用模型实例时，实际上会调用该方法执行前向传播</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.input_to_hidden_layer(x)</span><br><span class="line">        x = self.hidden_layer_activation(x)</span><br><span class="line">        x = self.hidden_to_output_layer(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"># 创建模型实例并指定设备</span><br><span class="line">my_net = MyNeuralNet().to(device)</span><br><span class="line"># 定义损失函数为均方差</span><br><span class="line">loss_func = nn.MSELoss()</span><br><span class="line"># 为模型指定使用随机梯度下降算法，且学习率为0.001</span><br><span class="line">opt = optim.SGD(my_net.parameters(), lr=0.001)</span><br><span class="line"></span><br><span class="line"># 执行迭代</span><br><span class="line">loss_history = []</span><br><span class="line">for epoch in range(500):</span><br><span class="line">    # 前向传播并计算损失值</span><br><span class="line">    output = my_net(X)</span><br><span class="line">    loss_value = loss_func(output,Y)</span><br><span class="line">    </span><br><span class="line">    # 反向传播和优化</span><br><span class="line">    # 每次都需要将梯度置零</span><br><span class="line">    opt.zero_grad()</span><br><span class="line">    # 计算梯度值</span><br><span class="line">    loss_value.backward()</span><br><span class="line">    # 根据计算到的梯度更新模型的参数（）</span><br><span class="line">    opt.step()</span><br><span class="line">    loss_history.append(loss_value.item())</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
<p><img src="/pictures/02.Pytorch%E5%9F%BA%E7%A1%80/%E6%8D%9F%E5%A4%B1%E5%80%BC%E6%9B%B2%E7%BA%BF.png" alt="image-20240405172003300"></p>
<h2 id="数据集、数据加载器和批大小"><a href="#数据集、数据加载器和批大小" class="headerlink" title="数据集、数据加载器和批大小"></a>数据集、数据加载器和批大小</h2><ol>
<li><strong>数据集（Dataset）</strong>： 数据集是一组数据样本的集合，用于训练、验证或测试模型。在PyTorch中，您可以通过创建一个自定义的数据集类来加载您的数据，该类需要继承自<code>torch.utils.data.Dataset</code>类，并实现<code>__len__()</code>和<code>__getitem__()</code>方法。数据集类负责加载和处理单个样本，并在需要时对其进行转换。</li>
<li><strong>数据加载器（DataLoader）</strong>： 数据加载器是一个用于加载数据集的迭代器，它会自动将数据划分为小批量（batch）并提供给模型。数据加载器使用数据集来生成批量数据，并支持多进程加载数据以加速训练。通过使用数据加载器，您可以方便地迭代整个数据集，并在训练过程中为模型提供批量数据。</li>
<li><strong>批大小（batch size）</strong>： 批大小是指在每次训练迭代中使用的样本数量。将数据划分为小批量进行训练可以提高训练的效率，并利用并行计算的能力。较大的批大小通常会导致更高的训练速度，但可能会增加内存消耗。通常，批大小的选择取决于您的硬件资源、模型的复杂度以及数据集的大小。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在上面代码的基础上变更以演示批大小的使用</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># 导入数据集和数据加载器模块</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line">x = [[<span class="number">1</span> ,<span class="number">2</span>] ,[<span class="number">3</span> ,<span class="number">4</span>] ,[<span class="number">5</span> ,<span class="number">6</span>] ,[<span class="number">7</span> ,<span class="number">8</span>]]</span><br><span class="line">y = [[<span class="number">3</span>] ,[<span class="number">7</span>] ,[<span class="number">11</span>] ,[<span class="number">15</span>]]</span><br><span class="line"></span><br><span class="line">X = torch.tensor(x).<span class="built_in">float</span>()</span><br><span class="line">Y = torch.tensor(y).<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">X = X.to(device)</span><br><span class="line">Y = Y.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义数据集类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self ,x ,y</span>):</span><br><span class="line">        <span class="comment"># 复制带有梯度的张量时，指定.detach().requires_grad_(True)避免报错</span></span><br><span class="line">        self.x = x.clone().detach().requires_grad_(<span class="literal">True</span>).<span class="built_in">float</span>()</span><br><span class="line">        self.y = y.clone().detach().requires_grad_(<span class="literal">True</span>).<span class="built_in">float</span>()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.x)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.x[idx], self.y[idx]</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNeuralNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.input_to_hidden_layer = nn.Linear(<span class="number">2</span> ,<span class="number">8</span>)</span><br><span class="line">        self.hidden_layer_activation = nn.ReLU()</span><br><span class="line">        self.hidden_to_output_layer = nn.Linear(<span class="number">8</span> ,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.input_to_hidden_layer(x)</span><br><span class="line">        x = self.hidden_layer_activation(x)</span><br><span class="line">        x = self.hidden_to_output_layer(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个数据集实例</span></span><br><span class="line">ds = MyDataset(X, Y)</span><br><span class="line"><span class="comment"># 创建数据加载器并设置批大小为2</span></span><br><span class="line">dl = DataLoader(ds, batch_size=<span class="number">2</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">my_net = MyNeuralNet().to(device)</span><br><span class="line">loss_func = nn.MSELoss()</span><br><span class="line">opt = optim.SGD(my_net.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50</span>):</span><br><span class="line">    <span class="comment"># 从数据加载器拿每批数据</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dl:</span><br><span class="line">        <span class="comment"># 注意，从数据加载器中拿到被分割的预估值并使用</span></span><br><span class="line">        x, y = data</span><br><span class="line">        output = my_net(x)</span><br><span class="line">        loss_value = loss_func(output ,y)</span><br><span class="line"></span><br><span class="line">        opt.zero_grad()</span><br><span class="line">        loss_value.backward()</span><br><span class="line">        opt.step()</span><br></pre></td></tr></table></figure>

<ul>
<li>一般来说，较大的批大小可能会导致更稳定的梯度估计和更快的收敛速度，因为每个更新步骤考虑了更多的数据样本。</li>
<li>而较小的批大小通常会减少内存消耗，并且可能会在一定程度上提高模型的泛化能力。</li>
</ul>
<h2 id="使用训练过的模型预测新值"><a href="#使用训练过的模型预测新值" class="headerlink" title="使用训练过的模型预测新值"></a>使用训练过的模型预测新值</h2><p>通过上面的步骤，我们获得了一个经过学习的模型，接下来使用该模型计算新输入，验证模型效果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val_x = [[<span class="number">10</span>, <span class="number">11</span>]]</span><br><span class="line">val_x = torch.tensor(val_x).<span class="built_in">float</span>().to(device)</span><br><span class="line">my_net(val_x)</span><br><span class="line"><span class="comment"># tensor([[20.4325]], device=&#x27;cuda:0&#x27;, grad_fn=&lt;AddmmBackward0&gt;) </span></span><br></pre></td></tr></table></figure>

<h2 id="实现自定义损失函数"><a href="#实现自定义损失函数" class="headerlink" title="实现自定义损失函数"></a>实现自定义损失函数</h2><p>不同的问题可能需要不同的损失函数，因此我们需要能针对具体问题具体分析，并写出更适合的损失函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在上面代码的基础上变更以演示自定义损失函数</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line">x = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]]</span><br><span class="line">y = [[<span class="number">3</span>], [<span class="number">7</span>], [<span class="number">11</span>], [<span class="number">15</span>]]</span><br><span class="line"></span><br><span class="line">X = torch.tensor(x).<span class="built_in">float</span>()</span><br><span class="line">Y = torch.tensor(y).<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">X = X.to(device)</span><br><span class="line">Y = Y.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        self.x = x.clone().detach().requires_grad_(<span class="literal">True</span>).<span class="built_in">float</span>()</span><br><span class="line">        self.y = y.clone().detach().requires_grad_(<span class="literal">True</span>).<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.x[idx], self.y[idx]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNeuralNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.input_to_hidden_layer = nn.Linear(<span class="number">2</span>, <span class="number">8</span>)</span><br><span class="line">        self.hidden_layer_activation = nn.ReLU()</span><br><span class="line">        self.hidden_to_output_layer = nn.Linear(<span class="number">8</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.input_to_hidden_layer(x)</span><br><span class="line">        x = self.hidden_layer_activation(x)</span><br><span class="line">        x = self.hidden_to_output_layer(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ds = MyDataset(X, Y)</span><br><span class="line">dl = DataLoader(ds, batch_size=<span class="number">2</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">my_net = MyNeuralNet().to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># loss_func = nn.MSELoss()</span></span><br><span class="line"><span class="comment"># 自定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_mean_squared_error</span>(<span class="params">_y, y</span>):</span><br><span class="line">    loss = (_y - y) ** <span class="number">2</span></span><br><span class="line">    loss = loss.mean()</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">opt = optim.SGD(my_net.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">loss_history = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    loss_value_count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dl:</span><br><span class="line">        x, y = data</span><br><span class="line">        output = my_net(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># loss_value = loss_func(output ,y)</span></span><br><span class="line">        <span class="comment"># 使用自定义损失函数</span></span><br><span class="line">        loss_value = my_mean_squared_error(output, y)</span><br><span class="line"></span><br><span class="line">        opt.zero_grad()</span><br><span class="line">        loss_value.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        loss_value_count += loss_value.item()</span><br><span class="line"></span><br><span class="line">    loss_history.append(loss_value_count)</span><br></pre></td></tr></table></figure>

<h2 id="获取中间层的值"><a href="#获取中间层的值" class="headerlink" title="获取中间层的值"></a>获取中间层的值</h2><p>某些情况下，我们也需要获取网络中间层的值，Pytorch提供了两种方法：</p>
<ul>
<li><p>直接调用我们在模型<code>__init__</code>函数中定义的属性即可</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mynet.input_to_hidden_layer(X)</span><br></pre></td></tr></table></figure>
</li>
<li><p>我们也可以在覆写forward函数时指定返回值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        hidden1 = self.input_to_hidden_layer(x)</span><br><span class="line">        hidden2 = self.hidden_layer_activation(hidden1)</span><br><span class="line">        x = self.hidden_to_output_layer(hidden2)</span><br><span class="line">        <span class="keyword">return</span> x, hidden1</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="使用Sequential类构建神经网络"><a href="#使用Sequential类构建神经网络" class="headerlink" title="使用Sequential类构建神经网络"></a>使用<code>Sequential</code>类构建神经网络</h2><p>在PyTorch中，<code>Sequential</code>类是一个简单的容器，允许将模块按照顺序排列在一起，以便构建神经网络。它允许你按顺序添加各种层（如全连接层、激活函数、池化层等），以构建模型的架构，而无需手动定义<code>forward</code>方法。这使得创建简单的神经网络变得更加简洁和直观。</p>
<ol>
<li><strong>构造方法</strong>：<code>Sequential</code>类的构造方法允许你传递一个模块列表作为参数，这些模块将按顺序组成神经网络的层结构。</li>
<li><strong>添加模块</strong>：你可以使用<code>.add_module()</code>方法或直接将模块作为参数传递给<code>Sequential</code>构造函数来添加模块。这些模块将按照它们被添加的顺序被调用。</li>
<li><strong>forward方法</strong>：当你调用<code>Sequential</code>实例的实例时，它会按照添加的顺序依次调用每个模块的<code>forward</code>方法。这意味着你可以像调用普通函数一样调用<code>Sequential</code>实例，并且输入数据会依次通过每个层。</li>
<li><strong>嵌套Sequential</strong>：你可以在<code>Sequential</code>中嵌套另一个<code>Sequential</code>，这使得构建复杂的神经网络变得更加方便。</li>
</ol>
<p>使用<code>Sequential</code>类可以大大简化神经网络模型的构建过程，特别是对于简单的线性层叠加结构而言。然而，对于具有更复杂结构或需要非顺序连接的模型，则可能需要使用<code>nn.Module</code>的子类来更灵活地定义<code>forward</code>方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># my_net = MyNeuralNet().to(device)</span></span><br><span class="line"><span class="comment"># 删除MyNeuralNet类直接使用 Sequential类定义模型架构</span></span><br><span class="line">my_net = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">2</span>, <span class="number">8</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">8</span>, <span class="number">1</span>)</span><br><span class="line">).to(device)</span><br></pre></td></tr></table></figure>

<h2 id="保存并加载模型"><a href="#保存并加载模型" class="headerlink" title="保存并加载模型"></a>保存并加载模型</h2><p>要保存一个完整的神经网络模型，通常需要保存以下要素：</p>
<ol>
<li><strong>模型的架构</strong>：包括神经网络的层次结构和参数。这可以通过将模型的定义保存为代码或元数据的形式来实现。</li>
<li><strong>模型的权重和偏置参数</strong>：这些是模型在训练过程中学习到的参数，用于将输入数据映射到输出。你需要将这些参数保存在文件中，以便在以后的推理过程中加载和使用。</li>
<li><strong>模型状态</strong>：包括模型的当前状态，如优化器的状态或训练过程中的其他状态信息。这些信息可能有助于在以后的推理或继续训练过程中保持模型的连续性。</li>
<li><strong>模型元数据</strong>：可能包括模型的输入规格、输出规格、激活函数类型等信息。这些元数据可以帮助在加载模型时正确地配置模型。</li>
<li><strong>可选：优化器状态</strong>：如果需要在以后的训练过程中继续训练模型，则可能需要保存优化器的状态信息。</li>
</ol>
<h3 id="state-dict"><a href="#state-dict" class="headerlink" title="state dict"></a>state dict</h3><p>pytorch中提供了 model.state_dict() 方法，返回一个包含键和值的字典，也可以理解为一个存放模型快照的字典。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">my_net.state_dict()</span><br><span class="line">```</span><br><span class="line">OrderedDict([(<span class="string">&#x27;0.weight&#x27;</span>,</span><br><span class="line">              tensor([[ <span class="number">0.8780</span>,  <span class="number">0.4156</span>],</span><br><span class="line">                      [-<span class="number">0.6139</span>, -<span class="number">0.4396</span>],</span><br><span class="line">                      [ <span class="number">0.3576</span>, -<span class="number">0.4563</span>],</span><br><span class="line">                      [-<span class="number">0.2003</span>, -<span class="number">0.1653</span>],</span><br><span class="line">                      [ <span class="number">0.2468</span>,  <span class="number">0.8232</span>],</span><br><span class="line">                      [ <span class="number">0.1288</span>, -<span class="number">0.2658</span>],</span><br><span class="line">                      [ <span class="number">0.2706</span>, -<span class="number">0.5380</span>],</span><br><span class="line">                      [-<span class="number">0.5914</span>, -<span class="number">0.6458</span>]], device=<span class="string">&#x27;cuda:0&#x27;</span>)),</span><br><span class="line">             (<span class="string">&#x27;0.bias&#x27;</span>,</span><br><span class="line">              tensor([ <span class="number">0.3281</span>,  <span class="number">0.6249</span>, -<span class="number">0.6074</span>, -<span class="number">0.5196</span>,  <span class="number">0.0526</span>,  <span class="number">0.3178</span>,  <span class="number">0.2535</span>, -<span class="number">0.5681</span>],</span><br><span class="line">                     device=<span class="string">&#x27;cuda:0&#x27;</span>)),</span><br><span class="line">             (<span class="string">&#x27;2.weight&#x27;</span>,</span><br><span class="line">              tensor([[ <span class="number">0.8013</span>, -<span class="number">0.0154</span>,  <span class="number">0.2462</span>, -<span class="number">0.2146</span>,  <span class="number">0.7951</span>, -<span class="number">0.2686</span>,  <span class="number">0.0645</span>,  <span class="number">0.1832</span>]],</span><br><span class="line">                     device=<span class="string">&#x27;cuda:0&#x27;</span>)),</span><br><span class="line">             (<span class="string">&#x27;2.bias&#x27;</span>, tensor([<span class="number">0.2748</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>))])</span><br><span class="line">```</span><br></pre></td></tr></table></figure>

<h3 id="保存"><a href="#保存" class="headerlink" title="保存"></a>保存</h3><p><code>torch.save()</code>函数用于将模型的状态保存到文件中。它的语法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># torch.save(obj, filepath)</span></span><br><span class="line">save_path = <span class="string">&#x27;my_net.pth&#x27;</span></span><br><span class="line">torch.save(my_net.state_dict(), save_path)</span><br><span class="line"><span class="comment"># 在文件根目录下出现 my_net.pth 文件</span></span><br></pre></td></tr></table></figure>

<ul>
<li><code>obj</code>：要保存的对象，可以是模型的状态字典、整个模型、优化器的状态字典等。</li>
<li><code>filepath</code>：保存文件的路径，可以是文件名或包含路径的完整文件路径。</li>
</ul>
<h3 id="加载"><a href="#加载" class="headerlink" title="加载"></a>加载</h3><p><code>torch.load()</code>函数用于从文件中加载保存的对象。它的语法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># torch.load(filepath, map_location=None)</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">2</span>, <span class="number">8</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">8</span>, <span class="number">1</span>)</span><br><span class="line">).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看模型结构</span></span><br><span class="line"><span class="keyword">from</span> torchsummary <span class="keyword">import</span> summary</span><br><span class="line"><span class="comment"># 需要指定模型输入值的形状</span></span><br><span class="line">summary(model, torch.zeros(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将模型加载入刚才创建的未训练模型</span></span><br><span class="line">load_path = <span class="string">&#x27;my_net.pth&#x27;</span></span><br><span class="line">model.load_state_dict(torch.load(load_path, map_location=torch.device(device)))</span><br><span class="line"></span><br><span class="line">model(torch.tensor([<span class="number">2</span>, <span class="number">3</span>]).<span class="built_in">float</span>().to(device))</span><br><span class="line"><span class="comment"># tensor([5.3419], device=&#x27;cuda:0&#x27;, grad_fn=&lt;ViewBackward0&gt;)</span></span><br></pre></td></tr></table></figure>

<ul>
<li><code>filepath</code>：保存文件的路径，可以是文件名或包含路径的完整文件路径。</li>
<li><code>map_location</code>：一个可选参数，用于指定将数据加载到的设备。如果不指定，默认会加载到CPU上。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/1124357">https://developer.aliyun.com/article/1124357</a>)</p>

    </div>
     
    <div class="post-footer__meta"><p>更新于 2024-04-06</p></div> 
    <div class="post-entry__tags"></div> 
</article>


    <div class="nav">
        <div class="nav__prev">
            
                <a href="/Deep%20Learning/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/" class="nav__link">
                    <div>
                        <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="24" height="24"><path d="M589.088 790.624L310.464 512l278.624-278.624 45.248 45.248L400.96 512l233.376 233.376z" fill="#808080"></path></svg>
                    </div>
                    <div>
                        <div class="nav__label">
                            上一篇
                        </div>
                        <div class="nav__title">
                            03.使用pytorch构建神经网络（下）
                        </div>
                    </div>
                </a>
            
        </div>
        <div class="nav__next">
            
                <a href="/Deep%20Learning/01.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" class="nav__link">
                    <div>
                        <div class="nav__label">
                            下一篇
                        </div>
                        <div class="nav__title">
                            01.神经网络基础
                        </div>
                    </div>
                    <div>
                        <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="24" height="24"><path d="M434.944 790.624l-45.248-45.248L623.04 512l-233.376-233.376 45.248-45.248L713.568 512z" fill="#808080"></path></svg>
                    </div>
                </a>
            
        </div>
    </div>





</main>

            <footer class="footer">
    
    


    
     
 

 
    
        
        <p class="footer-copyright">
            Copyright © 2024 <a href="/">My Learning Notes</a>
        </p>
    
    
    <p>Powered by <a href="https://hexo.io" target="_blank">Hexo</a> | Theme - <a href="https://github.com/ChrAlpha/hexo-theme-cards" target="_blank">Cards</a></p>
</footer>

        </div>
         

 

 

 

 



 



 


    
 

 

 

 

 

 




    </body>
</html>
