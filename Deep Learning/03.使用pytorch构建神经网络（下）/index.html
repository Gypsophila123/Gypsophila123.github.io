<!DOCTYPE html>
<html lang="zh-CN">
    <head>
    <meta charset="utf-8">

    

    <!-- 渲染优化 -->
    <meta name="renderer" content="webkit">
    <meta name="force-rendering" content="webkit">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="HandheldFriendly" content="True" >
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <!--icon-->

    
        <link rel="shortcut icon" href="/github.ico">
    
    
        <link rel="icon" type="image/png" sizes="16x16" href="/github_small.png">
    
    
        <link rel="icon" type="image/png" sizes="32x32" href="/github_medium.png">
    
    
    


    <!-- meta -->


<title>03.使用pytorch构建神经网络（下） | My Learning Notes</title>





    <!-- OpenGraph -->
 
    <meta name="description" content="[TOC] 理解不同损失优化器的影响在此前，我们一直使用随机梯度下降法（Stochastic Gradient Descent, SGD）作为优化方法，在本节，我们将使用亚当优化器或者说自适应矩估计算法（Adaptive Moment Estimation, Adam） 作为优化算法，测试不同优化器的影响，我们接下来需要：  导入并修改优化器为 Adam 123456789101112from t">
<meta property="og:type" content="article">
<meta property="og:title" content="03.使用pytorch构建神经网络（下）">
<meta property="og:url" content="http://example.com/Deep%20Learning/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/">
<meta property="og:site_name" content="My Learning Notes">
<meta property="og:description" content="[TOC] 理解不同损失优化器的影响在此前，我们一直使用随机梯度下降法（Stochastic Gradient Descent, SGD）作为优化方法，在本节，我们将使用亚当优化器或者说自适应矩估计算法（Adaptive Moment Estimation, Adam） 作为优化算法，测试不同优化器的影响，我们接下来需要：  导入并修改优化器为 Adam 123456789101112from t">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/01%E4%BD%BF%E7%94%A8SGD%E4%BC%98%E5%8C%96%E5%99%A8.png">
<meta property="og:image" content="http://example.com/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/02%E4%BD%BF%E7%94%A8%E4%BA%9A%E5%BD%93%E4%BC%98%E5%8C%96%E5%99%A8.png">
<meta property="og:image" content="http://example.com/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/03%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%BA0.1%E5%AF%B9%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%BD%B1%E5%93%8D.png">
<meta property="og:image" content="http://example.com/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/04%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%BA0.001%E5%AF%B9%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%BD%B1%E5%93%8D.png.png">
<meta property="og:image" content="http://example.com/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/05%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%BA0.00001%E5%AF%B9%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%BD%B1%E5%93%8D.png">
<meta property="og:image" content="http://example.com/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/06%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%BE%83%E5%A4%A7%E6%97%B6%E7%9A%84%E5%8F%82%E6%95%B0%E5%88%86%E5%B8%83.png">
<meta property="og:image" content="http://example.com/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/07%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%AD%E7%AD%89%E6%97%B6%E7%9A%84%E5%8F%82%E6%95%B0%E5%88%86%E5%B8%83.png">
<meta property="og:image" content="http://example.com/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/08%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%BE%83%E5%B0%8F%E6%97%B6%E7%9A%84%E5%8F%82%E6%95%B0%E5%88%86%E5%B8%83.png">
<meta property="og:image" content="http://example.com/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/09%E4%B8%8D%E7%BC%A9%E5%B0%8F%E6%95%B0%E6%8D%AE%E4%B8%94%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%BA0.1.png">
<meta property="og:image" content="http://example.com/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/10%E4%B8%8D%E7%BC%A9%E5%B0%8F%E6%95%B0%E6%8D%AE%E4%B8%94%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%BA0.001.png">
<meta property="og:image" content="http://example.com/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/11%E4%B8%8D%E7%BC%A9%E5%B0%8F%E6%95%B0%E6%8D%AE%E4%B8%94%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%BA0.00001.png">
<meta property="og:image" content="http://example.com/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/12%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F%E7%9A%84%E6%95%88%E6%9E%9C.png">
<meta property="article:published_time" content="2024-04-07T05:06:32.512Z">
<meta property="article:modified_time" content="2024-04-07T01:55:11.529Z">
<meta property="article:author" content="yuanjie xiang">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/01%E4%BD%BF%E7%94%A8SGD%E4%BC%98%E5%8C%96%E5%99%A8.png">


    
<link rel="stylesheet" href="/css/style/main.css">
 

    
    
    

    

     

    <!-- custom head -->

<meta name="generator" content="Hexo 7.1.1"></head>

    <body>
        <div id="app" tabindex="-1">
            <header class="header">
    <div class="header__left">
        <a href="/" class="button">
            <span class="logo__text">My Learning Notes</span>
        </a>
    </div>
    <div class="header__right">
        
            <div class="navbar__menus">
                
                    <a href="/" class="navbar-menu button">首页</a>
                
                    <a href="/archives/" class="navbar-menu button">归档</a>
                
            </div>
        
        
        

        
        

        
            <a class="dropdown-icon button" id="btn-dropdown" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width='24' height='24' fill="none" stroke="currentColor" stroke-width="0.7" stroke-linecap="round" stroke-linejoin="round"><path fill="currentColor" d="M3.314,4.8h13.372c0.41,0,0.743-0.333,0.743-0.743c0-0.41-0.333-0.743-0.743-0.743H3.314c-0.41,0-0.743,0.333-0.743,0.743C2.571,4.467,2.904,4.8,3.314,4.8z M16.686,15.2H3.314c-0.41,0-0.743,0.333-0.743,0.743s0.333,0.743,0.743,0.743h13.372c0.41,0,0.743-0.333,0.743-0.743S17.096,15.2,16.686,15.2z M16.686,9.257H3.314c-0.41,0-0.743,0.333-0.743,0.743s0.333,0.743,0.743,0.743h13.372c0.41,0,0.743-0.333,0.743-0.743S17.096,9.257,16.686,9.257z"></path></svg></a>
            <div class="dropdown-menus" id="dropdown-menus">
                
                    <a href="/" class="dropdown-menu button">首页</a>
                
                    <a href="/archives/" class="dropdown-menu button">归档</a>
                
            </div>
        
    </div>
</header>


            <main class="main">
    

<div class="post-title">
    <h1 class="post-title__text">
        03.使用pytorch构建神经网络（下）
    </h1>
    <div class="post-title__meta">
        <a href="/archives/2024/04/" class="post-meta__date button">2024-04-07</a>
        
 
        
    
    


 

 
    </div>
</div>


    <aside class="post-side">
        <div class="post-side__toc">
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E5%AF%B9%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">1.</span> <span class="toc-text">学习率对缩放数据集的影响</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%BE%83%E5%A4%A7"><span class="toc-number">1.1.</span> <span class="toc-text">学习率较大</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E9%80%82%E4%B8%AD"><span class="toc-number">1.2.</span> <span class="toc-text">学习率适中</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%BE%83%E5%B0%8F"><span class="toc-number">1.3.</span> <span class="toc-text">学习率较小</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E5%AD%A6%E4%B9%A0%E7%8E%87%E7%9A%84%E4%B8%8D%E5%90%8C%E5%8F%82%E6%95%B0%E5%88%86%E5%B8%83"><span class="toc-number">1.4.</span> <span class="toc-text">不同学习率的不同参数分布</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E5%AD%A6%E4%B9%A0%E7%8E%87%E5%AF%B9%E9%9D%9E%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">2.</span> <span class="toc-text">不同学习率对非缩放数据集的影响</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B2%A1%E6%9C%89%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E9%9D%9E%E5%B8%B8%E5%B0%8F%E7%9A%84%E8%BE%93%E5%85%A5%E5%80%BC"><span class="toc-number">3.</span> <span class="toc-text">没有批归一化的非常小的输入值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%8F%E8%BF%87%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E9%9D%9E%E5%B8%B8%E5%B0%8F%E7%9A%84%E8%BE%93%E5%85%A5%E5%80%BC"><span class="toc-number">4.</span> <span class="toc-text">经过批归一化的非常小的输入值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%BB%E5%8A%A0dropout%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">5.</span> <span class="toc-text">添加dropout的影响</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">6.</span> <span class="toc-text">正则化的影响</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#L1%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">6.1.</span> <span class="toc-text">L1正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#L2%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">6.2.</span> <span class="toc-text">L2正则化</span></a></li></ol></li></ol>
        </div>
    </aside>
    <a class="btn-toc button" id="btn-toc" tabindex="0">
        <svg viewBox="0 0 1024 1024" width="32" height="32" xmlns="http://www.w3.org/2000/svg">
            <path d="M128 256h64V192H128zM320 256h576V192H320zM128 544h64v-64H128zM320 544h576v-64H320zM128 832h64v-64H128zM320 832h576v-64H320z" fill="currentColor"></path>
        </svg>
    </a>
    <div class="toc-menus" id="toc-menus">
        <div class="toc-title">文章目录</div>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E5%AF%B9%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">1.</span> <span class="toc-text">学习率对缩放数据集的影响</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%BE%83%E5%A4%A7"><span class="toc-number">1.1.</span> <span class="toc-text">学习率较大</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E9%80%82%E4%B8%AD"><span class="toc-number">1.2.</span> <span class="toc-text">学习率适中</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%BE%83%E5%B0%8F"><span class="toc-number">1.3.</span> <span class="toc-text">学习率较小</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E5%AD%A6%E4%B9%A0%E7%8E%87%E7%9A%84%E4%B8%8D%E5%90%8C%E5%8F%82%E6%95%B0%E5%88%86%E5%B8%83"><span class="toc-number">1.4.</span> <span class="toc-text">不同学习率的不同参数分布</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E5%AD%A6%E4%B9%A0%E7%8E%87%E5%AF%B9%E9%9D%9E%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">2.</span> <span class="toc-text">不同学习率对非缩放数据集的影响</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B2%A1%E6%9C%89%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E9%9D%9E%E5%B8%B8%E5%B0%8F%E7%9A%84%E8%BE%93%E5%85%A5%E5%80%BC"><span class="toc-number">3.</span> <span class="toc-text">没有批归一化的非常小的输入值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%8F%E8%BF%87%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E9%9D%9E%E5%B8%B8%E5%B0%8F%E7%9A%84%E8%BE%93%E5%85%A5%E5%80%BC"><span class="toc-number">4.</span> <span class="toc-text">经过批归一化的非常小的输入值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%BB%E5%8A%A0dropout%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">5.</span> <span class="toc-text">添加dropout的影响</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">6.</span> <span class="toc-text">正则化的影响</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#L1%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">6.1.</span> <span class="toc-text">L1正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#L2%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">6.2.</span> <span class="toc-text">L2正则化</span></a></li></ol></li></ol>
    </div>


<article class="post post__with-toc content-card">
    <div class="post__header"></div>
    <div class="post__content">
        <p>[TOC]</p>
<h1 id="理解不同损失优化器的影响"><a href="#理解不同损失优化器的影响" class="headerlink" title="理解不同损失优化器的影响"></a>理解不同损失优化器的影响</h1><p>在此前，我们一直使用<strong>随机梯度下降法（Stochastic Gradient Descent, SGD）</strong>作为优化方法，在本节，我们将使用亚当优化器或者说自适应矩估计算法（Adaptive Moment Estimation, Adam） 作为优化算法，测试不同优化器的影响，我们接下来需要：</p>
<ol>
<li><p>导入并修改优化器为 Adam</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> SGD, Adam</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_model</span>():</span><br><span class="line">    model = nn.Sequential(</span><br><span class="line">        nn.Linear(<span class="number">28</span> * <span class="number">28</span>, <span class="number">1000</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Linear(<span class="number">1000</span>, <span class="number">10</span>)</span><br><span class="line">    ).to(device)</span><br><span class="line">    loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">    <span class="comment"># 使用Adam优化器</span></span><br><span class="line">    optimizer = Adam(model.parameters(), lr=<span class="number">1e-2</span>)</span><br><span class="line">    <span class="keyword">return</span> model, loss_fn, optimizer</span><br></pre></td></tr></table></figure>
</li>
<li><p>将训练时的批大小恢复为32</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_data</span>(): </span><br><span class="line">    train = FMNISTDataset(train_data, train_labels)</span><br><span class="line">    <span class="comment"># 恢复批大小为32</span></span><br><span class="line">    trn_dl = DataLoader(train, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    validation = FMNISTDataset(validation_data, validation_labels)</span><br><span class="line">    val_dl = DataLoader(validation, batch_size=<span class="built_in">len</span>(validation_data), shuffle=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> trn_dl, val_dl</span><br></pre></td></tr></table></figure>
</li>
<li><p>增加轮数到10以更好地比较</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">	...</span><br></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/01%E4%BD%BF%E7%94%A8SGD%E4%BC%98%E5%8C%96%E5%99%A8.png" alt="image-20240406190505255"></p>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/02%E4%BD%BF%E7%94%A8%E4%BA%9A%E5%BD%93%E4%BC%98%E5%8C%96%E5%99%A8.png" alt="image-20240406190116099"></p>
</li>
</ol>
<p>可以发现，虽然Adam优化器在一开始就取得较好的成绩，但在后面的几轮训练中波动较大，似乎不如SGD优化器稳定</p>
<h1 id="理解不同学习率的影响"><a href="#理解不同学习率的影响" class="headerlink" title="理解不同学习率的影响"></a>理解不同学习率的影响</h1><p>到目前为止，我们一直使用0.01作为学习率，接下来讨论不同学习率对缩放和非缩放数据集的影响</p>
<h2 id="学习率对缩放数据集的影响"><a href="#学习率对缩放数据集的影响" class="headerlink" title="学习率对缩放数据集的影响"></a>学习率对缩放数据集的影响</h2><p>在上一节代码的基础上（SGD优化器），仅修改学习率，查看不同学习率对训练的影响</p>
<h3 id="学习率较大"><a href="#学习率较大" class="headerlink" title="学习率较大"></a>学习率较大</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_model</span>():</span><br><span class="line">    model = nn.Sequential(</span><br><span class="line">        nn.Linear(<span class="number">28</span> * <span class="number">28</span>, <span class="number">1000</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Linear(<span class="number">1000</span>, <span class="number">10</span>)</span><br><span class="line">    ).to(device)</span><br><span class="line">    loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">    <span class="comment"># 修改学习率为0.1</span></span><br><span class="line">    optimizer = SGD(model.parameters(), lr=<span class="number">1e-1</span>)</span><br><span class="line">    <span class="keyword">return</span> model, loss_fn, optimizer</span><br></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/03%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%BA0.1%E5%AF%B9%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%BD%B1%E5%93%8D.png" alt="image-20240406192037177"></p>
<p>注意到损失函数下降速度和准确度提升速度相较学习度为0.01时虽然有所提升，但曲线波动较大，并且相比之下验证值曲线并没有明显优势</p>
<h3 id="学习率适中"><a href="#学习率适中" class="headerlink" title="学习率适中"></a>学习率适中</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改学习率为0.001</span></span><br><span class="line">optimizer = SGD(model.parameters(), lr=<span class="number">1e-3</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/04%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%BA0.001%E5%AF%B9%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%BD%B1%E5%93%8D.png.png" alt="image-20240406192632126"></p>
<p>注意到损失函数下降速度和准确度提升速度相比学习度为0.01时均有所降低，同时曲线相对更为平滑</p>
<h3 id="学习率较小"><a href="#学习率较小" class="headerlink" title="学习率较小"></a>学习率较小</h3><p>可以预见，当学习率到达0.00001时，其权重值更新会非常慢。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改学习率为0.00001</span></span><br><span class="line">optimizer = SGD(model.parameters(), lr=<span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/05%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%BA0.00001%E5%AF%B9%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%BD%B1%E5%93%8D.png" alt="image-20240406200627434"></p>
<h3 id="不同学习率的不同参数分布"><a href="#不同学习率的不同参数分布" class="headerlink" title="不同学习率的不同参数分布"></a>不同学习率的不同参数分布</h3><p>不知道你是否注意到，在不同学习率时训练曲线与验证曲线的接近程度不同，学习率越低，两个曲线越接近。</p>
<p>我们可以对的模型的参数进行分析，以研究其中的规律，我们的模型有以下四种参数：</p>
<ul>
<li>连接输入层和隐藏层的权重</li>
<li>隐藏层中的偏置项</li>
<li>连接隐藏层和输出层的权重</li>
<li>输出层中的偏置项</li>
</ul>
<p>通过如下方法绘制参数分布情况柱状图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> ix, par <span class="keyword">in</span> <span class="built_in">enumerate</span>(model.parameters()):</span><br><span class="line">    <span class="keyword">if</span>(ix==<span class="number">0</span>):</span><br><span class="line">        plt.hist(par.cpu().detach().numpy().flatten())</span><br><span class="line">        plt.title(<span class="string">&#x27;Distribution of weights conencting input to hidden layer&#x27;</span>)</span><br><span class="line">        plt.show()</span><br><span class="line">    <span class="keyword">elif</span>(ix ==<span class="number">1</span>):</span><br><span class="line">        plt.hist(par.cpu().detach().numpy().flatten())</span><br><span class="line">        plt.title(<span class="string">&#x27;Distribution of biases of hidden layer&#x27;</span>)</span><br><span class="line">        plt.show()</span><br><span class="line">    <span class="keyword">elif</span>(ix==<span class="number">2</span>):</span><br><span class="line">        plt.hist(par.cpu().detach().numpy().flatten())</span><br><span class="line">        plt.title(<span class="string">&#x27;Distribution of weights conencting hidden to output layer&#x27;</span>)</span><br><span class="line">        plt.show()</span><br><span class="line">    <span class="keyword">elif</span>(ix ==<span class="number">3</span>):</span><br><span class="line">        plt.hist(par.cpu().detach().numpy().flatten())</span><br><span class="line">        plt.title(<span class="string">&#x27;Distribution of biases of output layer&#x27;</span>)</span><br><span class="line">        plt.show() </span><br></pre></td></tr></table></figure>

<p>以下是按照学习率从大到小三种情况的参数分布情况（注意参数大小）</p>
<ul>
<li>学习率0.1时的参数分布</li>
</ul>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/06%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%BE%83%E5%A4%A7%E6%97%B6%E7%9A%84%E5%8F%82%E6%95%B0%E5%88%86%E5%B8%83.png" alt="image-20240406205241463"></p>
<ul>
<li>学习率0.001时的参数分布</li>
</ul>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/07%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%AD%E7%AD%89%E6%97%B6%E7%9A%84%E5%8F%82%E6%95%B0%E5%88%86%E5%B8%83.png" alt="image-20240406205529494"></p>
<ul>
<li>学习率0.00001时的参数分布</li>
</ul>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/08%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%BE%83%E5%B0%8F%E6%97%B6%E7%9A%84%E5%8F%82%E6%95%B0%E5%88%86%E5%B8%83.png" alt="image-20240406202843548"></p>
<p>可以看出，学习率越大，参数分布范围也越大</p>
<h2 id="不同学习率对非缩放数据集的影响"><a href="#不同学习率对非缩放数据集的影响" class="headerlink" title="不同学习率对非缩放数据集的影响"></a>不同学习率对非缩放数据集的影响</h2><p>在前面的学习中，我们设置学习率为0.01且不缩放变量时，模型无法达到一个理想的准确率，后面通过确保变量被限制在某个较小范围内，提升了模型的准确度。</p>
<p>这次我们探讨一下，如果不缩小变量范围，能否通过改变学习率使模型达到一个理想的准确度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FMNISTDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="comment"># 取消缩放像素点</span></span><br><span class="line">        <span class="comment"># x = x.float() / 255 </span></span><br><span class="line">        x = x.<span class="built_in">float</span>()</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">        self.x, self.y = x, y </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, ix</span>):</span><br><span class="line">        x, y = self.x[ix], self.y[ix] </span><br><span class="line">        <span class="keyword">return</span> x.to(device), y.to(device)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.x)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>不缩小数据且学习率为0.1：</p>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/09%E4%B8%8D%E7%BC%A9%E5%B0%8F%E6%95%B0%E6%8D%AE%E4%B8%94%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%BA0.1.png" alt="image-20240406211916990"></p>
</li>
<li><p>不缩小数据且学习率为0.001</p>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/10%E4%B8%8D%E7%BC%A9%E5%B0%8F%E6%95%B0%E6%8D%AE%E4%B8%94%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%BA0.001.png" alt="image-20240406212334619"></p>
</li>
<li><p>不缩小数据且学习率为0.00001</p>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/11%E4%B8%8D%E7%BC%A9%E5%B0%8F%E6%95%B0%E6%8D%AE%E4%B8%94%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%BA0.00001.png" alt="image-20240406212610297"></p>
</li>
<li><p>当学习率为0.1时，第一轮就使损失值达到了零，自然也无法继续增加精确度</p>
</li>
<li><p>而通过减小学习率，就能直接避免参数更新过大的问题</p>
</li>
<li><p>通过以上观察，猜测缩减数据值的大小和减小学习率有类似的效果</p>
</li>
<li><p>因此我们在训练模型时，尽量不要使用较大的数据值和学习率，猜测学习率在0.001-0.01间时，数据值在[-1, 1]之间是一个比较合适的区间。</p>
</li>
</ul>
<h1 id="理解不同学习率衰减的影响"><a href="#理解不同学习率衰减的影响" class="headerlink" title="理解不同学习率衰减的影响"></a>理解不同学习率衰减的影响</h1><p>通过对不同学习率下曲线的观察，我们也总结出一些规律：</p>
<ul>
<li>学习率较高时，模型收敛速度较快，能在一开始就达到较好效果，但后续波动较大</li>
<li>学习率较低时，模型收敛速度较慢，但曲线平滑，后期准确率会比较高</li>
</ul>
<p>那我们是否能结合学习率的优点，在训练过程中动态修改学习率，<strong>在训练初期学习率大一些，使得网络收敛迅速，在训练后期学习率小一些，使得网络更好的收敛到最优解。</strong></p>
<p>Pytorch提供了一些相关工具，可以使用这些工具动态降低学习率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># factor=0.5 表示每次触发该调节器将使学习率变为当前的0.5倍</span></span><br><span class="line"><span class="comment"># patience=0 表示只要有1次当前损失值没有改善，则触发调节器</span></span><br><span class="line"><span class="comment"># threshold=0.001 阈值表示损失率的改善最低值，低于该值则认为没有改善</span></span><br><span class="line"><span class="comment"># verbose=True 表示将打印学习率变化</span></span><br><span class="line"><span class="comment"># min_lr=1e-5 表示学习率下限为1e-5</span></span><br><span class="line"><span class="comment"># threshold_mode=&#x27;abs&#x27; 表示阈值是绝对值，也可以用&#x27;rel&#x27;表示百分比</span></span><br><span class="line">scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=<span class="number">0.5</span>, patience=<span class="number">0</span>, threshold=<span class="number">0.001</span>, verbose=<span class="literal">True</span>, min_lr=<span class="number">1e-5</span>, threshold_mode=<span class="string">&#x27;abs&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 30轮</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">30</span>):</span><br><span class="line">    <span class="comment"># 这里省略</span></span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> ix, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">iter</span>(validation_dataloader)):</span><br><span class="line">        x, y = batch</span><br><span class="line">        val_is_correct = accuracy(x, y, model)</span><br><span class="line">        validation_loss = val_loss(x, y, model)</span><br><span class="line">        val_epoch_accuracy = np.mean(val_is_correct)</span><br><span class="line">        validation_losses.append(validation_loss)</span><br><span class="line">        validation_accuracies.append(val_epoch_accuracy)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 用于监视每次损失值变化</span></span><br><span class="line">        scheduler.step(validation_loss)</span><br></pre></td></tr></table></figure>



<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/12%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F%E7%9A%84%E6%95%88%E6%9E%9C.png" alt="image-20240406221959090"></p>
<p>可以看出，使用学习率衰减后曲线的波动有所增加，最后取得的成绩也处于一个较好的水平。</p>
<h1 id="构建更深的神经网络"><a href="#构建更深的神经网络" class="headerlink" title="构建更深的神经网络"></a>构建更深的神经网络</h1><p>在本节，对比具有两个隐藏层和没有隐藏层的模型的性能。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>



<h1 id="理解不同批归一化的影响"><a href="#理解不同批归一化的影响" class="headerlink" title="理解不同批归一化的影响"></a>理解不同批归一化的影响</h1><h2 id="没有批归一化的非常小的输入值"><a href="#没有批归一化的非常小的输入值" class="headerlink" title="没有批归一化的非常小的输入值"></a>没有批归一化的非常小的输入值</h2><h2 id="经过批归一化的非常小的输入值"><a href="#经过批归一化的非常小的输入值" class="headerlink" title="经过批归一化的非常小的输入值"></a>经过批归一化的非常小的输入值</h2><h1 id="过拟合的概念"><a href="#过拟合的概念" class="headerlink" title="过拟合的概念"></a>过拟合的概念</h1><ol>
<li>过拟合（Overfitting）： 过拟合指的是模型在训练数据上表现得过于优秀，但在未见过的测试数据上表现不佳的情况。具体来说，过拟合通常表现为模型对训练数据中的噪声和随机变化过于敏感，导致模型过度地“记忆”了训练数据的特性，而未能学习到泛化到新数据的规律。过拟合的原因通常是模型过于复杂，参数数量过多，导致模型的学习能力过强，容易过度拟合训练数据。</li>
<li>欠拟合（Underfitting）： 欠拟合指的是模型在训练数据和测试数据上的表现都不佳的情况。具体来说，欠拟合通常表现为模型不能很好地拟合训练数据中的真实关系，表现出的拟合程度不足，无法捕捉到数据的一般规律。欠拟合的原因通常是模型过于简单，学习能力不足，或者特征量不足，无法很好地描述数据的复杂性。</li>
<li>解决过拟合和欠拟合问题的方法包括：<ul>
<li>过拟合：增加训练数据量、简化模型复杂度（如减少参数数量、增加正则化）、使用更多的特征工程、使用集成学习方法等。</li>
<li>欠拟合：增加模型复杂度（如增加参数数量、增加模型的层数）、增加更多的特征、改进模型算法等。</li>
</ul>
</li>
</ol>
<h2 id="添加dropout的影响"><a href="#添加dropout的影响" class="headerlink" title="添加dropout的影响"></a>添加dropout的影响</h2><h2 id="正则化的影响"><a href="#正则化的影响" class="headerlink" title="正则化的影响"></a>正则化的影响</h2><h3 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h3><h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3>
    </div>
     
    <div class="post-footer__meta"><p>更新于 2024-04-07</p></div> 
    <div class="post-entry__tags"></div> 
</article>


    <div class="nav">
        <div class="nav__prev">
            
                <a href="/Deep%20Learning/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/" class="nav__link">
                    <div>
                        <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="24" height="24"><path d="M589.088 790.624L310.464 512l278.624-278.624 45.248 45.248L400.96 512l233.376 233.376z" fill="#808080"></path></svg>
                    </div>
                    <div>
                        <div class="nav__label">
                            上一篇
                        </div>
                        <div class="nav__title">
                            03.使用pytorch构建神经网络（上）
                        </div>
                    </div>
                </a>
            
        </div>
        <div class="nav__next">
            
                <a href="/Deep%20Learning/02.pytorch%E5%9F%BA%E7%A1%80/" class="nav__link">
                    <div>
                        <div class="nav__label">
                            下一篇
                        </div>
                        <div class="nav__title">
                            02.pytorch基础
                        </div>
                    </div>
                    <div>
                        <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="24" height="24"><path d="M434.944 790.624l-45.248-45.248L623.04 512l-233.376-233.376 45.248-45.248L713.568 512z" fill="#808080"></path></svg>
                    </div>
                </a>
            
        </div>
    </div>





</main>

            <footer class="footer">
    
    


    
     
 

 
    
        
        <p class="footer-copyright">
            Copyright © 2024 <a href="/">My Learning Notes</a>
        </p>
    
    
    <p>Powered by <a href="https://hexo.io" target="_blank">Hexo</a> | Theme - <a href="https://github.com/ChrAlpha/hexo-theme-cards" target="_blank">Cards</a></p>
</footer>

        </div>
         

 

 

 

 



 



 


    
 

 

 

 

 

 




    </body>
</html>
