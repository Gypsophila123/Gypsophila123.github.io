<!DOCTYPE html>
<html lang="zh-CN">
    <head>
    <meta charset="utf-8">

    

    <!-- 渲染优化 -->
    <meta name="renderer" content="webkit">
    <meta name="force-rendering" content="webkit">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="HandheldFriendly" content="True" >
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <!--icon-->

    
        <link rel="shortcut icon" href="/github.ico">
    
    
        <link rel="icon" type="image/png" sizes="16x16" href="/github_small.png">
    
    
        <link rel="icon" type="image/png" sizes="32x32" href="/github_medium.png">
    
    
    


    <!-- meta -->


<title>03.使用pytorch构建深度神经网络（上） | 学习笔记</title>


    <meta name="keywords" content="Deep Learning">




    <!-- OpenGraph -->
 
    <meta name="description" content="[TOC] 表示图像 图像文件是由像素组成的，像素是构成图像的最小元素 在灰度图像上，每个像素灰度取值为0-255，0表示黑，255表示白 彩色像素中每个像素是一个三位向量，每个分量分别表示rgb  将图像转化为结构化数组和标量Python中很容易将图像转化为数组，接下来使用cv2从磁盘读取图像，使用matplotlib将图像绘制出，原始图像如下 灰度图像import cv2, matplotli">
<meta property="og:type" content="article">
<meta property="og:title" content="03.使用pytorch构建深度神经网络（上）">
<meta property="og:url" content="https://gypsophila123.github.io/Deep%20Learning/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/">
<meta property="og:site_name" content="学习笔记">
<meta property="og:description" content="[TOC] 表示图像 图像文件是由像素组成的，像素是构成图像的最小元素 在灰度图像上，每个像素灰度取值为0-255，0表示黑，255表示白 彩色像素中每个像素是一个三位向量，每个分量分别表示rgb  将图像转化为结构化数组和标量Python中很容易将图像转化为数组，接下来使用cv2从磁盘读取图像，使用matplotlib将图像绘制出，原始图像如下 灰度图像import cv2, matplotli">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gypsophila123.github.io/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/01%E6%B5%8B%E8%AF%95%E5%9B%BE%E5%83%8F.jpeg">
<meta property="og:image" content="https://gypsophila123.github.io/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/02%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F.png">
<meta property="og:image" content="https://gypsophila123.github.io/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/03%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F%E5%8E%8B%E7%BC%A9.png">
<meta property="og:image" content="https://gypsophila123.github.io/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/04%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F.png">
<meta property="og:image" content="https://gypsophila123.github.io/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/05%E9%9A%8F%E6%9C%BA%E5%B1%95%E7%A4%BA100%E5%BC%A0%E6%A0%B7%E6%9C%AC.png">
<meta property="og:image" content="https://gypsophila123.github.io/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/06%E6%8D%9F%E5%A4%B1%E6%9B%B2%E7%BA%BF.png">
<meta property="og:image" content="https://gypsophila123.github.io/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/07%E5%87%86%E7%A1%AE%E5%BA%A6.png">
<meta property="og:image" content="https://gypsophila123.github.io/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/08%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E6%8D%9F%E5%A4%B1%E6%9B%B2%E7%BA%BF.png">
<meta property="og:image" content="https://gypsophila123.github.io/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/09%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E7%A1%AE%E5%BA%A6.png">
<meta property="og:image" content="https://gypsophila123.github.io/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/10%E6%89%B9%E5%A4%A7%E5%B0%8F32%E7%9A%84%E6%8D%9F%E5%A4%B1%E6%9B%B2%E7%BA%BF.png">
<meta property="og:image" content="https://gypsophila123.github.io/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/11%E6%89%B9%E5%A4%A7%E5%B0%8F32%E7%9A%84%E5%87%86%E7%A1%AE%E5%BA%A6%E6%9B%B2%E7%BA%BF.png">
<meta property="og:image" content="https://gypsophila123.github.io/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/12%E6%89%B9%E5%A4%A7%E5%B0%8F10000%E7%9A%84%E6%8D%9F%E5%A4%B1%E6%9B%B2%E7%BA%BF.png">
<meta property="og:image" content="https://gypsophila123.github.io/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/13%E6%89%B9%E5%A4%A7%E5%B0%8F10000%E7%9A%84%E5%87%86%E7%A1%AE%E5%BA%A6%E6%9B%B2%E7%BA%BF.png">
<meta property="og:image" content="https://gypsophila123.github.io/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/01%E4%BD%BF%E7%94%A8SGD%E4%BC%98%E5%8C%96%E5%99%A8.png">
<meta property="og:image" content="https://gypsophila123.github.io/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/02%E4%BD%BF%E7%94%A8%E4%BA%9A%E5%BD%93%E4%BC%98%E5%8C%96%E5%99%A8.png">
<meta property="article:published_time" content="2024-04-11T09:09:45.304Z">
<meta property="article:modified_time" content="2024-04-11T09:09:45.657Z">
<meta property="article:author" content="yuanjie">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://gypsophila123.github.io/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/01%E6%B5%8B%E8%AF%95%E5%9B%BE%E5%83%8F.jpeg">


    
<link rel="stylesheet" href="/css/style/main.css">
 

    
    
    
        <link rel="stylesheet" id="hl-default-theme" href="https://cdn.jsdelivr.net/npm/highlight.js@10.1.2/styles/default.min.css" media="none" >
        
            <link rel="stylesheet" id="hl-dark-theme" href="https://cdn.jsdelivr.net/npm/highlight.js@10.1.2/styles/dark.min.css" media="none">
        
    

    

    
    
<link rel="stylesheet" href="/css/style/dark.css">

    
<script src="/js/darkmode.js"></script>



     

    <!-- custom head -->

<meta name="generator" content="Hexo 7.1.1"></head>

    <body>
        <div id="app" tabindex="-1">
            <header class="header">
    <div class="header__left">
        <a href="/" class="button">
            <span class="logo__text">学习笔记</span>
        </a>
    </div>
    <div class="header__right">
        
            <div class="navbar__menus">
                
                    <a href="/" class="navbar-menu button">首页</a>
                
                    <a href="/archives/" class="navbar-menu button">归档</a>
                
            </div>
        
        
        

        
        
    <a href="javaScript:void(0);" id="btn-toggle-dark">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg>
    </a>


        
            <a class="dropdown-icon button" id="btn-dropdown" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width='24' height='24' fill="none" stroke="currentColor" stroke-width="0.7" stroke-linecap="round" stroke-linejoin="round"><path fill="currentColor" d="M3.314,4.8h13.372c0.41,0,0.743-0.333,0.743-0.743c0-0.41-0.333-0.743-0.743-0.743H3.314c-0.41,0-0.743,0.333-0.743,0.743C2.571,4.467,2.904,4.8,3.314,4.8z M16.686,15.2H3.314c-0.41,0-0.743,0.333-0.743,0.743s0.333,0.743,0.743,0.743h13.372c0.41,0,0.743-0.333,0.743-0.743S17.096,15.2,16.686,15.2z M16.686,9.257H3.314c-0.41,0-0.743,0.333-0.743,0.743s0.333,0.743,0.743,0.743h13.372c0.41,0,0.743-0.333,0.743-0.743S17.096,9.257,16.686,9.257z"></path></svg></a>
            <div class="dropdown-menus" id="dropdown-menus">
                
                    <a href="/" class="dropdown-menu button">首页</a>
                
                    <a href="/archives/" class="dropdown-menu button">归档</a>
                
            </div>
        
    </div>
</header>


            <main class="main">
    

<div class="post-title">
    <h1 class="post-title__text">
        03.使用pytorch构建深度神经网络（上）
    </h1>
    <div class="post-title__meta">
        <a href="/archives/2024/04/" class="post-meta__date button">2024-04-11</a>
        
    <span class="separate-dot"></span><a href="/categories/deep-learning/" class="button">Deep Learning</a>

 
        
    
    


 

 
    </div>
</div>


    <aside class="post-side">
        <div class="post-side__toc">
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%86%E5%9B%BE%E5%83%8F%E8%BD%AC%E5%8C%96%E4%B8%BA%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E7%BB%84%E5%92%8C%E6%A0%87%E9%87%8F"><span class="toc-number">1.</span> <span class="toc-text">将图像转化为结构化数组和标量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F"><span class="toc-number">1.1.</span> <span class="toc-text">灰度图像</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F"><span class="toc-number">1.2.</span> <span class="toc-text">彩色图像</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95"><span class="toc-number">1.3.</span> <span class="toc-text">问题记录</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FashionMNIST%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.</span> <span class="toc-text">FashionMNIST数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE"><span class="toc-number">3.</span> <span class="toc-text">使用数据集中的数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%B9%E5%A4%A7%E5%B0%8F%E6%94%B9%E4%B8%BA10000"><span class="toc-number">4.</span> <span class="toc-text">批大小改为10000</span></a></li></ol>
        </div>
    </aside>
    <a class="btn-toc button" id="btn-toc" tabindex="0">
        <svg viewBox="0 0 1024 1024" width="32" height="32" xmlns="http://www.w3.org/2000/svg">
            <path d="M128 256h64V192H128zM320 256h576V192H320zM128 544h64v-64H128zM320 544h576v-64H320zM128 832h64v-64H128zM320 832h576v-64H320z" fill="currentColor"></path>
        </svg>
    </a>
    <div class="toc-menus" id="toc-menus">
        <div class="toc-title">文章目录</div>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%86%E5%9B%BE%E5%83%8F%E8%BD%AC%E5%8C%96%E4%B8%BA%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E7%BB%84%E5%92%8C%E6%A0%87%E9%87%8F"><span class="toc-number">1.</span> <span class="toc-text">将图像转化为结构化数组和标量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F"><span class="toc-number">1.1.</span> <span class="toc-text">灰度图像</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F"><span class="toc-number">1.2.</span> <span class="toc-text">彩色图像</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95"><span class="toc-number">1.3.</span> <span class="toc-text">问题记录</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FashionMNIST%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.</span> <span class="toc-text">FashionMNIST数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE"><span class="toc-number">3.</span> <span class="toc-text">使用数据集中的数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%B9%E5%A4%A7%E5%B0%8F%E6%94%B9%E4%B8%BA10000"><span class="toc-number">4.</span> <span class="toc-text">批大小改为10000</span></a></li></ol>
    </div>


<article class="post post__with-toc content-card">
    <div class="post__header"></div>
    <div class="post__content">
        <p>[TOC]</p>
<h1 id="表示图像"><a href="#表示图像" class="headerlink" title="表示图像"></a>表示图像</h1><ul>
<li>图像文件是由像素组成的，像素是构成图像的最小元素</li>
<li>在灰度图像上，每个像素灰度取值为0-255，0表示黑，255表示白</li>
<li>彩色像素中每个像素是一个三位向量，每个分量分别表示rgb</li>
</ul>
<h2 id="将图像转化为结构化数组和标量"><a href="#将图像转化为结构化数组和标量" class="headerlink" title="将图像转化为结构化数组和标量"></a>将图像转化为结构化数组和标量</h2><p>Python中很容易将图像转化为数组，接下来使用cv2从磁盘读取图像，使用matplotlib将图像绘制出，原始图像如下<img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/01%E6%B5%8B%E8%AF%95%E5%9B%BE%E5%83%8F.jpeg" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/01%E6%B5%8B%E8%AF%95%E5%9B%BE%E5%83%8F.jpeg" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="Hemanvi.jpeg"></p>
<h3 id="灰度图像"><a href="#灰度图像" class="headerlink" title="灰度图像"></a>灰度图像</h3><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> cv2, matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-comment"># 使用 cv2.imread 方法，返回像素数组</span><br>img = cv2.imread(<span class="hljs-string">&#x27;test.jpeg&#x27;</span>)<br><span class="hljs-comment"># 数组切片，也可以理解为图像裁切</span><br>img = img[<span class="hljs-number">50</span>:<span class="hljs-number">250</span>,<span class="hljs-number">40</span>:<span class="hljs-number">240</span>]<br><span class="hljs-comment"># 将彩色图像转换为灰度图像（三通道变为单通道）</span><br>img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br>plt.imshow(img_gray, cmap=<span class="hljs-string">&#x27;gray&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/02%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/02%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406090502891"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 将图片压缩至 25 x 25 像素</span><br>img_gray_small = cv2.resize(img_gray, (<span class="hljs-number">25</span>, <span class="hljs-number">25</span>))<br>plt.imshow(img_gray_small, cmap=<span class="hljs-string">&#x27;gray&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/03%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F%E5%8E%8B%E7%BC%A9.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/03%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F%E5%8E%8B%E7%BC%A9.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406090601993"></p>
<h3 id="彩色图像"><a href="#彩色图像" class="headerlink" title="彩色图像"></a>彩色图像</h3><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 将颜色通道从 BGR转为RGB，使图像能正常显示</span><br>img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)<br>plt.imshow(img)<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/04%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/04%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406093836583"></p>
<h3 id="问题记录"><a href="#问题记录" class="headerlink" title="问题记录"></a>问题记录</h3><p>opencv 编码问题不能识别中文路径，可以使用下面两个方法解决</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 读取图像，解决imread不能读取中文路径的问题</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">cv_imread</span>(<span class="hljs-params">path</span>):<br>    <span class="hljs-comment"># 通过np将文件读为一维的二进制数组</span><br>    byte_img = np.fromfile(path, dtype=np.uint8)<br>    <span class="hljs-comment"># 再通过cv解码为二维的图像数组</span><br>    cv_img = cv2.imdecode(byte_img, -<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> cv_img<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">cv_img_rgb</span>(<span class="hljs-params">path</span>):<br>    <span class="hljs-comment"># 用matplotlib读取图像数组</span><br>    img = plt.imread(path)<br>    <span class="hljs-comment"># 转为cv2数组，因为opencv读取是按照BGR的顺序，所以这里需要调换顺序</span><br>    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)<br>    <span class="hljs-keyword">return</span> img_rgb<br></code></pre></td></tr></table></figure>

<h1 id="为什么要用神经网络进行图像分析"><a href="#为什么要用神经网络进行图像分析" class="headerlink" title="为什么要用神经网络进行图像分析"></a>为什么要用神经网络进行图像分析</h1><p>在图像分析中，如果不使用神经网络，也可以利用传统的计算机视觉技术来提取各种特征。这些特征通常是基于图像的局部结构、纹理、颜色等属性而提取出来的。以下是一些常见的特征：</p>
<ol>
<li><strong>颜色特征：</strong> 提取图像中的颜色信息，包括颜色直方图、颜色均值、颜色矩等。</li>
<li><strong>纹理特征：</strong> 描述图像中的纹理信息，包括灰度共生矩阵（GLCM）、局部二值模式（LBP）、方向梯度直方图（HOG）等。</li>
<li><strong>形状特征：</strong> 描述图像中的形状信息，包括边缘检测、轮廓提取、形状描述符等。</li>
<li><strong>局部特征：</strong> 描述图像中的局部结构信息，包括尺度不变特征变换（SIFT）、加速稳定特征变换（SURF）、尺度空间极值检测（DoG）等。</li>
<li><strong>密集特征：</strong> 提取图像中的密集特征点，包括稠密光流、稠密角点等。</li>
</ol>
<p>这些传统的特征提取方法通常需要设计特定的算法来实现，并且对图像的预处理、参数选择等方面有一定的依赖。尽管这些特征提取方法在一些场景下仍然具有一定的效果，但是相比于神经网络，它们通常需要更多的人工设计和调优，并且在处理复杂的图像任务时可能会受到限制。因此，随着深度学习和神经网络的发展，越来越多的图像分析任务倾向于使用神经网络来提取特征和解决问题。</p>
<h1 id="为图像分类准备数据"><a href="#为图像分类准备数据" class="headerlink" title="为图像分类准备数据"></a>为图像分类准备数据</h1><h2 id="FashionMNIST数据集"><a href="#FashionMNIST数据集" class="headerlink" title="FashionMNIST数据集"></a><strong>FashionMNIST数据集</strong></h2><ul>
<li>数据集：FashionMNIST 数据集是一个包含服装和配件图像的数据集，共有 60000 个训练样本和 10000 个测试样本。图像尺寸为 28x28 像素，灰度图像。</li>
<li>训练集标签：训练集标签是一个长度为 60000 的一维数组，每个元素表示对应图像的服装或配件类别，取值范围为 0 到 9，对应的类别包括 T-shirt&#x2F;top、Trouser、Pullover、Dress、Coat、Sandal、Shirt、Sneaker、Bag 和 Ankle boot。</li>
</ul>
<p>你可以在 TensorFlow、PyTorch 等深度学习框架中直接加载 FashionMNIST 数据集，并使用它来训练和评估模型。下面准备这个数据集：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-comment"># This can be any directory you want </span><br>data_folder = <span class="hljs-string">&#x27;dataset&#x27;</span><br><span class="hljs-comment"># 从文件夹加载训练集对象，download=True表示若文件不存在，则自动从网络下载，指定 train=True 来加载训练集，train=False 来加载测试集</span><br>fmnist_train = datasets.FashionMNIST(data_folder, download=<span class="hljs-literal">True</span>, train=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 获取训练集和测试集数据</span><br>train_data = fmnist_train.data  <span class="hljs-comment"># 训练集图像数据 shape([60000, 28, 28])</span><br>train_labels = fmnist_train.targets  <span class="hljs-comment"># 训练集标签 shape([60000])</span><br></code></pre></td></tr></table></figure>

<p>FashionMNIST 数据集对象通常还具有其他一些有用的属性和方法，例如：</p>
<ol>
<li><strong><code>fmnist_train.transform</code>：</strong> 该属性存储了应用于数据集中图像的转换方法。在训练模型时，通常会对图像数据进行预处理和增强，例如归一化、随机翻转等操作。<code>transform</code> 属性可以指定这些预处理和增强操作。</li>
<li><strong><code>fmnist_train.classes</code></strong> 是一个包含数据集中所有类别标签的列表，每个元素对应一个类别标签。</li>
<li><strong><code>fmnist_train.dataset</code>：</strong> 该属性存储了数据集的原始数据。通常情况下，你不需要直接使用这个属性，而是通过 <code>fmnist_train.data</code> 和 <code>fmnist_train.targets</code> 来访问图像数据和标签。</li>
<li><strong><code>fmnist_train.loader</code>：</strong> 该属性存储了数据集的数据加载器。数据加载器用于在训练模型时批量加载数据，并提供了数据的迭代器接口，方便训练模型。</li>
</ol>
<h2 id="使用数据集中的数据"><a href="#使用数据集中的数据" class="headerlink" title="使用数据集中的数据"></a>使用数据集中的数据</h2><p>从所有的10个类别中分别随机拿出10个样本：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>R, C = <span class="hljs-built_in">len</span>(train_labels.unique()), <span class="hljs-number">10</span><br><span class="hljs-comment"># 创建一个 RxC 的子图网格，返回一个包含所有子图的 Figure 对象和 Axes 对象的数组。</span><br>fig, ax = plt.subplots(R, C, figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>))<br><span class="hljs-comment"># 使用 Axes 对象来绘制每个子图的内容</span><br><span class="hljs-keyword">for</span> label_class, plot_row <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(ax):<br>    <span class="hljs-comment"># 筛选train_labels中标签为指定标签元素的索引位置</span><br>    label_x_rows = np.where(train_labels == label_class)[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">for</span> plot_cell <span class="hljs-keyword">in</span> plot_row:<br>        plot_cell.grid(<span class="hljs-literal">False</span>); plot_cell.axis(<span class="hljs-string">&#x27;off&#x27;</span>)<br>        <span class="hljs-comment"># 随机选择一个索引</span><br>        ix = np.random.choice(label_x_rows)<br>        <span class="hljs-comment"># 拿取图片（实际为一个 28 * 28 数组）</span><br>        x, y = train_data[ix], train_labels[ix]<br>        plot_cell.imshow(x, cmap=<span class="hljs-string">&#x27;gray&#x27;</span>)<br>plt.tight_layout()<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/05%E9%9A%8F%E6%9C%BA%E5%B1%95%E7%A4%BA100%E5%BC%A0%E6%A0%B7%E6%9C%AC.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/05%E9%9A%8F%E6%9C%BA%E5%B1%95%E7%A4%BA100%E5%BC%A0%E6%A0%B7%E6%9C%AC.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406113435044"></p>
<h1 id="训练神经网络"><a href="#训练神经网络" class="headerlink" title="训练神经网络"></a>训练神经网络</h1><p>前面的章节中我们已经学习过如何构建一个神经网络模型，现在我们又获得了准备好的数据集，接下来我们可以正式开始训练一个神经网络了，训练一个神经网络的步骤为：</p>
<ul>
<li><p>准备好环境，并导入相关程序包</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset, DataLoader<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span><br></code></pre></td></tr></table></figure>
</li>
<li><p>构建一个数据集，要求该数据集能一次获取一个数据点（能用于Dataset类）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python">data_folder = <span class="hljs-string">&#x27;dataset&#x27;</span><br>fmnist_train = datasets.FashionMNIST(data_folder, download=<span class="hljs-literal">True</span>, train=<span class="hljs-literal">True</span>)<br><br>train_data = fmnist_train.data<br>train_labels = fmnist_train.targets<br></code></pre></td></tr></table></figure>
</li>
<li><p>将DataLoader包装到数据集中以满足分批训练的需求</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FMNISTDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, y</span>):<br>        x = x.<span class="hljs-built_in">float</span>()<br>        x = x.view(-<span class="hljs-number">1</span>,<span class="hljs-number">28</span>*<span class="hljs-number">28</span>)<br>        self.x, self.y = x, y <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, ix</span>):<br>        x, y = self.x[ix], self.y[ix] <br>        <span class="hljs-keyword">return</span> x.to(device), y.to(device)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>): <br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.x)<br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_data</span>(): <br>    train = FMNISTDataset(train_data, train_labels) <br>    <span class="hljs-comment"># 指定批大小为 32；shuffle=True 表示对数据随机洗牌，用于训练阶段</span><br>    trn_dl = DataLoader(train, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">return</span> trn_dl<br><br>train_dataloader = get_data()<br><span class="hljs-comment"># for data in train_dataloader:</span><br></code></pre></td></tr></table></figure>
</li>
<li><p>构建一个训练模型，定义损失函数和优化器</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> SGD<br><br><span class="hljs-comment"># 自定义模型</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_model</span>():<br>    <span class="hljs-comment"># 使用Sequential设定模型结构（前向传播过程）</span><br>    model = nn.Sequential(<br>        <span class="hljs-comment"># 一个隐藏层，隐藏层包含1000个神经元</span><br>        nn.Linear(<span class="hljs-number">28</span> * <span class="hljs-number">28</span>, <span class="hljs-number">1000</span>),<br>        nn.ReLU(),<br>        <span class="hljs-comment"># 输出结果为长度等于10的张量</span><br>        nn.Linear(<span class="hljs-number">1000</span>, <span class="hljs-number">10</span>)<br>    ).to(device)<br>    <span class="hljs-comment"># 交叉熵损失函数</span><br>    loss_fn = nn.CrossEntropyLoss()<br>    <span class="hljs-comment"># 优化器，随机梯度下降算法，学习率为0.01</span><br>    optimizer = SGD(model.parameters(), lr=<span class="hljs-number">1e-2</span>)<br>    <span class="hljs-keyword">return</span> model, loss_fn, optimizer<br><br>model, loss_fn, optimizer = get_model()<br></code></pre></td></tr></table></figure>
</li>
<li><p>定义两个函数用来训练和验证数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_batch</span>(<span class="hljs-params">x, y, model, opt, loss_fn</span>):<br>    <span class="hljs-comment"># 设置模型为训练模式，会启用 Dropout 层</span><br>    model.train() <br>    <span class="hljs-comment"># 前向传播</span><br>    prediction = model(x)<br>    <span class="hljs-comment"># 计算损失值</span><br>    batch_loss = loss_fn(prediction, y)<br>    <span class="hljs-comment"># 反向传播</span><br>    batch_loss.backward()<br>    <span class="hljs-comment"># 更新权重</span><br>    optimizer.step()<br>    <span class="hljs-comment"># 梯度值置零以准备下一轮</span><br>    optimizer.zero_grad()<br>    <span class="hljs-keyword">return</span> batch_loss.item()<br><br><span class="hljs-comment"># 不需要计算梯度</span><br><span class="hljs-meta">@torch.no_grad()</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">accuracy</span>(<span class="hljs-params">x, y, model</span>):<br>    <span class="hljs-comment"># 设置模型为评估模式</span><br>    model.<span class="hljs-built_in">eval</span>() <br>    prediction = model(x)<br>    <span class="hljs-comment"># argmaxes为最大值所在的索引</span><br>    max_values, argmaxes = prediction.<span class="hljs-built_in">max</span>(-<span class="hljs-number">1</span>)<br>	<span class="hljs-comment"># 将预测的类别索引与真实标签进行比较，得到一个布尔型张量，表示每个样本的预测结果是否正确。</span><br>    is_correct = argmaxes == y<br>    <span class="hljs-keyword">return</span> is_correct.cpu().numpy().tolist()<br></code></pre></td></tr></table></figure>
</li>
<li><p>记录每轮的损失值和精确度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 记录准确度和损失值</span><br>losses, accuracies = [], []<br><span class="hljs-comment"># 定义轮数</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>    <span class="hljs-built_in">print</span>(epoch)<br>    <span class="hljs-comment"># 记录本轮损失值和准确度</span><br>    epoch_losses, epoch_accuracies = [], []<br>	<span class="hljs-comment"># 迭代数据加载器</span><br>    <span class="hljs-keyword">for</span> ix, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">iter</span>(train_dataloader)):<br>        x, y = batch<br>        batch_loss = train_batch(x, y, model, optimizer, loss_fn)<br>        epoch_losses.append(batch_loss)<br>    <span class="hljs-comment"># 训练完一轮，计算本轮平均损失值</span><br>    epoch_loss = np.array(epoch_losses).mean()<br>    <span class="hljs-comment"># 再来一轮计算准确度</span><br>    <span class="hljs-keyword">for</span> ix, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">iter</span>(train_dataloader)):<br>        x, y = batch<br>        is_correct = accuracy(x, y, model)<br>        epoch_accuracies.extend(is_correct)<br>    <span class="hljs-comment"># 计算这一轮平均准确度</span><br>    epoch_accuracy = np.mean(epoch_accuracies)<br>    losses.append(epoch_loss)<br>    accuracies.append(epoch_accuracy)<br></code></pre></td></tr></table></figure>
</li>
<li><p>随着轮数增加，根据每批数据的训练情况调整权重</p>
</li>
</ul>
<p>使用以下代码显示关于轮数的曲线变化</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python">epochs = np.arange(<span class="hljs-number">5</span>)+<span class="hljs-number">1</span><br>plt.figure(figsize=(<span class="hljs-number">20</span>,<span class="hljs-number">5</span>))<br>plt.subplot(<span class="hljs-number">121</span>)<br>plt.title(<span class="hljs-string">&#x27;Loss value over increasing epochs&#x27;</span>)<br>plt.plot(epochs, losses, label=<span class="hljs-string">&#x27;Training Loss&#x27;</span>)<br>plt.legend()<br>plt.subplot(<span class="hljs-number">122</span>)<br>plt.title(<span class="hljs-string">&#x27;Accuracy value over increasing epochs&#x27;</span>)<br>plt.plot(epochs, accuracies, label=<span class="hljs-string">&#x27;Training Accuracy&#x27;</span>)<br>plt.gca().set_yticklabels([<span class="hljs-string">&#x27;&#123;:.0f&#125;%&#x27;</span>.<span class="hljs-built_in">format</span>(x*<span class="hljs-number">100</span>) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> plt.gca().get_yticks()]) <br>plt.legend()<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/06%E6%8D%9F%E5%A4%B1%E6%9B%B2%E7%BA%BF.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/06%E6%8D%9F%E5%A4%B1%E6%9B%B2%E7%BA%BF.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406155529253"></p>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/07%E5%87%86%E7%A1%AE%E5%BA%A6.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/07%E5%87%86%E7%A1%AE%E5%BA%A6.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406155542459"></p>
<p>可以发现，五轮训练后准确度依然维持在一个较低的水平，而损失值的变化早已趋近平滑。换句话说，无论再进行多少轮训练，模型都不太可能达到一个理想的准确度，接下来就需要我们进行对各种超参数的调整来完善该模型。</p>
<h1 id="归一化处理以提升模型准确度"><a href="#归一化处理以提升模型准确度" class="headerlink" title="归一化处理以提升模型准确度"></a>归一化处理以提升模型准确度</h1><p>归一化处理是确保变量被限制在某个有限范围内的过程，通常是0到1或者-1到1之间。在上面的数据集中，我们可以通过对所有自变量的值除以已知的最大值——255，将自变量的值限制在 0-1 之间。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 修改 FMNISTDataset 类，获取自变量时将所有像素点除以255</span><br><span class="hljs-keyword">class</span>  <span class="hljs-title class_">FMNISTDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, y</span>):<br>        <span class="hljs-comment"># x = x.float()</span><br>        x = x.<span class="hljs-built_in">float</span>() / <span class="hljs-number">255</span> <span class="hljs-comment"># 缩放像素点，保证值在 0-1 之间</span><br>        x = x.view(-<span class="hljs-number">1</span>,<span class="hljs-number">28</span>*<span class="hljs-number">28</span>)<br>        self.x, self.y = x, y <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, ix</span>):<br>        x, y = self.x[ix], self.y[ix] <br>        <span class="hljs-keyword">return</span> x.to(device), y.to(device)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>): <br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.x)<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/08%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E6%8D%9F%E5%A4%B1%E6%9B%B2%E7%BA%BF.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/08%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E6%8D%9F%E5%A4%B1%E6%9B%B2%E7%BA%BF.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406162038810"></p>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/09%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E7%A1%AE%E5%BA%A6.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/09%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E7%A1%AE%E5%BA%A6.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406162047439"></p>
<p>在通过对数据集进行缩放后，损失值的减小放缓，同时准确度也达到了一个相对理想的状态。</p>
<p>归一化处理在深度学习中具有多方面的好处，下面我将详细介绍其主要优点：</p>
<ol>
<li><strong>加速模型收敛速度：</strong> 归一化处理使得输入数据的数值范围被缩放到一个较小的区间内，这有助于避免梯度消失或梯度爆炸问题，从而加速模型的收敛速度。当输入数据的尺度较小时，梯度下降的更新步长也会更加合适，模型参数可以更快地收敛到最优解。</li>
<li><strong>提高模型泛化能力：</strong> 归一化处理有助于减少不同特征之间的尺度差异，使得模型更加关注数据中重要的模式和特征。这有助于提高模型的泛化能力，使其在未见过的数据上表现更好，减少过拟合的风险。</li>
<li><strong>增加模型的稳定性：</strong> 归一化处理可以减少不同批次数据之间的方差，使得模型在训练过程中更加稳定。这可以降低训练过程中参数更新的波动，防止模型陷入局部最优解。</li>
<li><strong>提高梯度下降的效率：</strong> 归一化处理可以使得优化算法更加高效，因为它能够更快地找到全局最优解。当输入数据的尺度被缩放到一个合适的范围后，梯度下降算法可以更快地收敛到最优解，从而提高了训练的效率。</li>
<li><strong>抵抗不同输入分布的影响：</strong> 归一化处理使得模型更加鲁棒，能够适应不同的输入数据分布。无论是标准正态分布还是其他分布，归一化处理都能够使得模型更容易学习到数据的模式，提高了模型的通用性。</li>
</ol>
<h1 id="理解不同批大小的影响"><a href="#理解不同批大小的影响" class="headerlink" title="理解不同批大小的影响"></a>理解不同批大小的影响</h1><p>在一批中会更新一个权重，前面的代码中每轮更新了 60000 &#x2F; 32 次权重，下面我们来通过修改批大小来展现不同批大小时对训练效果的影响。</p>
<p>不过在此之前，我们需要使用验证数据来观察模型对未知数据的准确度，以下对验证模型准确度相关的代码进行修改：</p>
<ol>
<li><p>使用验证数据集验证而不是原始训练集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 使用验证集</span><br>fmnist_validation = datasets.FashionMNIST(data_folder, download=<span class="hljs-literal">True</span>, train=<span class="hljs-literal">False</span>)<br><br>validation_data = fmnist_validation.data<br>validation_labels = fmnist_validation.targets<br></code></pre></td></tr></table></figure>
</li>
<li><p>获取验证数据集时不进行洗牌，并且一次加载完所有数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_data</span>(): <br>    train = FMNISTDataset(train_data, train_labels)<br>    <span class="hljs-comment"># shuffle=True 表示对数据随机洗牌，用于训练阶段</span><br>    trn_dl = DataLoader(train, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-literal">True</span>)<br>    <br>    validation = FMNISTDataset(validation_data, validation_labels)<br>    <span class="hljs-comment"># 指定批大小为数据集长度 shuffle=False 表示不进行洗牌，用于验证阶段</span><br>    val_dl = DataLoader(validation, batch_size=<span class="hljs-built_in">len</span>(validation_data), shuffle=<span class="hljs-literal">False</span>)<br>    <span class="hljs-keyword">return</span> trn_dl, val_dl<br><br>train_dataloader, validation_dataloader = get_data()<br><span class="hljs-comment"># for data in train_dataloader:</span><br><span class="hljs-comment"># for data in validation_dataloader:</span><br></code></pre></td></tr></table></figure>
</li>
<li><p>给验证集一个新的损失值计算函数，因为训练集中的损失值由模型自动计算</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 验证集需要单独的损失值计算</span><br><span class="hljs-meta">@torch.no_grad()</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">val_loss</span>(<span class="hljs-params">x, y, model</span>):<br>    model.<span class="hljs-built_in">eval</span>()<br>    prediction = model(x)<br>    val_loss = loss_fn(prediction, y)<br>    <span class="hljs-keyword">return</span> val_loss.item()<br></code></pre></td></tr></table></figure>
</li>
<li><p>将上面的更新应用于循环逻辑中</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 记录验证集数据</span><br>validation_losses, validation_accuracies = [], []<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>    <span class="hljs-built_in">print</span>(epoch)<br>    epoch_losses, epoch_accuracies = [], []<br>    <span class="hljs-keyword">for</span> ix, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">iter</span>(train_dataloader)):<br>        x, y = batch<br>        batch_loss = train_batch(x, y, model, optimizer, loss_fn)<br>        epoch_losses.append(batch_loss)<br>    epoch_loss = np.array(epoch_losses).mean()<br>    <span class="hljs-keyword">for</span> ix, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">iter</span>(train_dataloader)):<br>        x, y = batch<br>        is_correct = accuracy(x, y, model)<br>        epoch_accuracies.extend(is_correct)<br>    epoch_accuracy = np.mean(epoch_accuracies)<br>    losses.append(epoch_loss)<br>    accuracies.append(epoch_accuracy)<br>    <br>    <span class="hljs-comment"># 每轮结束再对验证集计算准确度和损失值</span><br>    <span class="hljs-keyword">for</span> ix, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">iter</span>(validation_dataloader)):<br>        x, y = batch<br>        val_is_correct = accuracy(x, y, model)<br>        validation_loss = val_loss(x, y, model)<br>        val_epoch_accuracy = np.mean(val_is_correct)<br>        validation_losses.append(validation_loss)<br>        validation_accuracies.append(val_epoch_accuracy)<br></code></pre></td></tr></table></figure></li>
</ol>
<p>下面是训练结果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python">epochs = np.arange(<span class="hljs-number">5</span>)+<span class="hljs-number">1</span><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> matplotlib.ticker <span class="hljs-keyword">as</span> mticker<br>%matplotlib inline<br>plt.subplot(<span class="hljs-number">211</span>)<br>plt.plot(epochs, losses, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training loss&#x27;</span>)<br>plt.plot(epochs, validation_losses, <span class="hljs-string">&#x27;r&#x27;</span>, label=<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(<span class="hljs-number">1</span>))<br>plt.title(<span class="hljs-string">&#x27;Training and validation loss when batch size is 32&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Loss&#x27;</span>)<br>plt.legend()<br>plt.grid(<span class="hljs-string">&#x27;off&#x27;</span>)<br>plt.show()<br>plt.subplot(<span class="hljs-number">212</span>)<br>plt.plot(epochs, accuracies, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training accuracy&#x27;</span>)<br>plt.plot(epochs, validation_accuracies, <span class="hljs-string">&#x27;r&#x27;</span>, label=<span class="hljs-string">&#x27;Validation accuracy&#x27;</span>)<br>plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(<span class="hljs-number">1</span>))<br>plt.title(<span class="hljs-string">&#x27;Training and validation accuracy when batch size is 32&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Accuracy&#x27;</span>)<br>plt.gca().set_yticks(plt.gca().get_yticks())<br>plt.gca().set_yticklabels([<span class="hljs-string">&#x27;&#123;:.0f&#125;%&#x27;</span>.<span class="hljs-built_in">format</span>(x*<span class="hljs-number">100</span>) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> plt.gca().get_yticks()]) <br>plt.legend()<br>plt.grid(<span class="hljs-string">&#x27;off&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/10%E6%89%B9%E5%A4%A7%E5%B0%8F32%E7%9A%84%E6%8D%9F%E5%A4%B1%E6%9B%B2%E7%BA%BF.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/10%E6%89%B9%E5%A4%A7%E5%B0%8F32%E7%9A%84%E6%8D%9F%E5%A4%B1%E6%9B%B2%E7%BA%BF.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406181505934"></p>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/11%E6%89%B9%E5%A4%A7%E5%B0%8F32%E7%9A%84%E5%87%86%E7%A1%AE%E5%BA%A6%E6%9B%B2%E7%BA%BF.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/11%E6%89%B9%E5%A4%A7%E5%B0%8F32%E7%9A%84%E5%87%86%E7%A1%AE%E5%BA%A6%E6%9B%B2%E7%BA%BF.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406181518383"></p>
<h2 id="批大小改为10000"><a href="#批大小改为10000" class="headerlink" title="批大小改为10000"></a>批大小改为10000</h2><p>只需修改以下函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_data</span>(): <br>    train = FMNISTDataset(train_data, train_labels)<br>    <span class="hljs-comment"># 将批大小改为10000</span><br>    trn_dl = DataLoader(train, batch_size=<span class="hljs-number">10000</span>, shuffle=<span class="hljs-literal">True</span>)<br>    <br>    validation = FMNISTDataset(validation_data, validation_labels)<br>    val_dl = DataLoader(validation, batch_size=<span class="hljs-built_in">len</span>(validation_data), shuffle=<span class="hljs-literal">False</span>)<br>    <span class="hljs-keyword">return</span> trn_dl, val_dl<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/12%E6%89%B9%E5%A4%A7%E5%B0%8F10000%E7%9A%84%E6%8D%9F%E5%A4%B1%E6%9B%B2%E7%BA%BF.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/12%E6%89%B9%E5%A4%A7%E5%B0%8F10000%E7%9A%84%E6%8D%9F%E5%A4%B1%E6%9B%B2%E7%BA%BF.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406181918016"></p>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/13%E6%89%B9%E5%A4%A7%E5%B0%8F10000%E7%9A%84%E5%87%86%E7%A1%AE%E5%BA%A6%E6%9B%B2%E7%BA%BF.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/13%E6%89%B9%E5%A4%A7%E5%B0%8F10000%E7%9A%84%E5%87%86%E7%A1%AE%E5%BA%A6%E6%9B%B2%E7%BA%BF.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406181946292"></p>
<p>可以发现批大小为10000时准确度远远低于批大小为32时的准确度，这是因为批大小32时每轮进行了1875次权重更新，而批大小10000时每轮只能进行6次权重更新。</p>
<h1 id="理解不同损失优化器的影响"><a href="#理解不同损失优化器的影响" class="headerlink" title="理解不同损失优化器的影响"></a>理解不同损失优化器的影响</h1><p>在此前，我们一直使用<strong>随机梯度下降法（Stochastic Gradient Descent, SGD）</strong>作为优化方法，在本节，我们将使用亚当优化器或者说自适应矩估计算法（Adaptive Moment Estimation, Adam） 作为优化算法，测试不同优化器的影响，我们接下来需要：</p>
<ol>
<li><p>导入并修改优化器为 Adam</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> SGD, Adam<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_model</span>():<br>    model = nn.Sequential(<br>        nn.Linear(<span class="hljs-number">28</span> * <span class="hljs-number">28</span>, <span class="hljs-number">1000</span>),<br>        nn.ReLU(),<br>        nn.Linear(<span class="hljs-number">1000</span>, <span class="hljs-number">10</span>)<br>    ).to(device)<br>    loss_fn = nn.CrossEntropyLoss()<br>    <span class="hljs-comment"># 使用Adam优化器</span><br>    optimizer = Adam(model.parameters(), lr=<span class="hljs-number">1e-2</span>)<br>    <span class="hljs-keyword">return</span> model, loss_fn, optimizer<br></code></pre></td></tr></table></figure>
</li>
<li><p>将训练时的批大小恢复为32</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_data</span>(): <br>    train = FMNISTDataset(train_data, train_labels)<br>    <span class="hljs-comment"># 恢复批大小为32</span><br>    trn_dl = DataLoader(train, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-literal">True</span>)<br>    <br>    validation = FMNISTDataset(validation_data, validation_labels)<br>    val_dl = DataLoader(validation, batch_size=<span class="hljs-built_in">len</span>(validation_data), shuffle=<span class="hljs-literal">False</span>)<br>    <span class="hljs-keyword">return</span> trn_dl, val_dl<br></code></pre></td></tr></table></figure>
</li>
<li><p>增加轮数到10以更好地比较</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>	...<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/01%E4%BD%BF%E7%94%A8SGD%E4%BC%98%E5%8C%96%E5%99%A8.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/01%E4%BD%BF%E7%94%A8SGD%E4%BC%98%E5%8C%96%E5%99%A8.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406190505255"></p>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/02%E4%BD%BF%E7%94%A8%E4%BA%9A%E5%BD%93%E4%BC%98%E5%8C%96%E5%99%A8.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/02%E4%BD%BF%E7%94%A8%E4%BA%9A%E5%BD%93%E4%BC%98%E5%8C%96%E5%99%A8.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406190116099"></p>
</li>
</ol>
<p>可以发现，虽然Adam优化器在一开始就取得较好的成绩，但在后面的几轮训练中波动较大，似乎不如SGD优化器稳定</p>

    </div>
     
    <div class="post-footer__meta"><p>更新于 2024-04-11</p></div> 
    <div class="post-entry__tags"></div> 
</article>


    <div class="nav">
        <div class="nav__prev">
            
                <a href="/Deep%20Learning/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/" class="nav__link">
                    <div>
                        <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="24" height="24"><path d="M589.088 790.624L310.464 512l278.624-278.624 45.248 45.248L400.96 512l233.376 233.376z" fill="#808080"></path></svg>
                    </div>
                    <div>
                        <div class="nav__label">
                            上一篇
                        </div>
                        <div class="nav__title">
                            03.使用pytorch构建深度神经网络（下）
                        </div>
                    </div>
                </a>
            
        </div>
        <div class="nav__next">
            
                <a href="/Deep%20Learning/02.pytorch%E5%9F%BA%E7%A1%80/" class="nav__link">
                    <div>
                        <div class="nav__label">
                            下一篇
                        </div>
                        <div class="nav__title">
                            02.pytorch基础
                        </div>
                    </div>
                    <div>
                        <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="24" height="24"><path d="M434.944 790.624l-45.248-45.248L623.04 512l-233.376-233.376 45.248-45.248L713.568 512z" fill="#808080"></path></svg>
                    </div>
                </a>
            
        </div>
    </div>





</main>

            <footer class="footer">
     
    <a href="#" class="button" id="b2t" aria-label="回到顶部" title="回到顶部">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" width="32" height="32">
            <path d="M233.376 722.752L278.624 768 512 534.624 745.376 768l45.248-45.248L512 444.128zM192 352h640V288H192z" fill="currentColor"></path>
        </svg>
    </a>

    


    
     
 

 
    
        
        <p class="footer-copyright">
            Copyright © 2024 <a href="/">学习笔记</a>
        </p>
    
    
    <p>Powered by <a href="https://hexo.io" target="_blank">Hexo</a> | Theme - <a href="https://github.com/ChrAlpha/hexo-theme-cards" target="_blank">Cards</a></p>
</footer>

        </div>
        
    <script defer src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.1.0/dist/lazyload.min.js"></script>
    <script>
        window.lazyLoadOptions = {
            elements_selector: ".lazy",
            threshold: 0
        };
    </script>
 

 

 

 

 



 



 


    
 

 


    <script>
        if (typeof MathJax === 'undefined') {
            window.MathJax = {
                loader: {
                    source: {
                        '[tex]/amsCd': '[tex]/amscd',
                        '[tex]/AMScd': '[tex]/amscd'
                    }
                },
                tex: {
                    inlineMath: {'[+]': [['$', '$']]},
                    tags: 'ams'
                },
                options: {
                    renderActions: {
                        findScript: [10, doc => {
                            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                                const display = !!node.type.match(/; *mode=display/);
                                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                                const text = document.createTextNode('');
                                node.parentNode.replaceChild(text, node);
                                math.start = {node: text, delim: '', n: 0};
                                math.end = {node: text, delim: '', n: 0};
                                doc.math.push(math);
                            });
                        }, '', false],
                        insertedScript: [200, () => {
                            document.querySelectorAll('mjx-container').forEach(node => {
                                let target = node.parentNode;
                                if (target.nodeName.toLowerCase() === 'li') {
                                    target.parentNode.classList.add('has-jax');
                                }
                            });
                        }, '', false]
                    }
                }
            };
            (function () {
                var script = document.createElement('script');
                script.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
                script.defer = true;
                document.head.appendChild(script);
            })();
        } else {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
        }
    </script>
 

 

 

 




    </body>
</html>
