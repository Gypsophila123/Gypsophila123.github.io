<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>00.机器视觉基础概念导读</title>
    <url>/Deep%20Learning/00.%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E5%AF%BC%E8%AF%BB/</url>
    <content><![CDATA[<p>[TOC]</p>
<h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>本人是一个深度学习初学者，本笔记以书籍内容为基础，通过查阅各方资料，根据自身实际操作而写。内容不严谨，且可能存在较多错漏。</p>
<p>书籍详情：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs plaintext">书名：PyTorch 计算机视觉实战 目标检测、图像处理与深度学习<br>书名原文：Modern Computer Vision with PyTorch  <br>机械工业出版社：ISBN 978-7-111-73339-3<br>源码地址：https://github.com/PacktPublishing/Modern-Computer-Vision-with-PyTorch<br></code></pre></td></tr></table></figure>

<p>资料查阅工具：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs plaintext">chatgpt_3.5：用于名词解释、代码纠错和代码解释等<br>其他搜索引擎：解决gpt不能解决的问题<br></code></pre></td></tr></table></figure>

<p>编辑工具：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs plaintext">操作系统：windows 11<br>代码编辑：PyCharm 2023.3.4<br>笔记编辑：Typora<br></code></pre></td></tr></table></figure>

<h1 id="一、深度学习基础"><a href="#一、深度学习基础" class="headerlink" title="一、深度学习基础"></a>一、深度学习基础</h1><p>神经网络的基本构建模块是什么，以及每个模块的作用是什么。</p>
<h2 id="1-1-神经网络基础"><a href="#1-1-神经网络基础" class="headerlink" title="1.1 神经网络基础"></a>1.1 神经网络基础</h2><p>人工神经网络（<strong>Artificial Neural Network, ANN</strong>），在<a href="https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>和<a href="https://zh.wikipedia.org/wiki/%E8%AE%A4%E7%9F%A5%E7%A7%91%E5%AD%A6">认知科学</a>领域，是一种<a href="https://zh.wikipedia.org/wiki/%E4%BB%BF%E7%94%9F%E5%AD%B8">模仿</a><a href="https://zh.wikipedia.org/wiki/%E7%94%9F%E7%89%A9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">生物神经网络</a>（动物的<a href="https://zh.wikipedia.org/wiki/%E4%B8%AD%E6%A8%9E%E7%A5%9E%E7%B6%93%E7%B3%BB%E7%B5%B1">中枢神经系统</a>，特别是<a href="https://zh.wikipedia.org/wiki/%E5%A4%A7%E8%84%91">大脑</a>）的结构和功能的<a href="https://zh.wikipedia.org/wiki/%E6%95%B0%E5%AD%A6%E6%A8%A1%E5%9E%8B">数学模型</a>或<a href="https://zh.wikipedia.org/wiki/%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B">计算模型</a>，用于对<a href="https://zh.wikipedia.org/wiki/%E5%87%BD%E6%95%B0">函数</a>进行估计或近似。神经网络由大量的人工神经元联结进行计算。是一种<a href="https://zh.wikipedia.org/w/index.php?title=%E8%87%AA%E9%80%82%E5%BA%94%E7%B3%BB%E7%BB%9F&action=edit&redlink=1">自适应系统</a>，通俗地讲就是具备学习功能。通过统计学的方法，人工神经网络能够类似人一样具有简单的决定能力和简单的判断能力。</p>
<h2 id="1-2-PyTorch基础"><a href="#1-2-PyTorch基础" class="headerlink" title="1.2 PyTorch基础"></a>1.2 <code>PyTorch</code>基础</h2><p>**<code>PyTorch</code>**是一个<a href="https://zh.wikipedia.org/wiki/%E5%BC%80%E6%BA%90">开源</a>的<a href="https://zh.wikipedia.org/wiki/Python">Python</a><a href="https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a><a href="https://zh.wikipedia.org/wiki/%E5%BA%93">库</a>，基于<a href="https://zh.wikipedia.org/w/index.php?title=Torch_(%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0)&action=edit&redlink=1">Torch</a><a href="https://zh.wikipedia.org/wiki/%E5%BA%93">库</a>[<a href="https://zh.wikipedia.org/wiki/PyTorch#cite_note-:1-2">2]</a>[<a href="https://zh.wikipedia.org/wiki/PyTorch#cite_note-:2-3">3]</a>[<a href="https://zh.wikipedia.org/wiki/PyTorch#cite_note-4">4]</a>，底层由C++实现，应用于<a href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD">人工智能</a>领域，如<a href="https://zh.wikipedia.org/wiki/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89">计算机视觉</a>和<a href="https://zh.wikipedia.org/wiki/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86">自然语言处理</a>[<a href="https://zh.wikipedia.org/wiki/PyTorch#cite_note-5">5]</a>。它最初由<a href="https://zh.wikipedia.org/wiki/Meta_Platforms">Meta Platforms</a>的人工智能研究团队开发，现在属于<a href="https://zh.wikipedia.org/wiki/Linux%E5%9F%BA%E9%87%91%E4%BC%9A">Linux基金会</a>的一部分[<a href="https://zh.wikipedia.org/wiki/PyTorch#cite_note-6">6]</a>[<a href="https://zh.wikipedia.org/wiki/PyTorch#cite_note-7">7]</a>[<a href="https://zh.wikipedia.org/wiki/PyTorch#cite_note-8">8]</a>。它是在修改后的<a href="https://zh.wikipedia.org/wiki/BSD%E8%A8%B1%E5%8F%AF%E8%AD%89">BSD许可证</a>下发布的<a href="https://zh.wikipedia.org/wiki/%E8%87%AA%E7%94%B1%E5%8F%8A%E5%BC%80%E6%94%BE%E6%BA%90%E4%BB%A3%E7%A0%81%E8%BD%AF%E4%BB%B6">自由及开放源代码软件</a>。 尽管<a href="https://zh.wikipedia.org/wiki/Python">Python</a>接口更加完善并且是开发的主要重点，但 <code>PyTorch</code> 也有<a href="https://zh.wikipedia.org/wiki/C%2B%2B">C++</a>接口[<a href="https://zh.wikipedia.org/wiki/PyTorch#cite_note-9">9]</a>。</p>
<p>许多<a href="https://zh.wikipedia.org/wiki/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92">深度学习</a>软件都是基于 <code>PyTorch</code> 构建的，包括<a href="https://zh.wikipedia.org/wiki/%E7%89%B9%E6%96%AF%E6%8B%89%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6">特斯拉自动驾驶</a>[<a href="https://zh.wikipedia.org/wiki/PyTorch#cite_note-10">10]</a>、<a href="https://zh.wikipedia.org/wiki/%E5%84%AA%E6%AD%A5">Uber</a>的Pyro[<a href="https://zh.wikipedia.org/wiki/PyTorch#cite_note-11">11]</a>、<a href="https://zh.wikipedia.org/wiki/Hugging_Face">Hugging Face</a>的Transformers[<a href="https://zh.wikipedia.org/wiki/PyTorch#cite_note-12">12]</a>、 <code>PyTorch</code> Lightning[<a href="https://zh.wikipedia.org/wiki/PyTorch#cite_note-13">13]</a>[<a href="https://zh.wikipedia.org/wiki/PyTorch#cite_note-14">14]</a>、和Catalyst[<a href="https://zh.wikipedia.org/wiki/PyTorch#cite_note-15">15]</a>[<a href="https://zh.wikipedia.org/wiki/PyTorch#cite_note-16">16]</a>。</p>
<h2 id="1-3-使用PyTorch构建深度神经网络"><a href="#1-3-使用PyTorch构建深度神经网络" class="headerlink" title="1.3 使用PyTorch构建深度神经网络"></a>1.3 使用<code>PyTorch</code>构建深度神经网络</h2><h1 id="二、物体分类与目标检测"><a href="#二、物体分类与目标检测" class="headerlink" title="二、物体分类与目标检测"></a>二、物体分类与目标检测</h1><h2 id="2-1-卷积神经网络"><a href="#2-1-卷积神经网络" class="headerlink" title="2.1 卷积神经网络"></a>2.1 卷积神经网络</h2><p><strong>卷积神经网络</strong>（英语：convolutional neural network，<a href="https://zh.wikipedia.org/wiki/%E7%B8%AE%E5%AF%AB">缩写</a>：<strong>CNN</strong>）是一种<a href="https://zh.wikipedia.org/wiki/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">前馈神经网络</a>，它的人工神经元可以响应一部分覆盖范围内的周围单元，[<a href="https://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C#cite_note-deeplearning-1">1]</a>对于大型图像处理有出色表现。由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网络能够利用输入数据的二维结构。与其他深度学习结构相比，卷积神经网络在图像和<a href="https://zh.wikipedia.org/wiki/%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB">语音识别</a>方面能够给出更好的结果。这一模型也可以使用<a href="https://zh.wikipedia.org/wiki/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95">反向传播算法</a>进行训练。相比较其他深度、前馈神经网络，卷积神经网络需要考量的参数更少，使之成为一种颇具吸引力的深度学习结构[<a href="https://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C#cite_note-STANCNN-2">2]</a>。</p>
<h2 id="2-2-面向图像分类的迁移学习"><a href="#2-2-面向图像分类的迁移学习" class="headerlink" title="2.2 面向图像分类的迁移学习"></a>2.2 面向图像分类的迁移学习</h2><p><strong>迁移学习</strong>（英语：Transfer learning）是属于<a href="https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>的一种研究领域。它专注于存储已有问题的解决模型，并将其利用在其他不同但相关问题上。[<a href="https://zh.wikipedia.org/wiki/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0#cite_note-1">1]</a> 比如说，用来辨识汽车的知识（或者是模型）也可以被用来提升识别卡车的能力。计算机领域的迁移学习和心理学常常提到的学习迁移在概念上有一定关系，但是两个领域在学术上的关系非常有限。</p>
<h2 id="2-3-图像分类的实战技术"><a href="#2-3-图像分类的实战技术" class="headerlink" title="2.3 图像分类的实战技术"></a>2.3 图像分类的实战技术</h2><h2 id="2-4-目标检测基础"><a href="#2-4-目标检测基础" class="headerlink" title="2.4 目标检测基础"></a>2.4 目标检测基础</h2><p><strong>维奥拉-琼斯目标检测框架</strong>（英语：Viola–Jones object detection framework）是第一种可以实时处理并给出很好的物体检出率的<a href="https://zh.wikipedia.org/wiki/%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B">物体检测</a>的方法，由保罗·维奥拉和迈克尔·琼斯于2001年提出[<a href="https://zh.wikipedia.org/wiki/%E7%BB%B4%E5%A5%A5%E6%8B%89-%E7%90%BC%E6%96%AF%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6#cite_note-1">1]</a>[<a href="https://zh.wikipedia.org/wiki/%E7%BB%B4%E5%A5%A5%E6%8B%89-%E7%90%BC%E6%96%AF%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6#cite_note-2">2]</a>。值得一提的是，提出该方法的论文于2011年的CVPR会议上评为<em>龙格-希金斯奖</em>[<a href="https://zh.wikipedia.org/wiki/%E7%BB%B4%E5%A5%A5%E6%8B%89-%E7%90%BC%E6%96%AF%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6#cite_note-3">3]</a>。虽然它可以被训练来寻找多种物体，它的主要应用还是在解决<a href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B">人脸检测</a>方面。这个方法在<a href="https://zh.wikipedia.org/wiki/OpenCV">OpenCV</a>中被实现为<code>cvHaarDetectObjects()</code>。</p>
<h2 id="2-5-目标检测进阶"><a href="#2-5-目标检测进阶" class="headerlink" title="2.5 目标检测进阶"></a>2.5 目标检测进阶</h2><h2 id="2-6-图像分割"><a href="#2-6-图像分割" class="headerlink" title="2.6 图像分割"></a>2.6 图像分割</h2><p>在<a href="https://zh.wikipedia.org/wiki/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89">计算机视觉</a>领域，<strong>图像分割</strong>（segmentation）指的是将<a href="https://zh.wikipedia.org/wiki/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F">数字图像</a>细分为多个图像子区域（像素的<a href="https://zh.wikipedia.org/wiki/%E9%9B%86%E5%90%88">集合</a>）（也被称作超像素）的过程。图像分割的目的是简化或改变图像的表示形式，使得图像更容易理解和分析。[<a href="https://zh.wikipedia.org/wiki/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2#cite_note-computervision-1">1]</a>图像分割通常用于定位图像中的物体和<a href="https://zh.wikipedia.org/wiki/%E8%BE%B9%E7%95%8C">边界</a>（<a href="https://zh.wikipedia.org/wiki/%E7%BA%BF">线</a>，<a href="https://zh.wikipedia.org/wiki/%E6%9B%B2%E7%BA%BF">曲线</a>等）。更精确的，图像分割是对图像中的每个<a href="https://zh.wikipedia.org/wiki/%E5%83%8F%E7%B4%A0">像素</a>加<a href="https://zh.wikipedia.org/wiki/%E6%A0%87%E7%AD%BE">标签</a>的一个过程，这一过程使得具有相同标签的像素具有某种共同视觉特性。</p>
<p>图像分割的结果是图像上子区域的集合（这些子区域的全体覆盖了整个图像），或是从图像中提取的<a href="https://zh.wikipedia.org/w/index.php?title=%E8%BD%AE%E5%BB%93&action=edit&redlink=1">轮廓</a>线的集合（例如<a href="https://zh.wikipedia.org/wiki/%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B">边缘检测</a>）。一个子区域中的每个像素在某种特性的度量下或是由计算得出的特性都是相似的，例如<a href="https://zh.wikipedia.org/wiki/%E9%A2%9C%E8%89%B2">颜色</a>、<a href="https://zh.wikipedia.org/wiki/%E4%BA%AE%E5%BA%A6">亮度</a>、<a href="https://zh.wikipedia.org/wiki/%E7%B4%8B%E7%90%86">纹理</a>。<a href="https://zh.wikipedia.org/w/index.php?title=%E9%82%BB%E6%8E%A5&action=edit&redlink=1">邻接</a>区域在某种特性的度量下有很大的不同。[<a href="https://zh.wikipedia.org/wiki/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2#cite_note-computervision-1">1]</a></p>
<h2 id="2-7-目标检测与图像分割的应用"><a href="#2-7-目标检测与图像分割的应用" class="headerlink" title="2.7 目标检测与图像分割的应用"></a>2.7 目标检测与图像分割的应用</h2><h1 id="三、图像处理"><a href="#三、图像处理" class="headerlink" title="三、图像处理"></a>三、图像处理</h1><h2 id="3-1-自编码器与图像处理"><a href="#3-1-自编码器与图像处理" class="headerlink" title="3.1 自编码器与图像处理"></a>3.1 自编码器与图像处理</h2><h2 id="3-2-基于GAN的图像生成"><a href="#3-2-基于GAN的图像生成" class="headerlink" title="3.2 基于GAN的图像生成"></a>3.2 基于GAN的图像生成</h2><p><strong>生成对抗网络</strong>（英语：Generative Adversarial Network，简称<strong>GAN</strong>）是<a href="https://zh.wikipedia.org/wiki/%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%BC%8F%E5%AD%A6%E4%B9%A0">非监督式学习</a>的一种方法，通过两个<a href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">神经网络</a>相互<a href="https://zh.wikipedia.org/wiki/%E5%8D%9A%E5%BC%88%E8%AE%BA">博弈</a>的方式进行学习。该方法由<a href="https://zh.wikipedia.org/wiki/%E4%BC%8A%E6%81%A9%C2%B7%E5%8F%A4%E5%BE%B7%E8%B4%B9%E6%B4%9B">伊恩·古德费洛</a>等人于2014年提出。[<a href="https://zh.wikipedia.org/wiki/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C#cite_note-MyUser_Arxiv.org_April_7_2016c-1">1]</a> 生成对抗网络由一个生成网络与一个判别网络组成。生成网络从潜在空间（latent space）中随机取样作为输入，其输出结果需要尽量模仿训练集中的真实样本。判别网络的输入则为真实样本或生成网络的输出，其目的是将生成网络的输出从真实样本中尽可能分辨出来。而生成网络则要尽可能地欺骗判别网络。两个网络相互对抗、不断调整参数，最终目的是使判别网络无法判断生成网络的输出结果是否真实。[<a href="https://zh.wikipedia.org/wiki/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C#cite_note-2">2]</a>[<a href="https://zh.wikipedia.org/wiki/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C#cite_note-MyUser_Arxiv.org_April_7_2016c-1">1]</a>[<a href="https://zh.wikipedia.org/wiki/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C#cite_note-OpenAI_com-3">3]</a></p>
<p><a href="https://zh.wikipedia.org/wiki/File:GAN_deepfake_white_girl.jpg"><img src="/pictures/00.%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E5%AF%BC%E8%AF%BB/220px-GAN_deepfake_white_girl.jpg" class="lazy" data-srcset="/pictures/00.%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E5%AF%BC%E8%AF%BB/220px-GAN_deepfake_white_girl.jpg" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="由GAN deepfake生成的人脸"></a></p>
<p>生成对抗网络常用于生成以假乱真的图片。[<a href="https://zh.wikipedia.org/wiki/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C#cite_note-ITT_GANs-4">4]</a>此外，该方法还被用于生成影片[<a href="https://zh.wikipedia.org/wiki/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C#cite_note-5">5]</a>、三维物体模型[<a href="https://zh.wikipedia.org/wiki/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C#cite_note-6">6]</a>等。</p>
<p>生成对抗网络虽然最开始提出是为了<a href="https://zh.wikipedia.org/wiki/%E7%84%A1%E7%9B%A3%E7%9D%A3%E5%AD%B8%E7%BF%92">无监督学习</a>，但经证明对<a href="https://zh.wikipedia.org/wiki/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0">半监督学习</a>[<a href="https://zh.wikipedia.org/wiki/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C#cite_note-ITT_GANs-4">4]</a>、<a href="https://zh.wikipedia.org/wiki/%E5%AE%8C%E5%85%A8%E7%9B%A3%E7%9D%A3%E5%AD%B8%E7%BF%92">完全监督学习</a>[<a href="https://zh.wikipedia.org/wiki/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C#cite_note-7">7]</a> 、<a href="https://zh.wikipedia.org/wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0">强化学习</a>[<a href="https://zh.wikipedia.org/wiki/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C#cite_note-8">8]</a>也有效。 在2016年的一个研讨会上，<a href="https://zh.wikipedia.org/wiki/%E6%9D%A8%E7%AB%8B%E6%98%86">杨立昆</a>称生成式对抗网络为“机器学习这二十年来最酷的想法”[<a href="https://zh.wikipedia.org/wiki/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C#cite_note-9">9]</a>。</p>
<h2 id="3-3-高级GAN图像处理"><a href="#3-3-高级GAN图像处理" class="headerlink" title="3.3 高级GAN图像处理"></a>3.3 高级GAN图像处理</h2><h1 id="四、计算机视觉与其它技术"><a href="#四、计算机视觉与其它技术" class="headerlink" title="四、计算机视觉与其它技术"></a>四、计算机视觉与其它技术</h1><p><strong>计算机视觉</strong>（Computer vision, CV）是一门研究如何使机器“<a href="https://zh.wikipedia.org/wiki/%E7%9C%8B">看</a>”的科学，更进一步的说，就是指用<a href="https://zh.wikipedia.org/wiki/%E6%91%84%E5%BD%B1%E6%9C%BA">摄影机</a>和<a href="https://zh.wikipedia.org/wiki/%E7%94%B5%E5%AD%90%E8%AE%A1%E7%AE%97%E6%9C%BA">计算机</a>代替人眼对目标进行识别、跟踪和测量等<a href="https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89">机器视觉</a>，并进一步做<a href="https://zh.wikipedia.org/wiki/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86">图像处理</a>，用计算机处理成为更适合人眼观察或传送给仪器检测的图像[<a href="https://zh.wikipedia.org/wiki/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89#cite_note-1">1]</a>。</p>
<p>作为一门科学学科，计算机视觉研究相关的理论和技术，试图建立能够从图像或者多维数据中获取“信息”的人工智能系统。这里所指的信息指<a href="https://zh.wikipedia.org/wiki/%E5%85%8B%E5%8B%9E%E5%BE%B7%C2%B7%E5%A4%8F%E8%BE%B2">香农</a>定义的，可以用来帮助做一个“决定”的信息。因为感知可以看作是从感官信号中提取信息，所以计算机视觉也可以看作是研究如何使人工系统从图像或多维数据中“感知”的科学。</p>
<h2 id="4-1-使用小样本进行模型训练"><a href="#4-1-使用小样本进行模型训练" class="headerlink" title="4.1 使用小样本进行模型训练"></a>4.1 使用小样本进行模型训练</h2><h2 id="4-2-计算机视觉与NLP"><a href="#4-2-计算机视觉与NLP" class="headerlink" title="4.2 计算机视觉与NLP"></a>4.2 计算机视觉与NLP</h2><p><strong>自然语言处理</strong>（英语：Natural Language Processing，<a href="https://zh.wikipedia.org/wiki/%E7%BC%A9%E5%86%99">缩写</a>作 <strong>NLP</strong>）是<a href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7">人工智能</a>和<a href="https://zh.wikipedia.org/wiki/%E8%AA%9E%E8%A8%80%E5%AD%B8">语言学</a>领域的分支学科。此领域探讨如何处理及运用<a href="https://zh.wikipedia.org/wiki/%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80">自然语言</a>；自然语言处理包括多方面和步骤，基本有认知、理解、生成等部分。</p>
<p>自然语言认知和理解是让电脑把输入的<a href="https://zh.wikipedia.org/wiki/%E8%AA%9E%E8%A8%80">语言</a>变成有意思的符号和关系，然后根据目的再处理。自然语言生成系统则是把计算机数据转化为自然语言。</p>
<p>自然语言处理要研制表示语言能力和语言应用的模型, 建立计算框架来实现并完善<a href="https://zh.wikipedia.org/wiki/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B">语言模型</a>，并根据语言模型设计各种实用系统及探讨这些系统的评测技术[<a href="https://zh.wikipedia.org/wiki/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86#cite_note-1">1]</a>。</p>
<h2 id="4-3-计算机视觉与强化学习"><a href="#4-3-计算机视觉与强化学习" class="headerlink" title="4.3 计算机视觉与强化学习"></a>4.3 计算机视觉与强化学习</h2><p><strong>强化学习</strong>（英语：Reinforcement learning，简称RL）是<a href="https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>中的一个领域，强调如何基于<a href="https://zh.wikipedia.org/wiki/%E7%94%9F%E6%80%81%E7%8E%AF%E5%A2%83">环境</a>而行动，以取得最大化的预期利益[<a href="https://zh.wikipedia.org/wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0#cite_note-1">1]</a>。强化学习是除了<a href="https://zh.wikipedia.org/wiki/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0">监督学习</a>和<a href="https://zh.wikipedia.org/wiki/%E7%84%A1%E7%9B%A3%E7%9D%A3%E5%AD%B8%E7%BF%92">非监督学习</a>之外的第三种基本的机器学习方法。与监督学习不同的是，强化学习不需要带标签的输入输出对，同时也无需对非最优解的精确地纠正。其关注点在于寻找探索（对未知领域的）和利用（对已有知识的）的平衡[<a href="https://zh.wikipedia.org/wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0#cite_note-kaelbling-2">2]</a>，强化学习中的“探索-利用”的交换，在<a href="https://zh.wikipedia.org/w/index.php?title=%E5%A4%9A%E8%87%82%E8%80%81%E8%99%8E%E6%9C%BA&action=edit&redlink=1">多臂老虎机</a>问题和有限MDP中研究得最多。</p>
<p>其灵感来源于心理学中的<a href="https://zh.wikipedia.org/wiki/%E8%A1%8C%E4%B8%BA%E4%B8%BB%E4%B9%89">行为主义</a>理论，即有机体如何在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。这个方法具有普适性，因此在其他许多领域都有研究，例如<a href="https://zh.wikipedia.org/wiki/%E5%8D%9A%E5%BC%88%E8%AE%BA">博弈论</a>、<a href="https://zh.wikipedia.org/wiki/%E6%8E%A7%E5%88%B6%E8%AE%BA">控制论</a>、<a href="https://zh.wikipedia.org/wiki/%E8%BF%90%E7%AD%B9%E5%AD%A6">运筹学</a>、<a href="https://zh.wikipedia.org/wiki/%E4%BF%A1%E6%81%AF%E8%AE%BA">信息论</a>、仿真优化、<a href="https://zh.wikipedia.org/wiki/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E7%B3%BB%E7%BB%9F">多智能体系统</a>、<a href="https://zh.wikipedia.org/wiki/%E7%BE%A4%E4%BD%93%E6%99%BA%E8%83%BD">群体智能</a>、<a href="https://zh.wikipedia.org/wiki/%E7%BB%9F%E8%AE%A1%E5%AD%A6">统计学</a>以及<a href="https://zh.wikipedia.org/wiki/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95">遗传算法</a>。在运筹学和控制理论研究的语境下，强化学习被称作“近似动态规划”（approximate dynamic programming，ADP）。在<a href="https://zh.wikipedia.org/wiki/%E6%9C%80%E4%BC%98%E6%8E%A7%E5%88%B6">最优控制</a>理论中也有研究这个问题，虽然大部分的研究是关于最优解的存在和特性，并非是学习或者近似方面。在<a href="https://zh.wikipedia.org/wiki/%E7%BB%8F%E6%B5%8E%E5%AD%A6">经济学</a>和<a href="https://zh.wikipedia.org/wiki/%E5%8D%9A%E5%BC%88%E8%AE%BA">博弈论</a>中，强化学习被用来解释在<a href="https://zh.wikipedia.org/wiki/%E6%9C%89%E9%99%90%E7%90%86%E6%80%A7">有限理性</a>的条件下如何出现平衡。</p>
<h2 id="4-4-模型的实际应用部署"><a href="#4-4-模型的实际应用部署" class="headerlink" title="4.4 模型的实际应用部署"></a>4.4 模型的实际应用部署</h2><h2 id="4-5-使用OpenCV实用程序进行图像分析"><a href="#4-5-使用OpenCV实用程序进行图像分析" class="headerlink" title="4.5 使用OpenCV实用程序进行图像分析"></a>4.5 使用OpenCV实用程序进行图像分析</h2>]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title>02.pytorch基础</title>
    <url>/Deep%20Learning/02.pytorch%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h1 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h1><h2 id="推荐使用Anaconda安装（可选）"><a href="#推荐使用Anaconda安装（可选）" class="headerlink" title="推荐使用Anaconda安装（可选）"></a>推荐使用Anaconda安装（可选）</h2><p>进入Anaconda官网下载安装即可，安装完成后搜索 Anaconda Prompt 以打开命令行。</p>
<p>路径前括号包裹的名字即为当前环境名，如 <code>(base) C:\Users\Administrator&gt;</code>即为base环境，该环境在Anaconda根目录下，不可删除。如果创建新环境，则存放于根目录下的<code>envs</code>文件夹内。</p>
<p><strong>开始：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 设置清华镜像</span><br>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/<br>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/<br>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/<br>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/<br><span class="hljs-comment"># 设置bioconda</span><br>conda config --add channels bioconda<br>conda config --add channels conda-forge<br><span class="hljs-comment"># 设置搜索时显示通道地址</span><br>conda config --<span class="hljs-built_in">set</span> show_channel_urls yes<br><span class="hljs-comment"># 更新</span><br>conda update conda<br>conda update Anaconda<br></code></pre></td></tr></table></figure>

<p><strong>基本使用</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 创建虚拟环境</span><br>conda create -n env_name python=<span class="hljs-number">3.8</span><br>conda create -n env_name numpy matplotlib python=<span class="hljs-number">3.8</span><br><span class="hljs-comment"># 查看有哪些虚拟环境</span><br>conda env <span class="hljs-built_in">list</span><br>conda info -e<br>conda info --envs<br><span class="hljs-comment"># 启用虚拟环境</span><br>conda activate env_name<br><span class="hljs-comment"># 查看当前python版本</span><br>python --version<br><br><span class="hljs-comment"># 导出特定环境</span><br><span class="hljs-comment">#获得环境中的所有配置</span><br>conda env export --name myenv &gt; myenv.yml<br><span class="hljs-comment">#重新还原环境</span><br>conda env create -f  myenv.yml<br><br><span class="hljs-comment"># 查看环境安装了哪些包</span><br>conda <span class="hljs-built_in">list</span><br><span class="hljs-comment"># 安装和更新包</span><br>conda install &lt;package&gt;<br><span class="hljs-comment"># 卸载包</span><br>conda uninstall package_name<br><span class="hljs-comment"># 卸载包及依赖该包的包（不建议）</span><br>conda uninstall package_name --force<br><span class="hljs-comment"># 变更Python版本</span><br>conda install python=<span class="hljs-number">3.6</span><br>conda update python<br></code></pre></td></tr></table></figure>

<h3 id="pycharm-配置-Anaconda为Python解释器"><a href="#pycharm-配置-Anaconda为Python解释器" class="headerlink" title="pycharm 配置 Anaconda为Python解释器"></a>pycharm 配置 Anaconda为Python解释器</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs plaintext"># 设置-项目-python解释器<br># 指定路径为安装目录，或者指定安装目录下的 python.exe 文件<br></code></pre></td></tr></table></figure>



<h2 id="安装Pytorch（必须）"><a href="#安装Pytorch（必须）" class="headerlink" title="安装Pytorch（必须）"></a>安装Pytorch（必须）</h2><figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">进入官网：https://pytorch.org</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">选择快速开始</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">按你的环境选择操作系统、安装方式等</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">如果需要使用cuda，进入cmd输入nvidia-smi查看自己支持的cuda版本</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">最后复制命令行，进入conda命令行开始安装</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">如下是在 windows 下使用 conda 安装pytorch的一个例子</span><br>conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia<br></code></pre></td></tr></table></figure>

<p>安装完成后，进入Python命令行以查看是否成功。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-built_in">print</span>(torch.__version__)<br></code></pre></td></tr></table></figure>

<h2 id="使用Jupyter（可选）"><a href="#使用Jupyter（可选）" class="headerlink" title="使用Jupyter（可选）"></a>使用Jupyter（可选）</h2><blockquote>
<p>安装Anaconda后在Anaconda内安装Jupyter或使用命令行安装</p>
<p>如果使用pycharm，配置好Anaconda为解释器后，直接在pycahrm中使用即可</p>
</blockquote>
<p>使用Jupyter notebook以便捷的演示代码效果。jupyter 的目录在 Anaconda 的 scripts 下，将其加入环境变量。输入：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">命令行进入自己的文档目录</span><br>jupyter notebook  ./<br><span class="hljs-meta prompt_"># </span><span class="language-bash">指定路径</span><br>jupyter notebook  [文件夹路径]<br></code></pre></td></tr></table></figure>

<p>上述命令会打开浏览器，进入Jupyter界面，右键新建 <code>New Notebook</code> 即可开始使用。</p>
<p>在单元中我们可以编辑文字、编写代码、绘制图片等等。</p>
<p>单元中包含<strong>两种模式与快捷键</strong>，蓝色为命令模式，绿色为编辑模式，使用Enter和Esc在两种模式下切换。</p>
<ul>
<li>通用<ul>
<li><code>Shift+Enter</code>，执行本单元代码，并跳转到下一单元</li>
<li><code>Ctrl+Enter</code>，执行本单元代码，留在本单元</li>
</ul>
</li>
<li>命令模式<ul>
<li><code>Y</code>：cell切换到Code模式</li>
<li><code>M</code>：cell切换到Markdown模式</li>
<li><code>A</code>：在当前cell的上面添加cell</li>
<li><code>B</code>：在当前cell的下面添加cell</li>
<li><code>双击D</code>：删除当前cell</li>
<li><code>Z</code>：回退</li>
<li><code>Ctrl+Shift+减号</code>：分隔cell，在光标处</li>
<li><code>L</code>：为当前cell加上行号</li>
</ul>
</li>
<li>编辑模式(和大多数编辑器一样)<ul>
<li><code>Ctrl+Z(Mac:CMD+Z)</code>：回退</li>
<li><code>Ctrl+Y</code>：重做</li>
<li><code>Tab键</code>：代码补全</li>
<li><code>Ctrl+/</code>：注释多行代码</li>
</ul>
</li>
</ul>
<h2 id="问题记录"><a href="#问题记录" class="headerlink" title="问题记录"></a>问题记录</h2><ul>
<li>opencv安装 <code>conda install opencv</code> 后提示 <code>ImportError: DLL load failed while importing cv2: 找不到指定的模块</code></li>
<li>发现如果是未安装提示为：<code> ModuleNotFoundError: No module named &#39;xx&#39;</code></li>
<li>所以这是安装成功，但环境没配置好导致出现问题</li>
<li>使用 <code>conda remove opencv</code> 再 <code>pip install opencv-python</code>，问题解决</li>
<li>猜测原因是某些包使用 pip 安装，导致与conda环境不兼容，以后安装包要与最开始保持一致</li>
</ul>
<blockquote>
<p>参考 <a href="https://developer.aliyun.com/article/1124357">opencv-python 最新4.6.0.66版安装及介绍翻译-阿里云开发者社区 (aliyun.com)</a></p>
</blockquote>
<h1 id="张量-Tensor"><a href="#张量-Tensor" class="headerlink" title="张量 Tensor"></a>张量 Tensor</h1><blockquote>
<p>涉及到可以演示的代码放在同名的 <code>jupyter notebook</code> 文件中</p>
</blockquote>
<h2 id="基础使用"><a href="#基础使用" class="headerlink" title="基础使用"></a>基础使用</h2><p>张量 Tensor 是PyTorch中的基本数据类型。张量是一个多维矩阵，类似于NumPy中的ndarrays：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> torch<br>x = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])<br>y = torch.tensor([[<span class="hljs-number">1</span>], [<span class="hljs-number">2</span>]])<br><span class="hljs-built_in">print</span>(x)<br><span class="hljs-built_in">print</span>(y)<br><span class="hljs-comment"># 如果初始数组中元素类型不同，会转化为能兼容全部值的元素类型</span><br>z = torch.tensor([<span class="hljs-literal">False</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">2</span>])<br></code></pre></td></tr></table></figure>

<ul>
<li>张量实现了一些和numpy一样的基础函数，所以我决定不专门学习其函数，遇到一个与numpy不同的地方就记录一个</li>
<li>下面记录一些与numpy不同的函数<ul>
<li>torch 默认使用float32为基本元素类型</li>
<li>使用 matmul 或者 @ 进行矩阵乘法</li>
<li>使用permute交换维度</li>
<li>使用cat实现张量的连接</li>
<li>标量（仅一个元素）使用<code>tensor.item()</code>取值 </li>
<li>使用<code>tensor.numpy()</code>转化为np对象</li>
<li>使用<code>tensor.to(device)</code>使在不同设备使用</li>
<li>使用<code>torch.stack</code>进行张量堆叠</li>
</ul>
</li>
</ul>
<h2 id="torch-nn-库"><a href="#torch-nn-库" class="headerlink" title="torch.nn 库"></a>torch.nn 库</h2><p><code>torch.nn</code>是PyTorch中用于构建神经网络的模块。<code>nn</code>模块提供了一系列用于构建神经网络模型的类和函数，包括各种层（如全连接层、卷积层、循环神经网络层等）、损失函数、优化器等。</p>
<p><code>torch.nn</code>中的主要组件包括：</p>
<ol>
<li><strong>各种神经网络层</strong>：如全连接层（<code>nn.Linear</code>）、卷积层（<code>nn.Conv2d</code>）、池化层（<code>nn.MaxPool2d</code>）、循环神经网络层（<code>nn.RNN</code>、<code>nn.LSTM</code>、<code>nn.GRU</code>）等。</li>
<li><strong>激活函数</strong>：如ReLU激活函数（<code>nn.ReLU</code>）、Sigmoid激活函数（<code>nn.Sigmoid</code>）、Tanh激活函数（<code>nn.Tanh</code>）等。</li>
<li><strong>损失函数</strong>：如均方误差损失（<code>nn.MSELoss</code>）、交叉熵损失（<code>nn.CrossEntropyLoss</code>）等。</li>
<li><strong>优化器</strong>：如随机梯度下降优化器（<code>torch.optim.SGD</code>）、Adam优化器（<code>torch.optim.Adam</code>）等。</li>
</ol>
<p>使用<code>torch.nn</code>模块可以简化神经网络的构建过程，提供了丰富的工具和组件来搭建和训练神经网络模型。通常，您可以通过继承<code>nn.Module</code>类来定义自己的神经网络模型，并在其中组合和使用<code>nn</code>模块中提供的各种层和函数。</p>
<h2 id="张量对象的自动梯度"><a href="#张量对象的自动梯度" class="headerlink" title="张量对象的自动梯度"></a>张量对象的自动梯度</h2><p>微分和计算梯度在更新权重中起着关键作用。PyTorch的张量对象自带计算梯度的内置功能。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 指定requires_grad为True表示要为张量对象计算梯度</span><br>x = torch.tensor([[<span class="hljs-number">2.</span>, -<span class="hljs-number">1</span>], [<span class="hljs-number">1.</span>, <span class="hljs-number">1</span>]], requires_grad=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 定义输出方式为 x 平方和，这里返回的实际是一个Tensor对象</span><br>out = x.<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>()<br><span class="hljs-comment"># 为 out 函数计算梯度，实际上我们知道 x^2 的微分结果为 2x</span><br>out.backward()<br><span class="hljs-comment"># 如此可以直接得到梯度结果</span><br>x.grad<br><span class="hljs-comment"># tensor([[ 4., -2.], [ 2.,  2.]])</span><br></code></pre></td></tr></table></figure>

<h2 id="tensor-对-ndarray-的优势"><a href="#tensor-对-ndarray-的优势" class="headerlink" title="tensor 对 ndarray 的优势"></a>tensor 对 ndarray 的优势</h2><ul>
<li><p>torch优化了张量能与GPU一起工作，如果每个权重由不同的内核并行完成，则可以优化权重的计算过程，大幅度提高计算效率</p>
</li>
<li><pre><code class="Python"># 随机生成两个torch对象
x = torch.rand(1, 6400)
y = torch.rand(6400, 5000)
# print(x, y)
# 定义使用 gpu 进行计算，如果没有gpu则使用cpu
device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;

# 执行矩阵乘法并计时
x, y = x.to(device), y.to(device)
%timeit z = x@y

x, y = x.cpu(), y.cpu()
%timeit z = x@y
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs plaintext"><br></code></pre></td></tr></table></figure>
412 µs ± 5.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
3.79 ms ± 14.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs plaintext"><br># 使用pytorch构建神经网络模型<br><br>下面通过tensor实现一个神经网络，解决两个数相加的问题<br><br>在每轮训练中，我们需要做的步骤有：<br><br>1. 计算给定输入输出所对应的损失值<br>2. 计算每个参数对应的梯度<br>3. 根据梯度更新权重<br><br>```Python<br>import torch<br># torch中用于构建神经网络的模块<br>import torch.nn as nn<br># torch中的梯度下降算法模块<br>import torch.optim as optim<br><br># 两个数相加问题<br>x = [[1,2],[3,4],[5,6],[7,8]]<br>y = [[3],[7],[11],[15]]<br><br># 将数转化为浮点数，避免精度损失<br>X = torch.tensor(x).float()<br>Y = torch.tensor(y).float()<br><br># 使用 GPU 进行计算<br>device = &#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;<br>X = X.to(device)<br>Y = Y.to(device)<br><br># 一个简单的神经网络模型，继承于nn.Module类<br>class MyNeuralNet(nn.Module):<br>    def __init__(self):<br>        super().__init__()<br>        # 输入大小为2，输出大小为8的全连接层<br>        self.input_to_hidden_layer = nn.Linear(2,8)<br>        # ReLU 作激活函数<br>        self.hidden_layer_activation = nn.ReLU()<br>        # 输入大小为8，输出大小为1的全连接层<br>        self.hidden_to_output_layer = nn.Linear(8,1)<br>        <br>    # 覆写前向传播方法，当调用模型实例时，实际上会调用该方法执行前向传播<br>    def forward(self, x):<br>        x = self.input_to_hidden_layer(x)<br>        x = self.hidden_layer_activation(x)<br>        x = self.hidden_to_output_layer(x)<br>        return x<br><br># 创建模型实例并指定设备<br>my_net = MyNeuralNet().to(device)<br># 定义损失函数为均方差<br>loss_func = nn.MSELoss()<br># 为模型指定使用随机梯度下降算法，且学习率为0.001<br>opt = optim.SGD(my_net.parameters(), lr=0.001)<br><br># 执行迭代<br>loss_history = []<br>for epoch in range(500):<br>    # 前向传播并计算损失值<br>    output = my_net(X)<br>    loss_value = loss_func(output,Y)<br>    <br>    # 反向传播和优化<br>    # 每次都需要将梯度置零<br>    opt.zero_grad()<br>    # 计算梯度值<br>    loss_value.backward()<br>    # 根据计算到的梯度更新模型的参数（）<br>    opt.step()<br>    loss_history.append(loss_value.item())<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
<p><img src="/pictures/02.Pytorch%E5%9F%BA%E7%A1%80/%E6%8D%9F%E5%A4%B1%E5%80%BC%E6%9B%B2%E7%BA%BF.png" class="lazy" data-srcset="/pictures/02.Pytorch%E5%9F%BA%E7%A1%80/%E6%8D%9F%E5%A4%B1%E5%80%BC%E6%9B%B2%E7%BA%BF.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240405172003300"></p>
<h2 id="数据集、数据加载器和批大小"><a href="#数据集、数据加载器和批大小" class="headerlink" title="数据集、数据加载器和批大小"></a>数据集、数据加载器和批大小</h2><ol>
<li><strong>数据集（Dataset）</strong>： 数据集是一组数据样本的集合，用于训练、验证或测试模型。在PyTorch中，您可以通过创建一个自定义的数据集类来加载您的数据，该类需要继承自<code>torch.utils.data.Dataset</code>类，并实现<code>__len__()</code>和<code>__getitem__()</code>方法。数据集类负责加载和处理单个样本，并在需要时对其进行转换。</li>
<li><strong>数据加载器（DataLoader）</strong>： 数据加载器是一个用于加载数据集的迭代器，它会自动将数据划分为小批量（batch）并提供给模型。数据加载器使用数据集来生成批量数据，并支持多进程加载数据以加速训练。通过使用数据加载器，您可以方便地迭代整个数据集，并在训练过程中为模型提供批量数据。</li>
<li><strong>批大小（batch size）</strong>： 批大小是指在每次训练迭代中使用的样本数量。将数据划分为小批量进行训练可以提高训练的效率，并利用并行计算的能力。较大的批大小通常会导致更高的训练速度，但可能会增加内存消耗。通常，批大小的选择取决于您的硬件资源、模型的复杂度以及数据集的大小。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 在上面代码的基础上变更以演示批大小的使用</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-comment"># 导入数据集和数据加载器模块</span><br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset, DataLoader<br><br>x = [[<span class="hljs-number">1</span> ,<span class="hljs-number">2</span>] ,[<span class="hljs-number">3</span> ,<span class="hljs-number">4</span>] ,[<span class="hljs-number">5</span> ,<span class="hljs-number">6</span>] ,[<span class="hljs-number">7</span> ,<span class="hljs-number">8</span>]]<br>y = [[<span class="hljs-number">3</span>] ,[<span class="hljs-number">7</span>] ,[<span class="hljs-number">11</span>] ,[<span class="hljs-number">15</span>]]<br><br>X = torch.tensor(x).<span class="hljs-built_in">float</span>()<br>Y = torch.tensor(y).<span class="hljs-built_in">float</span>()<br><br>device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span><br>X = X.to(device)<br>Y = Y.to(device)<br><br><span class="hljs-comment"># 自定义数据集类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self ,x ,y</span>):<br>        <span class="hljs-comment"># 复制带有梯度的张量时，指定.detach().requires_grad_(True)避免报错</span><br>        self.x = x.clone().detach().requires_grad_(<span class="hljs-literal">True</span>).<span class="hljs-built_in">float</span>()<br>        self.y = y.clone().detach().requires_grad_(<span class="hljs-literal">True</span>).<span class="hljs-built_in">float</span>()<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.x)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">return</span> self.x[idx], self.y[idx]<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyNeuralNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.input_to_hidden_layer = nn.Linear(<span class="hljs-number">2</span> ,<span class="hljs-number">8</span>)<br>        self.hidden_layer_activation = nn.ReLU()<br>        self.hidden_to_output_layer = nn.Linear(<span class="hljs-number">8</span> ,<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.input_to_hidden_layer(x)<br>        x = self.hidden_layer_activation(x)<br>        x = self.hidden_to_output_layer(x)<br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-comment"># 创建一个数据集实例</span><br>ds = MyDataset(X, Y)<br><span class="hljs-comment"># 创建数据加载器并设置批大小为2</span><br>dl = DataLoader(ds, batch_size=<span class="hljs-number">2</span>, shuffle=<span class="hljs-literal">True</span>)<br><br>my_net = MyNeuralNet().to(device)<br>loss_func = nn.MSELoss()<br>opt = optim.SGD(my_net.parameters(), lr=<span class="hljs-number">0.001</span>)<br><br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">50</span>):<br>    <span class="hljs-comment"># 从数据加载器拿每批数据</span><br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dl:<br>        <span class="hljs-comment"># 注意，从数据加载器中拿到被分割的预估值并使用</span><br>        x, y = data<br>        output = my_net(x)<br>        loss_value = loss_func(output ,y)<br><br>        opt.zero_grad()<br>        loss_value.backward()<br>        opt.step()<br></code></pre></td></tr></table></figure>

<ul>
<li>一般来说，较大的批大小可能会导致更稳定的梯度估计和更快的收敛速度，因为每个更新步骤考虑了更多的数据样本。</li>
<li>而较小的批大小通常会减少内存消耗，并且可能会在一定程度上提高模型的泛化能力。</li>
</ul>
<h2 id="使用训练过的模型预测新值"><a href="#使用训练过的模型预测新值" class="headerlink" title="使用训练过的模型预测新值"></a>使用训练过的模型预测新值</h2><p>通过上面的步骤，我们获得了一个经过学习的模型，接下来使用该模型计算新输入，验证模型效果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python">val_x = [[<span class="hljs-number">10</span>, <span class="hljs-number">11</span>]]<br>val_x = torch.tensor(val_x).<span class="hljs-built_in">float</span>().to(device)<br>my_net(val_x)<br><span class="hljs-comment"># tensor([[20.4325]], device=&#x27;cuda:0&#x27;, grad_fn=&lt;AddmmBackward0&gt;) </span><br></code></pre></td></tr></table></figure>

<h2 id="实现自定义损失函数"><a href="#实现自定义损失函数" class="headerlink" title="实现自定义损失函数"></a>实现自定义损失函数</h2><p>不同的问题可能需要不同的损失函数，因此我们需要能针对具体问题具体分析，并写出更适合的损失函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 在上面代码的基础上变更以演示自定义损失函数</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset, DataLoader<br><br>x = [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>], [<span class="hljs-number">5</span>, <span class="hljs-number">6</span>], [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>]]<br>y = [[<span class="hljs-number">3</span>], [<span class="hljs-number">7</span>], [<span class="hljs-number">11</span>], [<span class="hljs-number">15</span>]]<br><br>X = torch.tensor(x).<span class="hljs-built_in">float</span>()<br>Y = torch.tensor(y).<span class="hljs-built_in">float</span>()<br><br>device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span><br>X = X.to(device)<br>Y = Y.to(device)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, y</span>):<br>        self.x = x.clone().detach().requires_grad_(<span class="hljs-literal">True</span>).<span class="hljs-built_in">float</span>()<br>        self.y = y.clone().detach().requires_grad_(<span class="hljs-literal">True</span>).<span class="hljs-built_in">float</span>()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.x)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">return</span> self.x[idx], self.y[idx]<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyNeuralNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.input_to_hidden_layer = nn.Linear(<span class="hljs-number">2</span>, <span class="hljs-number">8</span>)<br>        self.hidden_layer_activation = nn.ReLU()<br>        self.hidden_to_output_layer = nn.Linear(<span class="hljs-number">8</span>, <span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.input_to_hidden_layer(x)<br>        x = self.hidden_layer_activation(x)<br>        x = self.hidden_to_output_layer(x)<br>        <span class="hljs-keyword">return</span> x<br><br><br>ds = MyDataset(X, Y)<br>dl = DataLoader(ds, batch_size=<span class="hljs-number">2</span>, shuffle=<span class="hljs-literal">True</span>)<br>my_net = MyNeuralNet().to(device)<br><br><br><span class="hljs-comment"># loss_func = nn.MSELoss()</span><br><span class="hljs-comment"># 自定义损失函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">my_mean_squared_error</span>(<span class="hljs-params">_y, y</span>):<br>    loss = (_y - y) ** <span class="hljs-number">2</span><br>    loss = loss.mean()<br>    <span class="hljs-keyword">return</span> loss<br><br>opt = optim.SGD(my_net.parameters(), lr=<span class="hljs-number">0.001</span>)<br><br>loss_history = []<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    loss_value_count = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dl:<br>        x, y = data<br>        output = my_net(x)<br><br>        <span class="hljs-comment"># loss_value = loss_func(output ,y)</span><br>        <span class="hljs-comment"># 使用自定义损失函数</span><br>        loss_value = my_mean_squared_error(output, y)<br><br>        opt.zero_grad()<br>        loss_value.backward()<br>        opt.step()<br>        loss_value_count += loss_value.item()<br><br>    loss_history.append(loss_value_count)<br></code></pre></td></tr></table></figure>

<h2 id="获取中间层的值"><a href="#获取中间层的值" class="headerlink" title="获取中间层的值"></a>获取中间层的值</h2><p>某些情况下，我们也需要获取网络中间层的值，Pytorch提供了两种方法：</p>
<ul>
<li><p>直接调用我们在模型<code>__init__</code>函数中定义的属性即可</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python">mynet.input_to_hidden_layer(X)<br></code></pre></td></tr></table></figure>
</li>
<li><p>我们也可以在覆写forward函数时指定返回值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        hidden1 = self.input_to_hidden_layer(x)<br>        hidden2 = self.hidden_layer_activation(hidden1)<br>        x = self.hidden_to_output_layer(hidden2)<br>        <span class="hljs-keyword">return</span> x, hidden1<br></code></pre></td></tr></table></figure></li>
</ul>
<h2 id="使用Sequential类构建神经网络"><a href="#使用Sequential类构建神经网络" class="headerlink" title="使用Sequential类构建神经网络"></a>使用<code>Sequential</code>类构建神经网络</h2><p>在PyTorch中，<code>Sequential</code>类是一个简单的容器，允许将模块按照顺序排列在一起，以便构建神经网络。它允许你按顺序添加各种层（如全连接层、激活函数、池化层等），以构建模型的架构，而无需手动定义<code>forward</code>方法。这使得创建简单的神经网络变得更加简洁和直观。</p>
<ol>
<li><strong>构造方法</strong>：<code>Sequential</code>类的构造方法允许你传递一个模块列表作为参数，这些模块将按顺序组成神经网络的层结构。</li>
<li><strong>添加模块</strong>：你可以使用<code>.add_module()</code>方法或直接将模块作为参数传递给<code>Sequential</code>构造函数来添加模块。这些模块将按照它们被添加的顺序被调用。</li>
<li><strong>forward方法</strong>：当你调用<code>Sequential</code>实例的实例时，它会按照添加的顺序依次调用每个模块的<code>forward</code>方法。这意味着你可以像调用普通函数一样调用<code>Sequential</code>实例，并且输入数据会依次通过每个层。</li>
<li><strong>嵌套Sequential</strong>：你可以在<code>Sequential</code>中嵌套另一个<code>Sequential</code>，这使得构建复杂的神经网络变得更加方便。</li>
</ol>
<p>使用<code>Sequential</code>类可以大大简化神经网络模型的构建过程，特别是对于简单的线性层叠加结构而言。然而，对于具有更复杂结构或需要非顺序连接的模型，则可能需要使用<code>nn.Module</code>的子类来更灵活地定义<code>forward</code>方法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># my_net = MyNeuralNet().to(device)</span><br><span class="hljs-comment"># 删除MyNeuralNet类直接使用 Sequential类定义模型架构</span><br>my_net = nn.Sequential(<br>    nn.Linear(<span class="hljs-number">2</span>, <span class="hljs-number">8</span>),<br>    nn.ReLU(),<br>    nn.Linear(<span class="hljs-number">8</span>, <span class="hljs-number">1</span>)<br>).to(device)<br></code></pre></td></tr></table></figure>

<h2 id="保存并加载模型"><a href="#保存并加载模型" class="headerlink" title="保存并加载模型"></a>保存并加载模型</h2><p>要保存一个完整的神经网络模型，通常需要保存以下要素：</p>
<ol>
<li><strong>模型的架构</strong>：包括神经网络的层次结构和参数。这可以通过将模型的定义保存为代码或元数据的形式来实现。</li>
<li><strong>模型的权重和偏置参数</strong>：这些是模型在训练过程中学习到的参数，用于将输入数据映射到输出。你需要将这些参数保存在文件中，以便在以后的推理过程中加载和使用。</li>
<li><strong>模型状态</strong>：包括模型的当前状态，如优化器的状态或训练过程中的其他状态信息。这些信息可能有助于在以后的推理或继续训练过程中保持模型的连续性。</li>
<li><strong>模型元数据</strong>：可能包括模型的输入规格、输出规格、激活函数类型等信息。这些元数据可以帮助在加载模型时正确地配置模型。</li>
<li><strong>可选：优化器状态</strong>：如果需要在以后的训练过程中继续训练模型，则可能需要保存优化器的状态信息。</li>
</ol>
<h3 id="state-dict"><a href="#state-dict" class="headerlink" title="state dict"></a>state dict</h3><p>pytorch中提供了 model.state_dict() 方法，返回一个包含键和值的字典，也可以理解为一个存放模型快照的字典。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python">my_net.state_dict()<br>```<br>OrderedDict([(<span class="hljs-string">&#x27;0.weight&#x27;</span>,<br>              tensor([[ <span class="hljs-number">0.8780</span>,  <span class="hljs-number">0.4156</span>],<br>                      [-<span class="hljs-number">0.6139</span>, -<span class="hljs-number">0.4396</span>],<br>                      [ <span class="hljs-number">0.3576</span>, -<span class="hljs-number">0.4563</span>],<br>                      [-<span class="hljs-number">0.2003</span>, -<span class="hljs-number">0.1653</span>],<br>                      [ <span class="hljs-number">0.2468</span>,  <span class="hljs-number">0.8232</span>],<br>                      [ <span class="hljs-number">0.1288</span>, -<span class="hljs-number">0.2658</span>],<br>                      [ <span class="hljs-number">0.2706</span>, -<span class="hljs-number">0.5380</span>],<br>                      [-<span class="hljs-number">0.5914</span>, -<span class="hljs-number">0.6458</span>]], device=<span class="hljs-string">&#x27;cuda:0&#x27;</span>)),<br>             (<span class="hljs-string">&#x27;0.bias&#x27;</span>,<br>              tensor([ <span class="hljs-number">0.3281</span>,  <span class="hljs-number">0.6249</span>, -<span class="hljs-number">0.6074</span>, -<span class="hljs-number">0.5196</span>,  <span class="hljs-number">0.0526</span>,  <span class="hljs-number">0.3178</span>,  <span class="hljs-number">0.2535</span>, -<span class="hljs-number">0.5681</span>],<br>                     device=<span class="hljs-string">&#x27;cuda:0&#x27;</span>)),<br>             (<span class="hljs-string">&#x27;2.weight&#x27;</span>,<br>              tensor([[ <span class="hljs-number">0.8013</span>, -<span class="hljs-number">0.0154</span>,  <span class="hljs-number">0.2462</span>, -<span class="hljs-number">0.2146</span>,  <span class="hljs-number">0.7951</span>, -<span class="hljs-number">0.2686</span>,  <span class="hljs-number">0.0645</span>,  <span class="hljs-number">0.1832</span>]],<br>                     device=<span class="hljs-string">&#x27;cuda:0&#x27;</span>)),<br>             (<span class="hljs-string">&#x27;2.bias&#x27;</span>, tensor([<span class="hljs-number">0.2748</span>], device=<span class="hljs-string">&#x27;cuda:0&#x27;</span>))])<br>```<br></code></pre></td></tr></table></figure>

<h3 id="保存"><a href="#保存" class="headerlink" title="保存"></a>保存</h3><p><code>torch.save()</code>函数用于将模型的状态保存到文件中。它的语法如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># torch.save(obj, filepath)</span><br>save_path = <span class="hljs-string">&#x27;my_net.pth&#x27;</span><br>torch.save(my_net.state_dict(), save_path)<br><span class="hljs-comment"># 在文件根目录下出现 my_net.pth 文件</span><br></code></pre></td></tr></table></figure>

<ul>
<li><code>obj</code>：要保存的对象，可以是模型的状态字典、整个模型、优化器的状态字典等。</li>
<li><code>filepath</code>：保存文件的路径，可以是文件名或包含路径的完整文件路径。</li>
</ul>
<h3 id="加载"><a href="#加载" class="headerlink" title="加载"></a>加载</h3><p><code>torch.load()</code>函数用于从文件中加载保存的对象。它的语法如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># torch.load(filepath, map_location=None)</span><br>model = nn.Sequential(<br>    nn.Linear(<span class="hljs-number">2</span>, <span class="hljs-number">8</span>),<br>    nn.ReLU(),<br>    nn.Linear(<span class="hljs-number">8</span>, <span class="hljs-number">1</span>)<br>).to(device)<br><br><br><span class="hljs-comment"># 将模型加载入刚才创建的未训练模型</span><br>load_path = <span class="hljs-string">&#x27;my_net.pth&#x27;</span><br>model.load_state_dict(torch.load(load_path, map_location=torch.device(device)))<br><br>model(torch.tensor([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]).<span class="hljs-built_in">float</span>().to(device))<br><span class="hljs-comment"># tensor([5.3419], device=&#x27;cuda:0&#x27;, grad_fn=&lt;ViewBackward0&gt;)</span><br></code></pre></td></tr></table></figure>

<ul>
<li><code>filepath</code>：保存文件的路径，可以是文件名或包含路径的完整文件路径。</li>
<li><code>map_location</code>：一个可选参数，用于指定将数据加载到的设备。如果不指定，默认会加载到CPU上。</li>
</ul>
<h4 id="查看模型结构"><a href="#查看模型结构" class="headerlink" title="查看模型结构"></a>查看模型结构</h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> torchsummary <span class="hljs-keyword">import</span> summary<br><span class="hljs-comment"># 需要指定模型输入值的形状</span><br>summary(model, (<span class="hljs-number">1</span>, <span class="hljs-number">2</span>))<br></code></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title>01.神经网络基础</title>
    <url>/Deep%20Learning/01.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h2 id="比较人工智能和传统机器学习"><a href="#比较人工智能和传统机器学习" class="headerlink" title="比较人工智能和传统机器学习"></a>比较人工智能和传统机器学习</h2><ul>
<li>机器学习<ul>
<li>需要专业人员花费大量时间进行特征提取</li>
<li>对于模糊情况容易判断失误</li>
</ul>
</li>
<li>神经网络<ul>
<li>只需要手动调整策略</li>
<li>由模型自动获取特征</li>
<li>需要大量样本</li>
</ul>
</li>
</ul>
<h2 id="人工神经网络的构成"><a href="#人工神经网络的构成" class="headerlink" title="人工神经网络的构成"></a>人工神经网络的构成</h2><p>人工神经网络是一系列张量（权重）和数学运算的组合。可以将人工神经网络看作一个函数，输入一个或多个张量，输出一个或多个张量。</p>
<p>人工神经网络由如下模块构成：</p>
<ul>
<li>输入层：将自变量作为输入</li>
<li>隐藏（中间）层：该层连接输入层和输出层，并对输入数据进行转换。隐藏层可以有很多层。</li>
<li>输出层：包含了输入变量期望产生的值</li>
</ul>
<h3 id="神经元和激活函数"><a href="#神经元和激活函数" class="headerlink" title="神经元和激活函数"></a>神经元和激活函数</h3><p><img src="/pictures%5C01.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%5C01.%E7%A5%9E%E7%BB%8F%E5%85%83%E5%92%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.png" class="lazy" data-srcset="/pictures%5C01.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%5C01.%E7%A5%9E%E7%BB%8F%E5%85%83%E5%92%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="img"></p>
<p>上图是一个基本的神经元模型：</p>
<ul>
<li>x1, x2, x3, 是输入变量</li>
<li>b是偏置值。从生物学上解释，在脑神经细胞中，一定是**<a href="https://www.zhihu.com/search?q=%E8%BE%93%E5%85%A5%E4%BF%A1%E5%8F%B7&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:2848356271%7D">输入信号</a>的电平&#x2F;电流大于某个临界值时**，神经元细胞才会处于兴奋状态，因此我们也需要使用偏置值来增强神经元的效率</li>
<li>w1, w2, w3是每个输入变量的权重</li>
<li>$A &#x3D; f(b + \sum_{i&#x3D;1}^{n}w_ix_i)$，其中函数f是激活函数，激活函数可以使神经网络从线性变化转化为非线性变化，由此实现任意曲面的拟合。</li>
</ul>
<h4 id="常用激活函数"><a href="#常用激活函数" class="headerlink" title="常用激活函数"></a>常用激活函数</h4><p>激活函数应该<strong>定义为任意处</strong> ，并且<strong>在实数空间中任意处都是连续的</strong> 。这个函数还要求在整个实数空间上是<strong>可微</strong>的，以下是常用激活函数</p>
<p><img src="/pictures/01.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/02.%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.png" class="lazy" data-srcset="/pictures/01.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/02.%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="02.常见的激活函数"></p>
<ul>
<li><p>sigmoid: $(0, 1)$  在靠近0时输出变化大，但在其他位置梯度趋于平缓，而且只输出正值，一般用于二分类问题</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>	<span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span>+np.exp(-x)<br></code></pre></td></tr></table></figure></li>
<li><p>Tanh: $(-1,1)$，Sigmoid的一种放大版本，解决sigmoid输出的符号相同问题</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">tanh</span>(<span class="hljs-params">x</span>):<br>	<span class="hljs-keyword">return</span> (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))<br></code></pre></td></tr></table></figure></li>
<li><p><code>ReLU</code>: $max(0,x)$ 输入不大于0则不会激活，这提高了神经网络的稀疏性。问题在输入为负值时输出0，这可能导致产生死神经元（永远不会激活）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">relu</span>(<span class="hljs-params">x</span>):<br>	<span class="hljs-keyword">return</span> np.where(x&gt;<span class="hljs-number">0</span>, x, <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure></li>
<li><p><code>Leaky ReLU max(0.1x, x)</code>防止死神经元</p>
</li>
<li><p>ELU: $a(e^x-1)$</p>
</li>
<li><p><code>softmax</code>：用于计算某个类别的概率，通过该操作将结果限制在0-1内，对于处理多分类问题比较好用，最好在输出层使用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax</span>(<span class="hljs-params">x</span>):<br>	<span class="hljs-keyword">return</span> np.exp(x) / np.<span class="hljs-built_in">sum</span>(np.exp(x))<br></code></pre></td></tr></table></figure></li>
</ul>
<h2 id="前向传播（forward）"><a href="#前向传播（forward）" class="headerlink" title="前向传播（forward）"></a>前向传播（forward）</h2><p>简单理解就是将上一层的输出作为下一层的输入，并计算下一层的输出，直到运算到输出层。</p>
<p>接下来通过以下操作理解前向传播：</p>
<ol>
<li><p>计算隐藏层的值：</p>
<ul>
<li>分配权重，一般在训练开始前使用随机权重初始化。</li>
<li>输入与权重相乘，计算隐藏单元的值 $h_i&#x3D;x_1w_1 + x_2w_2…$</li>
</ul>
</li>
<li><p>应用激活函数</p>
<ul>
<li>应用激活函数，前一步$h_i$与自变量输入依旧是线性关系，需要使用激活函数改变将其改变为非线性关系，结果 $A_i&#x3D;f(h_i)$可以视为下一层的自变量。</li>
<li>通过不断进行以上循环直到输出层。</li>
</ul>
</li>
<li><p>计算损失值</p>
<ol>
<li><p>损失值表示模型的准确程度，损失值越小，则模型越准确</p>
</li>
<li><p>假设$\overline {y_i} &#x3D; f_1(f_2(f_3(…)))$为输出层的结果（预估值），$y_i$为应该输出的值（实际值），m为数据点总数</p>
</li>
<li><p>记 p 为预估值数组，y为实际值数组</p>
</li>
<li><p>连续变量的损失值计算</p>
<ul>
<li><p>一般使用实际值与预估值差的平方作为损失值，即<strong>均方误差</strong> $j&#x3D;\frac{1}{m}\sum_{i&#x3D;1}^{m}(y_i-\overline {y_i})^2$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">mse</span>(<span class="hljs-params">p, y</span>):<br>	<span class="hljs-keyword">return</span> np.mean(np.square(p-y))<br></code></pre></td></tr></table></figure>
</li>
<li><p>还可以使用<strong>平均值绝对误差</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">mae</span>(<span class="hljs-params">p, y</span>):<br>    <span class="hljs-keyword">return</span> np.mean(np.<span class="hljs-built_in">abs</span>(p-y))<br></code></pre></td></tr></table></figure>
</li>
<li><p>一般在预期输出小于1时，使用均方误差减小损失</p>
</li>
</ul>
</li>
<li><p>离散变量的损失值计算（变量中只有几个类别）</p>
<ul>
<li><p>交叉熵的概念：交叉熵是信息论中的一个概念，用于衡量两个概率分布之间的差异。交叉熵的值越小，表示两个概率分布越接近，也就是模型的预测越准确。</p>
</li>
<li><p>当预测变量只有两个不同取值时，使用<strong>二元交叉熵</strong>为损失函数：$-\frac{1}{m}\sum_{i&#x3D;1}^{m}(y_i\log{\overline{y_i}} + (1-y_i)\log(1-\overline{y_i}))$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">binary_cross_entropy</span>(<span class="hljs-params">p, y</span>):<br>	<span class="hljs-keyword">return</span> -np.mean(np.<span class="hljs-built_in">sum</span>((y*np.log(p)) + (<span class="hljs-number">1</span>-y)*(np.log(<span class="hljs-number">1</span>-p))))<br></code></pre></td></tr></table></figure>
</li>
<li><p>使用<strong>分类交叉熵</strong>为损失函数，设C为类别总数，损失值为：$-\frac{1}{m}\sum_{j&#x3D;1}^{C}\sum_{i&#x3D;1}^{m}y_i\log{\overline{y_i}}$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">categorical_cross_entropy</span>(<span class="hljs-params">p, y</span>):<br>	<span class="hljs-keyword">return</span> -np.mean(np.<span class="hljs-built_in">sum</span>(np.<span class="hljs-built_in">sum</span>(y*np.log(p))))<br></code></pre></td></tr></table></figure>
</li>
<li><p>概率处于0-1之间，因此可能的最小损失为0，最大损失为无穷</p>
</li>
</ul>
</li>
</ol>
</li>
</ol>
<h2 id="反向传播（backward）"><a href="#反向传播（backward）" class="headerlink" title="反向传播（backward）"></a>反向传播（backward）</h2><p>在前向传播的介绍中，我们用随机值作为初始化权重，可预见的损失值也会比较大。在反向传播中，我们需要通过得到的损失值，动态的更新权重和偏置值，使损失值尽可能的减小。</p>
<p>有一种基础的反向传播的步骤如下：</p>
<ol>
<li>少量的修改神经网络中的一个权重值</li>
<li>当权重变化量为$\Delta W$时，计算出损失的变化量为$\Delta L$</li>
<li>使当前权重更新量为 $-k \frac{\Delta L}{\Delta W}$，其中k是一个正值，是一个称为<strong>学习率</strong>的超参数，$\frac{\Delta L}{\Delta W}$可以视为一个微分</li>
<li>反复进行上述步骤，直到修改完每一个权重值</li>
</ol>
<blockquote>
<p> 从3可以看出，如果更改某个权重较大的减小了损失值，则对其更新也会较大，反之对其的更新也会较小。</p>
</blockquote>
<ul>
<li>如果对某个数据集执行了n次反向传播，则称为对模型进行了n轮训练。</li>
<li>通过设置一个学习率，来缓慢的建立信任，而不是直接将权重变化量设为梯度值</li>
<li>通过更新权重来减小误差的整个过程被称为梯度下降</li>
</ul>
<h3 id="使用链式法则实现反向传播"><a href="#使用链式法则实现反向传播" class="headerlink" title="使用链式法则实现反向传播"></a>使用链式法则实现反向传播</h3><p>在上面的步骤中，每次计算新权重值时都要完整的进行一遍前向传播以计算损失值的变化，当节点非常多时这样的效率会非常低，是否有一种方式能便捷的计算某权重变化后对损失值的影响率呢？</p>
<p>下面以使用均方差做损失函数，sigmoid做激活函数来进行链式求偏导计算权重变化时损失值的变化率。</p>
<ul>
<li>损失值 $C &#x3D; (y-\overline{y})^2$</li>
<li>预测输出值 $\overline{y}&#x3D;a_1w_{21}+a_2w_{22}+a_3w_{23}$</li>
<li>隐藏层激活函数输出 $a_1&#x3D;\frac{1}{1+e^{-h_{1}}}$</li>
<li>隐藏层值 $h_1&#x3D;x_1w_{11}+x_2w_{12}$</li>
<li>故损失值 C对$w_{11}$ 的偏导 $\frac{\partial C}{\partial w_{11}}&#x3D;\frac{\partial C}{\partial {\overline y}}\frac{\partial {\overline y}}{\partial a_{1}}\frac{\partial a_{1}}{\partial h_{1}}\frac{\partial h_{1}}{\partial w_{11}}&#x3D;-2(y-\overline y)w_{21}a_{1}(1-a_{1})x_1$</li>
<li>由上述公式可以通过公式直接计算权重值变化对损失值的影响率，从而避免了每次重新计算前向传播</li>
</ul>
<h3 id="学习率的影响"><a href="#学习率的影响" class="headerlink" title="学习率的影响"></a>学习率的影响</h3><ul>
<li>学习率影响权重变化的速率，成正相关</li>
<li>学习率较大时可能导致权重变化过大，导致之后损失值变化非常小，使权重值无法达到最优值</li>
<li>学习率太小，则权重收敛较慢</li>
<li>学习率的范围一般在 $10^{-6}$ 到1.0之间</li>
</ul>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title>04.卷积神经网络（上）</title>
    <url>/Deep%20Learning/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/</url>
    <content><![CDATA[<h2 id="传统深度神经网络（DNN）的问题"><a href="#传统深度神经网络（DNN）的问题" class="headerlink" title="传统深度神经网络（DNN）的问题"></a>传统深度神经网络（DNN）的问题</h2><p>在此前，我们已经学会了搭建一个传统的深度神经网络。现在，拿出一个训练得比较好（在验证集上90%准确率）的模型，开始做一些测试：</p>
<ol>
<li><p>随便从数据集中拿一张图</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>idx = <span class="hljs-number">666</span><br>plt.imshow(train_data[idx], cmap=<span class="hljs-string">&#x27;gray&#x27;</span>)<br>plt.title(fmnist_train.classes[train_labels[idx]])<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/01%E9%9A%8F%E4%BE%BF%E6%8B%BF%E5%BC%A0%E6%A0%B7%E6%9C%AC.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/01%E9%9A%8F%E4%BE%BF%E6%8B%BF%E5%BC%A0%E6%A0%B7%E6%9C%AC.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240409093438831"></p>
</li>
<li><p>使用模型计算该图像的概率（注意将模型转为评估模式）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax</span>(<span class="hljs-params">x</span>):<br>	<span class="hljs-keyword">return</span> np.exp(x) / np.<span class="hljs-built_in">sum</span>(np.exp(x))<br><span class="hljs-comment"># 按照训练时的处理方法处理图像</span><br>img = train_data[idx] / <span class="hljs-number">255</span><br>img = img.view(-<span class="hljs-number">1</span>, <span class="hljs-number">28</span>*<span class="hljs-number">28</span>)<br>img = img.to(device)<br><span class="hljs-comment"># 计算概率分布</span><br>p = softmax(model(img).cpu().detach().numpy())<br><span class="hljs-comment"># 使用表格表示</span><br>pd.DataFrame(&#123;<span class="hljs-string">&quot;class&quot;</span>: fmnist_train.classes, <span class="hljs-string">&quot;rate&quot;</span>: p[<span class="hljs-number">0</span>]&#125;)<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/02%E6%A8%A1%E5%9E%8B%E7%AE%97%E5%87%BA%E7%9A%84%E6%A6%82%E7%8E%87.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/02%E6%A8%A1%E5%9E%8B%E7%AE%97%E5%87%BA%E7%9A%84%E6%A6%82%E7%8E%87.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240409102043095"></p>
</li>
<li><p>对原始图像进行一些细微的修改，再测试其结果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 记录每次预测结果</span><br>predicts = &#123;&#125;<br><span class="hljs-comment"># 将图像转回为 28*28 以便平移</span><br>img = img.view(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>)<br><span class="hljs-comment"># 平移图像，范围为[-5, 5]</span><br><span class="hljs-keyword">for</span> px <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(-<span class="hljs-number">5</span>,<span class="hljs-number">6</span>):<br>    <span class="hljs-comment"># 上下平移图像</span><br>    img2 = np.roll(img.cpu(), px, axis=<span class="hljs-number">0</span>)<br>    img3 = torch.Tensor(img2).view(-<span class="hljs-number">1</span>, <span class="hljs-number">28</span>*<span class="hljs-number">28</span>).to(device)<br>    p = softmax(model(img3).cpu().detach().numpy())<br>    predicts[<span class="hljs-string">&quot;pixel&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(px)] = p[<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/03%E5%90%91%E4%B8%8A%E5%B9%B3%E7%A7%BB5%E5%83%8F%E7%B4%A0.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/03%E5%90%91%E4%B8%8A%E5%B9%B3%E7%A7%BB5%E5%83%8F%E7%B4%A0.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="平移后的示例"></p>
</li>
<li><p>上图是向上平移5像素后的效果，接下来绘出热力图以直观的感受平移导致的预测结果偏差</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br>plt.title(<span class="hljs-string">&#x27;Probability of each class for various translations&#x27;</span>)<br>sns.heatmap(pd.DataFrame(predicts, index=fmnist_train.classes), fmt=<span class="hljs-string">&#x27;.2f&#x27;</span>)<br></code></pre></td></tr></table></figure></li>
</ol>
<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/04%E5%B9%B3%E7%A7%BB%E5%90%8E%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E7%83%AD%E5%8A%9B%E5%9B%BE.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/04%E5%B9%B3%E7%A7%BB%E5%90%8E%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E7%83%AD%E5%8A%9B%E5%9B%BE.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240409113246482"></p>
<p>通过实践我们发现，即使图像主体没有变化，当像素平移超过两点后，模型便已基本失去作用。这是因为们的模型只学习了特定的样本，导致模型过于依赖于输入的精确特征，而无法对稍微变化的输入进行泛化。</p>
<h2 id="卷积神经网络（Convolutional-Neural-Network-CNN）的基本构成"><a href="#卷积神经网络（Convolutional-Neural-Network-CNN）的基本构成" class="headerlink" title="卷积神经网络（Convolutional Neural Network, CNN）的基本构成"></a>卷积神经网络（Convolutional Neural Network, CNN）的基本构成</h2><p>卷积神经网络通过卷积操作，能有效的捕捉数据的局部特征，从而增强模型的泛化能力</p>
<h3 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h3><p>卷积操做本质上是求矩阵乘法的和，以下图为例，左边是一个2x2的卷积核，右边则是3x3的输入数据。</p>
<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/05%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C%E6%BC%94%E7%A4%BA.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/05%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C%E6%BC%94%E7%A4%BA.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="img"></p>
<ol>
<li>将卷积核视为一个可以滑动的窗口，第一次操作为：$\begin{bmatrix}1 &amp; 0\1 &amp; 0\end{bmatrix}\begin{bmatrix}1 &amp; 2\4 &amp; 5\end{bmatrix}&#x3D;\begin{bmatrix}1 &amp; 2\1 &amp; 2\end{bmatrix}$，将结果矩阵中的值累加得到6，这个结果就是output矩阵的第一行第一列</li>
<li>通过不断执行以上步骤，output矩阵结果为：$\begin{bmatrix}6 &amp; 10\18 &amp; 22\end{bmatrix}$</li>
<li>卷积操作的本质就是通过设定的卷积核，获取周围元素的特征，从而获得更好的泛化能力</li>
</ol>
<h3 id="滤波器"><a href="#滤波器" class="headerlink" title="滤波器"></a>滤波器</h3><p><strong>滤波器</strong>也称为<strong>卷积核（Convolutional Kernel）</strong>或<strong>特征检测器（Feature Detector）</strong>。</p>
<p>通常是一个小型的矩阵，其大小可以根据网络结构和任务需求进行调整。在图像处理中，滤波器通常是二维的，对于每个通道都有自己的滤波器。<strong>滤波器的参数是在训练过程中通过反向传播和梯度下降等优化算法学习得到的</strong>。</p>
<p>滤波器在卷积操作中与输入数据进行逐元素相乘，并将结果相加，从而生成卷积特征图。<strong>通过多个不同滤波器的组合，卷积层可以提取出输入数据的不同特征，例如边缘、纹理、形状等</strong>。</p>
<p>常见的滤波器包括：</p>
<ol>
<li><strong>边缘检测滤波器</strong>：用于检测图像中的边缘和轮廓，例如Sobel滤波器和Prewitt滤波器。</li>
<li><strong>模糊滤波器</strong>：用于平滑图像并模糊细节，例如高斯滤波器和均值滤波器。</li>
<li><strong>锐化滤波器</strong>：用于增强图像的边缘和细节，例如拉普拉斯滤波器和Sobel滤波器的变体。</li>
<li><strong>特定模式检测滤波器</strong>：用于检测图像中特定的模式或形状，例如线条、角点等。</li>
</ol>
<p>滤波器的设计和选择直接影响了网络对输入数据特征的提取能力和性能表现。</p>
<h3 id="步长和填充"><a href="#步长和填充" class="headerlink" title="步长和填充"></a>步长和填充</h3><p>步长（stride）和填充（padding）是两个重要的超参数，用于控制卷积操作的输出大小和形状。</p>
<ol>
<li><strong>步长（Stride）</strong>： 步长是指滤波器在输入数据上滑动的距离。当步长为1时，滤波器每次移动一个像素；当步长为2时，滤波器每次移动两个像素，以此类推。较大的步长会减小输出特征图的大小，而较小的步长则会保持输出特征图的大小接近输入特征图的大小。</li>
<li><strong>填充（Padding）</strong>： 填充是指在输入数据的边界上添加额外的值（通常是0），以控制卷积操作的输出大小和形状。填充可以分为两种类型：<ul>
<li><strong>零填充（Zero Padding）</strong>：在输入数据的边界上添加零值。</li>
<li><strong>有效填充（Valid Padding）</strong>：不在输入数据上进行填充，只在滤波器完全覆盖输入数据的情况下进行卷积操作。这种情况下，输出特征图的大小会随着滤波器大小和步长的改变而变化。</li>
</ul>
</li>
</ol>
<p>步长和填充对输出特征图的大小和形状有着直接的影响：</p>
<ul>
<li>增大步长会减小输出特征图的大小。</li>
<li>增大填充会增加输出特征图的大小。</li>
</ul>
<h3 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h3><p><strong>池化层（Pooling Layer）</strong>：池化层用于减少特征图的空间维度，降低数据量和参数数量，同时保留重要的特征。常用的池化操作包括最大池化（Max Pooling）和平均池化（Average Pooling）。</p>
<ol>
<li><strong>最大池化</strong>：对于每个池化窗口（通常是一个 2x2 的窗口），最大池化操作会选择窗口中的最大值作为输出特征。这样做的好处是原始特征图被压缩，并且保留了最显著特征。能够提高模型对于平移、旋转等变换的不变性。</li>
<li><strong>平均池化</strong>：平均池化操作会计算窗口中所有像素值的平均值，并将其作为输出特征。它更多地保留了局部区域的平均特征。与最大池化相比，它更加平滑，但可能会丢失一些细节信息。</li>
</ol>
<h4 id="扁平层的概念"><a href="#扁平层的概念" class="headerlink" title="扁平层的概念"></a>扁平层的概念</h4><p>前面我们已经学习了由全连接层构成的深度神经网络，在卷积神经网络中，全连接层也被称为扁平层，通过一些列的卷积和池化操作得到特征数据后，再将数据扁平化传入扁平层（可以理解为输入层）。</p>
<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/06%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%A4%BA%E6%84%8F%E5%9B%BE.jpg" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/06%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%A4%BA%E6%84%8F%E5%9B%BE.jpg" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="img"></p>
<p>卷积和池化在深度学习中除了起着自动特征提取的作用，还有助于减少扁平层的节点数量。</p>
<h3 id="卷积和池化的图像平移不变性原理"><a href="#卷积和池化的图像平移不变性原理" class="headerlink" title="卷积和池化的图像平移不变性原理"></a>卷积和池化的图像平移不变性原理</h3><p>还记得开头的测试吗？在传统深度神经网络中，当图像进行了微小的平移后，模型的预测准确度就会急剧下降。</p>
<p>而卷积和池化在图像处理中具有平移不变性：</p>
<ol>
<li><strong>卷积的平移不变性</strong>： 在卷积操作中，滤波器（卷积核）在输入数据上进行滑动操作，并计算得到卷积特征图。如果对输入数据进行平移，那么在滤波器与输入数据进行卷积时，滤波器的相对位置也会相应地发生平移。因此，即使输入数据发生了平移，由于滤波器的平移，最终的卷积特征图仍然能够捕捉到相同的特征。</li>
<li><strong>池化的平移不变性</strong>： 常用的池化方式包括最大池化和平均池化。无论是哪种池化方式，在对输入数据进行池化时，它们都是在局部区域内寻找最大值或计算平均值。因此，如果对输入数据进行平移，局部区域内的最大值或平均值也会相应地发生平移，但整体的池化结果不会受到影响。换句话说，池化操作不关心数据的绝对位置，而只关心局部区域内的特征。</li>
</ol>
<p>总的来说，卷积和池化后的图像不再关注每个像素点的信息，而是对某一小块区域的一种抽象，这使卷积神经网络能具有更好的泛化能力。</p>
<h2 id="使用深度CNN分类图像"><a href="#使用深度CNN分类图像" class="headerlink" title="使用深度CNN分类图像"></a>使用深度CNN分类图像</h2><p>通过前面的学习，我们理解了什么是卷积神经网络，使用卷积神经网络有什么好处，接下来试试使用学到的知识优化前面那个模型。</p>
<p>简洁起见，我这里只贴了将原代码改为卷积神经网络需要修改的代码，其他代码和上一篇文章几乎相同。</p>
<ol>
<li><p>修改数据加载类：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FMNISTDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):     <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, y</span>):<br>        x = x.<span class="hljs-built_in">float</span>() / <span class="hljs-number">255</span> <br>        <span class="hljs-comment"># 因为要用卷积操作，shape用(1, 28, 28)</span><br>        x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>)<br>        self.x, self.y = x, y <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, ix</span>):<br>        x, y = self.x[ix], self.y[ix] <br>        <span class="hljs-keyword">return</span> x.to(device), y.to(device)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>): <br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.x)<br></code></pre></td></tr></table></figure>
</li>
<li><p>修改模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_model</span>():<br>    model = nn.Sequential(<br>        <span class="hljs-comment"># 卷积操作，滤波器为3x3，1个图像输入，64个滤波器</span><br>        nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>),<br>        <span class="hljs-comment"># 最大池化层，池化窗口为2x2</span><br>        nn.MaxPool2d(<span class="hljs-number">2</span>),<br>        nn.LeakyReLU(),<br>        <span class="hljs-comment"># 64个输入，128个滤波器</span><br>        nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">3</span>),<br>        nn.MaxPool2d(<span class="hljs-number">2</span>),<br>        nn.LeakyReLU(),<br>        <span class="hljs-comment"># 扁平化</span><br>        nn.Flatten(),<br>        <span class="hljs-comment"># 下面是深度神经网络的部分，5*5是原始图像经历两次卷积两次池化的结果</span><br>        nn.Linear(<span class="hljs-number">128</span>*<span class="hljs-number">5</span>*<span class="hljs-number">5</span>, <span class="hljs-number">256</span>),<br>        nn.BatchNorm1d(<span class="hljs-number">256</span>),<br>        nn.LeakyReLU(),<br>        nn.Dropout(<span class="hljs-number">0.5</span>),<br>        nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">10</span>),<br>        nn.BatchNorm1d(<span class="hljs-number">10</span>)<br>    ).to(device)<br>    loss_fn = nn.CrossEntropyLoss()<br>    optimizer = SGD(model.parameters(), lr=<span class="hljs-number">1e-2</span>)<br>    <span class="hljs-keyword">return</span> model, loss_fn, optimizer<br></code></pre></td></tr></table></figure>
</li>
<li><p>可以用torch_summary来查看模型的摘要信息</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> torchsummary <span class="hljs-keyword">import</span> summary<br>summary(model, torch.zeros(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>))<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/07%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E6%91%98%E8%A6%81.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/07%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E6%91%98%E8%A6%81.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240409182901235"></p>
</li>
<li><p>训练结果如下图</p>
<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/08%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%88%E6%9E%9C%E5%9B%BE.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/08%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%88%E6%9E%9C%E5%9B%BE.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240409183127350"></p>
</li>
<li><p>测试卷积神经网络对平移的效果：使用卷积后模型相比最初有所提升，但我现在更关注其对图像平移的泛化能力，接下来就让我们试试。</p>
</li>
</ol>
<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/09%E4%BD%BF%E7%94%A8%E5%8D%B7%E7%A7%AF%E5%90%8E%E7%9A%84%E5%B9%B3%E7%A7%BB%E9%A2%84%E6%B5%8B%E7%83%AD%E5%8A%9B%E5%9B%BE.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/09%E4%BD%BF%E7%94%A8%E5%8D%B7%E7%A7%AF%E5%90%8E%E7%9A%84%E5%B9%B3%E7%A7%BB%E9%A2%84%E6%B5%8B%E7%83%AD%E5%8A%9B%E5%9B%BE.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240409184018308"></p>
<p>上图是最终结果，卷积确实增强了模型对图像平移的预测效果，但并未完全解决这个问题。接下来我们学习图像增强技术以进一步解决这个问题</p>
<h2 id="图像增强"><a href="#图像增强" class="headerlink" title="图像增强"></a>图像增强</h2><p>图像增强是一种常用的数据预处理技术，用于扩充训练数据集，以改善模型的泛化能力和抵抗过拟合。它通过对原始数据进行一系列随机变换或变形来生成新的训练样本，从而增加数据的多样性，使模型更好地学习数据的不变性和泛化能力。</p>
<p>常用的图像增强技术包括：</p>
<ol>
<li><strong>随机旋转（Random Rotation）</strong>：对图像进行随机旋转，以模拟不同角度下的观察情况。</li>
<li><strong>随机缩放（Random Scaling）</strong>：对图像进行随机缩放，以模拟不同尺度下的观察情况。</li>
<li><strong>随机平移（Random Translation）</strong>：对图像进行随机平移，以模拟不同位置下的观察情况。</li>
<li><strong>水平翻转（Horizontal Flipping）</strong>：对图像进行水平翻转，以模拟镜像对称的观察情况。</li>
<li><strong>随机剪裁（Random Cropping）</strong>：对图像进行随机剪裁，以模拟不同裁剪区域下的观察情况。</li>
<li><strong>颜色变换（Color Jittering）</strong>：对图像的颜色进行随机变换，如亮度、对比度、饱和度等。</li>
<li><strong>添加噪声（Adding Noise）</strong>：向图像中添加随机噪声，以增加数据的多样性。</li>
<li><strong>随机变形（Random Deformation）</strong>：对图像进行随机变形，以模拟不同形状和姿态下的观察情况。</li>
</ol>
<p>这些技术可以单独应用，也可以组合使用。在训练过程中，每个样本都会经过随机选择的一系列数据增强操作，从而生成多样化的训练样本。这样做可以有效地提高模型的鲁棒性，减少过拟合，并且更好地适应不同的输入变化和噪声。</p>
<h3 id="使用imgaug库进行图像增强"><a href="#使用imgaug库进行图像增强" class="headerlink" title="使用imgaug库进行图像增强"></a>使用imgaug库进行图像增强</h3><p><code>imgaug</code> 是一个用于图像增强的 Python 库，可以用于生成具有多样性的训练数据。它支持许多图像增强技术，它比较突出的增强技术如下：仿射变换、改变亮度、增加噪声。</p>
<h4 id="仿射变换"><a href="#仿射变换" class="headerlink" title="仿射变换"></a>仿射变换</h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> imgaug.augmenters <span class="hljs-keyword">as</span> iaa<br><br>iaa.Affine(scale=<span class="hljs-literal">None</span>, translate_percent=<span class="hljs-literal">None</span>, translate_px=<span class="hljs-literal">None</span>,<br>                 rotate=<span class="hljs-literal">None</span>, shear=<span class="hljs-literal">None</span>, order=<span class="hljs-number">1</span>, cval=<span class="hljs-number">0</span>, mode=<span class="hljs-string">&quot;constant&quot;</span>,<br>                 fit_output=<span class="hljs-literal">False</span>, backend=<span class="hljs-string">&quot;auto&quot;</span>,<br>                 seed=<span class="hljs-literal">None</span>, name=<span class="hljs-literal">None</span>,<br>                 random_state=<span class="hljs-string">&quot;deprecated&quot;</span>, deterministic=<span class="hljs-string">&quot;deprecated&quot;</span>)<br></code></pre></td></tr></table></figure>

<ul>
<li>scale 指定对图像的缩放量</li>
<li>translate_percent指定平移百分比</li>
<li>translate_px指定平移像素的绝对值</li>
<li>rotate指定旋转量</li>
<li>shear指定旋转中心</li>
<li>cval表示填充的像素，默认0（黑色）</li>
</ul>
<h4 id="改变亮度"><a href="#改变亮度" class="headerlink" title="改变亮度"></a>改变亮度</h4><ol>
<li><p>Multiply() ：直接对所有值乘一个值</p>
<figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs Py">iaa.Multiply(mul=(<span class="hljs-number">0.8</span>, <span class="hljs-number">1.2</span>), per_channel=<span class="hljs-literal">False</span>,<br>                 seed=<span class="hljs-literal">None</span>, name=<span class="hljs-literal">None</span>,<br>                 random_state=<span class="hljs-string">&quot;deprecated&quot;</span>, deterministic=<span class="hljs-string">&quot;deprecated&quot;</span>)<br></code></pre></td></tr></table></figure>
</li>
<li><p>Linearcontrast() ：记像素值为x，输出y。则$y&#x3D;127+a \times (x-127)$</p>
<figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py">iaa.LinearContrast(alpha=(<span class="hljs-number">0.6</span>, <span class="hljs-number">1.4</span>), per_channel=<span class="hljs-literal">False</span>,<br>                 seed=<span class="hljs-literal">None</span>, name=<span class="hljs-literal">None</span>,<br>                 random_state=<span class="hljs-string">&quot;deprecated&quot;</span>, deterministic=<span class="hljs-string">&quot;deprecated&quot;</span>)<br></code></pre></td></tr></table></figure></li>
</ol>
<h4 id="添加噪声"><a href="#添加噪声" class="headerlink" title="添加噪声"></a>添加噪声</h4><ol>
<li><p>Dropout()：随机删除一些像素</p>
<figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py">iaa.Dropout(p=(<span class="hljs-number">0.0</span>, <span class="hljs-number">0.05</span>), per_channel=<span class="hljs-literal">False</span>,<br>                 seed=<span class="hljs-literal">None</span>, name=<span class="hljs-literal">None</span>,<br>                 random_state=<span class="hljs-string">&quot;deprecated&quot;</span>, deterministic=<span class="hljs-string">&quot;deprecated&quot;</span>)<br></code></pre></td></tr></table></figure>
</li>
<li><p>SaltAndPepper：随机向图像添加黑色和白色的像素</p>
<figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py">iaa.SaltAndPepper(p=(<span class="hljs-number">0.0</span>, <span class="hljs-number">0.03</span>), per_channel=<span class="hljs-literal">False</span>,<br>                 seed=<span class="hljs-literal">None</span>, name=<span class="hljs-literal">None</span>,<br>                 random_state=<span class="hljs-string">&quot;deprecated&quot;</span>, deterministic=<span class="hljs-string">&quot;deprecated&quot;</span>)<br></code></pre></td></tr></table></figure>
</li>
<li><p>GaussianBlur()：高斯模糊</p>
<figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py">iaa.GaussianBlur(sigma=(<span class="hljs-number">0.0</span>, <span class="hljs-number">3.0</span>),<br>                 seed=<span class="hljs-literal">None</span>, name=<span class="hljs-literal">None</span>,<br>                 random_state=<span class="hljs-string">&quot;deprecated&quot;</span>, deterministic=<span class="hljs-string">&quot;deprecated&quot;</span>)<br></code></pre></td></tr></table></figure></li>
</ol>
<blockquote>
<p> 你是否注意到很多参数的属性是一个元组，实际上这表示一个区间，<strong>函数增强程度会随机在该区间内</strong>。</p>
</blockquote>
<h4 id="实际演示："><a href="#实际演示：" class="headerlink" title="实际演示："></a>实际演示：</h4><ol>
<li><p>单个操作：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">import</span> imgaug.augmenters <span class="hljs-keyword">as</span> iaa<br><br>trans = iaa.Affine(scale=<span class="hljs-number">1.2</span>, translate_percent=<span class="hljs-number">0.1</span>, rotate=(-<span class="hljs-number">90</span>, <span class="hljs-number">90</span>), cval=<span class="hljs-number">127</span>)<br>img = trans.augment_image(to_numpy(train_data[<span class="hljs-number">666</span>]))<br>plt.grid(<span class="hljs-literal">False</span>)<br>plt.imshow(img, cmap=<span class="hljs-string">&quot;gray&quot;</span>)<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/10imgaug%E5%8D%95%E6%AC%A1%E6%93%8D%E4%BD%9C.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/10imgaug%E5%8D%95%E6%AC%A1%E6%93%8D%E4%BD%9C.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="10imgaug单次操作"></p>
</li>
<li><p>组合操作：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py">trans = iaa.Sequential([<br>    iaa.Affine(translate_px=&#123;<span class="hljs-string">&#x27;x&#x27;</span>:(-<span class="hljs-number">10</span>,<span class="hljs-number">10</span>)&#125;),<br>    iaa.GaussianBlur(),<br>    iaa.Multiply(),<br>    iaa.SaltAndPepper(),<br>])<br>img = trans.augment_image(to_numpy(train_data[<span class="hljs-number">666</span>]))<br>plt.grid(<span class="hljs-literal">False</span>)<br>plt.imshow(img, cmap=<span class="hljs-string">&quot;gray&quot;</span>)<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/11imgaug%E7%BB%84%E5%90%88%E6%93%8D%E4%BD%9C.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/11imgaug%E7%BB%84%E5%90%88%E6%93%8D%E4%BD%9C.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240409213736734"></p>
</li>
</ol>
<h4 id="按批次增强图像collate-fn"><a href="#按批次增强图像collate-fn" class="headerlink" title="按批次增强图像collate_fn"></a>按批次增强图像<code>collate_fn</code></h4><p><code>imgaug</code>库提供了<code>trans.augment_images</code>方法用于批量增强图像，该方法的速度比迭代快不少，所以我们希望在训练时能<strong>一次增强一批数据</strong>。</p>
<p><code>collate_fn</code> 是 DataLoader 类的一个参数，用于处理批次中的样本数据。它可以自定义批次数据的处理方式，比如对样本进行填充、调整大小等操作。通常情况下，<code>collate_fn</code> 接收一个批次的样本列表，并返回一个批次的数据。这个函数需要根据具体情况来定义，以确保输入和输出的格式与你的模型匹配。</p>
<p>需要修改的代码如下：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FMNISTDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-comment"># 只传入一个图像增强的方法，并不在这一步处理图像</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, y, aug=<span class="hljs-literal">None</span></span>):<br>        self.x, self.y = x, y<br>        self.aug = aug<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, ix: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">tuple</span>[torch.Tensor, torch.Tensor]:<br>        x, y = self.x[ix], self.y[ix]<br>        <span class="hljs-keyword">return</span> x, y<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>): <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.x)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">collate_fn</span>(<span class="hljs-params">self, batch</span>) -&gt; <span class="hljs-built_in">tuple</span>[torch.Tensor, torch.Tensor]:<br>        <span class="hljs-comment"># 分成两个元组，元组内的元素为张量</span><br>        ims, classes = <span class="hljs-built_in">zip</span>(*batch)<br>        <span class="hljs-comment"># 通过堆叠将元组变为张量，tensor.shape(batch_size, 28, 28)</span><br>        ims = torch.stack(ims)<br>        <span class="hljs-keyword">if</span> self.aug:<br>            <span class="hljs-comment"># 先将数据转换为 NumPy 数组，再应用数据增强</span><br>            ims = self.aug.augment_images(images=ims.cpu().numpy())<br>            <span class="hljs-comment"># 将增强后的数据转换为张量并移到设备上</span><br>            ims = torch.tensor(ims, dtype=torch.float32)<br>        <span class="hljs-comment"># 如果没有数据增强，仅将数据标准化到 [0, 1] 范围</span><br>        ims = ims.to(device) / <span class="hljs-number">255</span><br>        classes = torch.tensor(classes).to(device)<br>        <span class="hljs-comment"># 保持张量维度一致</span><br>        <span class="hljs-keyword">return</span> ims[:, <span class="hljs-literal">None</span>, :, :], classes<br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_data</span>(): <br>    <span class="hljs-comment"># 增加的参数记得加上</span><br>    train = FMNISTDataset(train_data, train_labels, aug)<br>    <span class="hljs-comment"># 告诉加载器使用collate_fn方法</span><br>    trn_dl = DataLoader(train, batch_size=<span class="hljs-number">64</span>, collate_fn=train.collate_fn, shuffle=<span class="hljs-literal">True</span>)<br><br>    validation = FMNISTDataset(validation_data, validation_labels)<br>    <span class="hljs-comment"># 告诉加载器使用collate_fn方法</span><br>    val_dl = DataLoader(validation, batch_size=<span class="hljs-built_in">len</span>(validation_data), collate_fn=train.collate_fn, shuffle=<span class="hljs-literal">False</span>)<br>    <span class="hljs-keyword">return</span> trn_dl, val_dl<br></code></pre></td></tr></table></figure>

<blockquote>
<p>版本问题，以上代码与书中有所不同</p>
</blockquote>
<h3 id="使用torchvision-transforms库进行图像增强"><a href="#使用torchvision-transforms库进行图像增强" class="headerlink" title="使用torchvision.transforms库进行图像增强"></a>使用torchvision.transforms库进行图像增强</h3><p>transforms库是torch内部提供的图像增强库，这意味着它<strong>可以在gpu上处理图像</strong>。</p>
<ol>
<li>裁剪<ul>
<li>中心裁剪：transforms.CenterCrop</li>
<li>随机裁剪：transforms.RandomCrop</li>
<li>随机长宽比裁剪：transforms.RandomResizedCrop</li>
<li>上下左右中心裁剪：transforms.FiveCrop</li>
<li>上下左右中心裁剪后翻转，transforms.TenCrop</li>
</ul>
</li>
<li>翻转和旋转<ul>
<li>依概率p水平翻转：transforms.RandomHorizontalFlip(p&#x3D;0.5)</li>
<li>依概率p垂直翻转：transforms.RandomVerticalFlip(p&#x3D;0.5)</li>
<li>随机旋转：transforms.RandomRotation</li>
</ul>
</li>
<li>图像变换<ul>
<li>resize：transforms.Resize</li>
<li>标准化：transforms.Normalize</li>
<li>转为tensor，并归一化至[0-1]：transforms.ToTensor</li>
<li>填充：transforms.Pad</li>
<li>修改亮度、对比度和饱和度：transforms.ColorJitter</li>
<li>转灰度图：transforms.Grayscale</li>
<li>线性变换：transforms.LinearTransformation()</li>
<li>仿射变换：transforms.RandomAffine</li>
<li>依概率p转为灰度图：transforms.RandomGrayscale</li>
<li>将数据转换为PILImage：transforms.ToPILImage</li>
<li>transforms.Lambda：Apply a user-defined lambda as a transform.</li>
</ul>
</li>
<li>对transforms操作<ul>
<li>transforms.RandomChoice(transforms)， 从给定的一系列transforms中选一个进行操作</li>
<li>transforms.RandomApply(transforms, p&#x3D;0.5)，给一个transform加上概率，依概率进行操作</li>
<li>transforms.RandomOrder，将transforms中的操作随机打乱</li>
</ul>
</li>
</ol>
<blockquote>
<p>参数 size<code>可以是</code>tuple<code>也可以是</code>Integer</p>
</blockquote>
<p>下面是一些实操演示：</p>
<ol>
<li><p>单个操作</p>
<figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py">img = train_data[idx]<br><span class="hljs-comment"># 图像中心放大，尺寸为30*20（超出的尺寸自动填充0）</span><br>img1 = transforms.CenterCrop((<span class="hljs-number">30</span>, <span class="hljs-number">20</span>))(img)<br>plt.imshow(img1, cmap=<span class="hljs-string">&quot;gray&quot;</span>)<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/12transforms%E5%8D%95%E6%AC%A1%E6%93%8D%E4%BD%9C.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/12transforms%E5%8D%95%E6%AC%A1%E6%93%8D%E4%BD%9C.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240409193033316"></p>
</li>
<li><p>组合操作</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python">transform = transforms.Compose([<br>    <span class="hljs-comment"># tensor 转为 pil 图像</span><br>    transforms.ToPILImage(),<br>    <span class="hljs-comment"># 随机旋转，范围为左右80度，默认中心旋转，填充0</span><br>    transforms.RandomRotation(<span class="hljs-number">80</span>),<br>    <span class="hljs-comment"># 先在四周镜像填充20的padding，再随机裁剪图像为30*30</span><br>    transforms.RandomCrop(<span class="hljs-number">30</span>, padding=<span class="hljs-number">5</span>, padding_mode=<span class="hljs-string">&#x27;reflect&#x27;</span>)<br>    <span class="hljs-comment"># 图像缩放</span><br>    transforms.Resize((<span class="hljs-number">20</span>, <span class="hljs-number">28</span>)),<br>])<br>plt.imshow(transform(img), cmap=<span class="hljs-string">&quot;gray&quot;</span>)<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/13transforms%E7%BB%84%E5%90%88%E6%93%8D%E4%BD%9C.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/13transforms%E7%BB%84%E5%90%88%E6%93%8D%E4%BD%9C.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240409200407825"></p>
</li>
</ol>
<blockquote>
<p>这个库功能不如imgaug包中的augmenters全面</p>
</blockquote>
<h4 id="整合transforms"><a href="#整合transforms" class="headerlink" title="整合transforms"></a>整合transforms</h4><p>transforms没有提供批量增强的方法，因此我们直接在原方法上修改：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FMNISTDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, y, transform=<span class="hljs-literal">None</span></span>):<br>        self.x, self.y = x, y <br>        self.transform = transform<br><br>	<span class="hljs-comment"># 在获取数据时再进行图像增强操作</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, ix: <span class="hljs-built_in">int</span></span>):<br>        x, y = self.x[ix], self.y[ix]<br>        <span class="hljs-keyword">if</span> self.transform:<br>            <span class="hljs-comment"># 使用transforms.ToTensor()进行数据缩放</span><br>            x = self.transform(x)<br>        x = x.<span class="hljs-built_in">float</span>().view(<span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>)<br>        <span class="hljs-keyword">return</span> x.to(device), y.to(device)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>): <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.x)<br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_data</span>(): <br>    <span class="hljs-comment"># 增加的参数记得加上</span><br>    train = FMNISTDataset(train_data, train_labels, transform)<br>    trn_dl = DataLoader(train, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>)<br><br>    validation = FMNISTDataset(validation_data, validation_labels)<br>    val_dl = DataLoader(validation, batch_size=<span class="hljs-built_in">len</span>(validation_data), shuffle=<span class="hljs-literal">False</span>)<br>    <span class="hljs-keyword">return</span> trn_dl, val_dl<br></code></pre></td></tr></table></figure>

<h3 id="两种库图像增强速度的比较"><a href="#两种库图像增强速度的比较" class="headerlink" title="两种库图像增强速度的比较"></a>两种库图像增强速度的比较</h3><p>图像增强是一种非常耗时的操作，因此我希望能比较二者的能力，选择比较快的方式进行图像增强。我的设想是对每张图片进行随机平移和旋转操作，计算每轮时间</p>
<p>因为设备环境可能不同，所以该测试仅供参考，使用设备：</p>
<ul>
<li>windows11</li>
<li>cpu: i5 12400</li>
<li>gpu: 3060 12g</li>
</ul>
<h4 id="augimg"><a href="#augimg" class="headerlink" title="augimg"></a>augimg</h4><figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py">aug = iaa.Sequential([    <br>    <span class="hljs-comment"># rotate 表示旋转角度，translate_percent便是</span><br>    iaa.Affine(rotate=(-<span class="hljs-number">90</span>, <span class="hljs-number">90</span>), translate_percent=&#123;<span class="hljs-string">&quot;x&quot;</span>: (-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>), <span class="hljs-string">&quot;y&quot;</span>: (-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>)&#125;),<br>])<br><span class="hljs-comment"># epochs：10 | time: 5min 29s | epochs average time：32.9s</span><br><span class="hljs-comment"># cpu 占用：90% 左右 | gpu 占用：32% 左右</span><br></code></pre></td></tr></table></figure>

<h4 id="transforms"><a href="#transforms" class="headerlink" title="transforms"></a>transforms</h4><figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py">transform = transforms.Compose([<br>    <span class="hljs-comment"># tensor 转为 pil 图像</span><br>    transforms.ToPILImage(),<br>    <span class="hljs-comment"># 仿射变换，degrees表示左右偏转角度，translate表示(x,y)轴的平移百分比，会随机在该区间内做图像变换</span><br>    transforms.RandomAffine(degrees=<span class="hljs-number">90</span>, translate=(<span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>)),<br>    <span class="hljs-comment"># 重要！！！该操作会自动将数据缩放到[0, 1]区间</span><br>    transforms.ToTensor(),<br>])<br><span class="hljs-comment"># epochs：10 | time: 5min 51s | epochs average time：35.1s</span><br><span class="hljs-comment"># cpu 占用：30% 左右 | gpu 占用：96% 左右 </span><br></code></pre></td></tr></table></figure>

<p>实际上速度差不多，<code>augimg</code>略胜一筹，可能就是因为其对批量增强做了优化导致。</p>
<h3 id="用于图像平移的数据增强"><a href="#用于图像平移的数据增强" class="headerlink" title="用于图像平移的数据增强"></a>用于图像平移的数据增强</h3><p>掌握图像增强后，继续尝试解决前面的图像平移问题，这次训练前</p>
<ul>
<li><p>按照上次的经验，在卷积层后加了一次dropout操作以减少过拟合</p>
<figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py">model = nn.Sequential(<br>        nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>),<br>        nn.MaxPool2d(<span class="hljs-number">2</span>),<br>        nn.LeakyReLU(),<br>        nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">3</span>),<br>        nn.MaxPool2d(<span class="hljs-number">2</span>),<br>        nn.LeakyReLU(),<br>        <span class="hljs-comment"># 增加一次dropout以减少过拟合</span><br>        nn.Dropout(<span class="hljs-number">0.3</span>),<br>        nn.Flatten(),<br>        nn.Linear(<span class="hljs-number">128</span>*<span class="hljs-number">5</span>*<span class="hljs-number">5</span>, <span class="hljs-number">256</span>),<br>        nn.BatchNorm1d(<span class="hljs-number">256</span>),<br>        nn.LeakyReLU(),<br>        nn.Dropout(<span class="hljs-number">0.3</span>),<br>        nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">10</span>),<br>        nn.BatchNorm1d(<span class="hljs-number">10</span>)<br>    ).to(device)<br></code></pre></td></tr></table></figure>


</li>
<li><p>为了方便验证，修改平移量为像素绝对值</p>
<figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py">aug = iaa.Sequential([  <br>    iaa.Affine(rotate=(-<span class="hljs-number">90</span>, <span class="hljs-number">90</span>), translate_px=&#123;<span class="hljs-string">&quot;x&quot;</span>: (-<span class="hljs-number">5</span>, <span class="hljs-number">5</span>), <span class="hljs-string">&quot;y&quot;</span>: (-<span class="hljs-number">5</span>, <span class="hljs-number">5</span>)&#125;),<br>])<br></code></pre></td></tr></table></figure></li>
</ul>
<ol>
<li><p>训练效果如下，成绩可以说很不好，可能是因为模型的复杂度不够，而有些图片经过增强后会丢失很多特征。按照经验，想要进一步提升需要减小dropout，并增加模型节点数（待办）：</p>
<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/15%E4%BD%BF%E7%94%A8%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E5%90%8E%E7%9A%84%E7%83%AD%E5%8A%9B%E5%9B%BE.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/15%E4%BD%BF%E7%94%A8%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E5%90%8E%E7%9A%84%E7%83%AD%E5%8A%9B%E5%9B%BE.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240411083001010"></p>
</li>
<li><p>即使训练成绩不理想，但模型对上下平移的泛化能力也远超之前：</p>
<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/14%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E5%90%8E%E7%9A%84%E8%AE%AD%E7%BB%83%E6%9B%B2%E7%BA%BF.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/14%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E5%90%8E%E7%9A%84%E8%AE%AD%E7%BB%83%E6%9B%B2%E7%BA%BF.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240411083214407"></p>
</li>
</ol>
<p>到这里，学会了使用神经网络和图像增强技术。下一节：深入理解滤波器的作用</p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title>03.使用pytorch构建深度神经网络（下）</title>
    <url>/Deep%20Learning/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/</url>
    <content><![CDATA[<p>[TOC]</p>
<h1 id="理解不同学习率的影响"><a href="#理解不同学习率的影响" class="headerlink" title="理解不同学习率的影响"></a>理解不同学习率的影响</h1><p>到目前为止，我们一直使用0.01作为学习率，接下来讨论不同学习率对缩放和非缩放数据集的影响</p>
<h2 id="学习率对缩放数据集的影响"><a href="#学习率对缩放数据集的影响" class="headerlink" title="学习率对缩放数据集的影响"></a>学习率对缩放数据集的影响</h2><p>在上一节代码的基础上（SGD优化器），仅修改学习率，查看不同学习率对训练的影响</p>
<h3 id="学习率较大"><a href="#学习率较大" class="headerlink" title="学习率较大"></a>学习率较大</h3><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_model</span>():<br>    model = nn.Sequential(<br>        nn.Linear(<span class="hljs-number">28</span> * <span class="hljs-number">28</span>, <span class="hljs-number">1000</span>),<br>        nn.ReLU(),<br>        nn.Linear(<span class="hljs-number">1000</span>, <span class="hljs-number">10</span>)<br>    ).to(device)<br>    loss_fn = nn.CrossEntropyLoss()<br>    <span class="hljs-comment"># 修改学习率为0.1</span><br>    optimizer = SGD(model.parameters(), lr=<span class="hljs-number">1e-1</span>)<br>    <span class="hljs-keyword">return</span> model, loss_fn, optimizer<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/03%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%BA0.1%E5%AF%B9%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%BD%B1%E5%93%8D.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/03%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%BA0.1%E5%AF%B9%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%BD%B1%E5%93%8D.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406192037177"></p>
<p>注意到损失函数下降速度和准确度提升速度相较学习度为0.01时虽然有所提升，但曲线波动较大，并且相比之下验证值曲线并没有明显优势</p>
<h3 id="学习率适中"><a href="#学习率适中" class="headerlink" title="学习率适中"></a>学习率适中</h3><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 修改学习率为0.001</span><br>optimizer = SGD(model.parameters(), lr=<span class="hljs-number">1e-3</span>)<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/04%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%BA0.001%E5%AF%B9%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%BD%B1%E5%93%8D.png.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/04%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%BA0.001%E5%AF%B9%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%BD%B1%E5%93%8D.png.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406192632126"></p>
<p>注意到损失函数下降速度和准确度提升速度相比学习度为0.01时均有所降低，同时曲线相对更为平滑</p>
<h3 id="学习率较小"><a href="#学习率较小" class="headerlink" title="学习率较小"></a>学习率较小</h3><p>可以预见，当学习率到达0.00001时，其权重值更新会非常慢。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 修改学习率为0.00001</span><br>optimizer = SGD(model.parameters(), lr=<span class="hljs-number">1e-5</span>)<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/05%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%BA0.00001%E5%AF%B9%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%BD%B1%E5%93%8D.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/05%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%BA0.00001%E5%AF%B9%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%BD%B1%E5%93%8D.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406200627434"></p>
<h3 id="不同学习率的不同参数分布"><a href="#不同学习率的不同参数分布" class="headerlink" title="不同学习率的不同参数分布"></a>不同学习率的不同参数分布</h3><p>不知道你是否注意到，在不同学习率时训练曲线与验证曲线的接近程度不同，学习率越低，两个曲线越接近。</p>
<p>我们可以对的模型的参数进行分析，以研究其中的规律，我们的模型有以下四种参数：</p>
<ul>
<li>连接输入层和隐藏层的权重</li>
<li>隐藏层中的偏置项</li>
<li>连接隐藏层和输出层的权重</li>
<li>输出层中的偏置项</li>
</ul>
<p>通过如下方法绘制参数分布情况柱状图</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">for</span> ix, par <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(model.parameters()):<br>    <span class="hljs-keyword">if</span>(ix==<span class="hljs-number">0</span>):<br>        plt.hist(par.cpu().detach().numpy().flatten())<br>        plt.title(<span class="hljs-string">&#x27;Distribution of weights conencting input to hidden layer&#x27;</span>)<br>        plt.show()<br>    <span class="hljs-keyword">elif</span>(ix ==<span class="hljs-number">1</span>):<br>        plt.hist(par.cpu().detach().numpy().flatten())<br>        plt.title(<span class="hljs-string">&#x27;Distribution of biases of hidden layer&#x27;</span>)<br>        plt.show()<br>    <span class="hljs-keyword">elif</span>(ix==<span class="hljs-number">2</span>):<br>        plt.hist(par.cpu().detach().numpy().flatten())<br>        plt.title(<span class="hljs-string">&#x27;Distribution of weights conencting hidden to output layer&#x27;</span>)<br>        plt.show()<br>    <span class="hljs-keyword">elif</span>(ix ==<span class="hljs-number">3</span>):<br>        plt.hist(par.cpu().detach().numpy().flatten())<br>        plt.title(<span class="hljs-string">&#x27;Distribution of biases of output layer&#x27;</span>)<br>        plt.show() <br></code></pre></td></tr></table></figure>

<p>以下是按照学习率从大到小三种情况的参数分布情况（注意参数大小）</p>
<ul>
<li>学习率0.1时的参数分布</li>
</ul>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/06%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%BE%83%E5%A4%A7%E6%97%B6%E7%9A%84%E5%8F%82%E6%95%B0%E5%88%86%E5%B8%83.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/06%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%BE%83%E5%A4%A7%E6%97%B6%E7%9A%84%E5%8F%82%E6%95%B0%E5%88%86%E5%B8%83.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406205241463"></p>
<ul>
<li>学习率0.001时的参数分布</li>
</ul>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/07%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%AD%E7%AD%89%E6%97%B6%E7%9A%84%E5%8F%82%E6%95%B0%E5%88%86%E5%B8%83.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/07%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%AD%E7%AD%89%E6%97%B6%E7%9A%84%E5%8F%82%E6%95%B0%E5%88%86%E5%B8%83.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406205529494"></p>
<ul>
<li>学习率0.00001时的参数分布</li>
</ul>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/08%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%BE%83%E5%B0%8F%E6%97%B6%E7%9A%84%E5%8F%82%E6%95%B0%E5%88%86%E5%B8%83.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/08%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%BE%83%E5%B0%8F%E6%97%B6%E7%9A%84%E5%8F%82%E6%95%B0%E5%88%86%E5%B8%83.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406202843548"></p>
<p>可以看出：</p>
<ul>
<li>学习率越大，参数分布范围也越大</li>
<li>学习率太小可能在局部最优时就收敛了，导致还未到达最优结果</li>
<li><strong>学习效果越好，其权重分布越接近正态分布</strong></li>
</ul>
<h2 id="不同学习率对非缩放数据集的影响"><a href="#不同学习率对非缩放数据集的影响" class="headerlink" title="不同学习率对非缩放数据集的影响"></a>不同学习率对非缩放数据集的影响</h2><p>在前面的学习中，我们设置学习率为0.01且不缩放变量时，模型无法达到一个理想的准确率，后面通过确保变量被限制在某个较小范围内，提升了模型的准确度。</p>
<p>这次我们探讨一下，如果不缩小变量范围，能否通过改变学习率使模型达到一个理想的准确度：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FMNISTDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, y</span>):<br>        <span class="hljs-comment"># 取消缩放像素点</span><br>        <span class="hljs-comment"># x = x.float() / 255 </span><br>        x = x.<span class="hljs-built_in">float</span>()<br>        x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">28</span>*<span class="hljs-number">28</span>)<br>        self.x, self.y = x, y <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, ix</span>):<br>        x, y = self.x[ix], self.y[ix] <br>        <span class="hljs-keyword">return</span> x.to(device), y.to(device)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>): <br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.x)<br></code></pre></td></tr></table></figure>

<ul>
<li><p>不缩小数据且学习率为0.1：</p>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/09%E4%B8%8D%E7%BC%A9%E5%B0%8F%E6%95%B0%E6%8D%AE%E4%B8%94%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%BA0.1.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/09%E4%B8%8D%E7%BC%A9%E5%B0%8F%E6%95%B0%E6%8D%AE%E4%B8%94%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%BA0.1.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406211916990"></p>
</li>
<li><p>不缩小数据且学习率为0.001</p>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/10%E4%B8%8D%E7%BC%A9%E5%B0%8F%E6%95%B0%E6%8D%AE%E4%B8%94%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%BA0.001.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/10%E4%B8%8D%E7%BC%A9%E5%B0%8F%E6%95%B0%E6%8D%AE%E4%B8%94%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%BA0.001.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406212334619"></p>
</li>
<li><p>不缩小数据且学习率为0.00001</p>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/11%E4%B8%8D%E7%BC%A9%E5%B0%8F%E6%95%B0%E6%8D%AE%E4%B8%94%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%BA0.00001.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/11%E4%B8%8D%E7%BC%A9%E5%B0%8F%E6%95%B0%E6%8D%AE%E4%B8%94%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%BA0.00001.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406212610297"></p>
</li>
<li><p>当学习率为0.1时，第一轮就使损失值达到了零，自然也无法继续增加精确度</p>
</li>
<li><p>而通过减小学习率，就能直接避免参数更新过大的问题</p>
</li>
<li><p>通过以上观察，猜测缩减数据值的大小和减小学习率有类似的效果</p>
</li>
<li><p>因此我们在训练模型时，尽量不要使用较大的数据值和学习率，猜测学习率在0.001-0.01间时，数据值在[-1, 1]之间是一个比较合适的区间。</p>
</li>
</ul>
<h1 id="理解不同学习率衰减的影响"><a href="#理解不同学习率衰减的影响" class="headerlink" title="理解不同学习率衰减的影响"></a>理解不同学习率衰减的影响</h1><p>通过对不同学习率下曲线的观察，我们也总结出一些规律：</p>
<ul>
<li>学习率较高时，模型收敛速度较快，能在一开始就达到较好效果，但后续波动较大</li>
<li>学习率较低时，模型收敛速度较慢，但曲线平滑，后期准确率会比较高</li>
</ul>
<p>那我们是否能结合学习率的优点，在训练过程中动态修改学习率，<strong>在训练初期学习率大一些，使得网络收敛迅速，在训练后期学习率小一些，使得网络更好的收敛到最优解。</strong></p>
<p>Pytorch提供了一些相关工具，可以使用这些工具动态降低学习率：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> optim<br><br><span class="hljs-comment"># factor=0.5 表示每次触发该调节器将使学习率变为当前的0.5倍</span><br><span class="hljs-comment"># patience=0 表示只要有1次当前损失值没有改善，则触发调节器</span><br><span class="hljs-comment"># threshold=0.001 阈值表示损失率的改善最低值，低于该值则认为没有改善</span><br><span class="hljs-comment"># verbose=True 表示将打印学习率变化 （已弃用）</span><br><span class="hljs-comment"># min_lr=1e-5 表示学习率下限为1e-5</span><br><span class="hljs-comment"># threshold_mode=&#x27;abs&#x27; 表示阈值是绝对值，也可以用&#x27;rel&#x27;表示百分比</span><br>scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=<span class="hljs-number">0.5</span>, patience=<span class="hljs-number">0</span>, threshold=<span class="hljs-number">0.001</span>, min_lr=<span class="hljs-number">1e-5</span>, threshold_mode=<span class="hljs-string">&#x27;abs&#x27;</span>)<br><br><span class="hljs-comment"># 30轮</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">30</span>):<br>    <span class="hljs-comment"># 获取当前学习率</span><br>    current_lr = optimizer.get_last_lr()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">&#123;epoch&#125;</span>: Learning Rate = <span class="hljs-subst">&#123;current_lr&#125;</span>&quot;</span>)<br>    <span class="hljs-comment"># 这里省略</span><br>    ...<br>    <br>    <span class="hljs-keyword">for</span> ix, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">iter</span>(validation_dataloader)):<br>        x, y = batch<br>        val_is_correct = accuracy(x, y, model)<br>        validation_loss = val_loss(x, y, model)<br>        val_epoch_accuracy = np.mean(val_is_correct)<br>        validation_losses.append(validation_loss)<br>        validation_accuracies.append(val_epoch_accuracy)<br>        <br>        <span class="hljs-comment"># 用于监视每次损失值变化</span><br>        scheduler.step(validation_loss)<br></code></pre></td></tr></table></figure>



<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/12%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F%E7%9A%84%E6%95%88%E6%9E%9C.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/12%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F%E7%9A%84%E6%95%88%E6%9E%9C.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406221959090"></p>
<p>可以看出，使用学习率衰减后曲线的波动有所增加，最后取得的成绩也处于一个较好的水平。</p>
<h1 id="构建更深的神经网络"><a href="#构建更深的神经网络" class="headerlink" title="构建更深的神经网络"></a>构建更深的神经网络</h1><p>在本节，对比具有两个隐藏层和没有隐藏层的模型的性能。注意先将学习率调回0.01。</p>
<h2 id="隐藏层增加一层"><a href="#隐藏层增加一层" class="headerlink" title="隐藏层增加一层"></a>隐藏层增加一层</h2><p>然后修改模型部分代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_model</span>():<br>    model = nn.Sequential(<br>        nn.Linear(<span class="hljs-number">28</span> * <span class="hljs-number">28</span>, <span class="hljs-number">1000</span>),<br>        nn.ReLU(),<br>        <span class="hljs-comment"># 隐藏层增加一层</span><br>        nn.Linear(<span class="hljs-number">1000</span>, <span class="hljs-number">1000</span>),<br>        nn.ReLU(),<br>        nn.Linear(<span class="hljs-number">1000</span>, <span class="hljs-number">10</span>)<br>    ).to(device)<br>    loss_fn = nn.CrossEntropyLoss()<br>    optimizer = SGD(model.parameters(), lr=<span class="hljs-number">1e-2</span>)<br>    <span class="hljs-keyword">return</span> model, loss_fn, optimizer<br></code></pre></td></tr></table></figure>



<h2 id="没有隐藏层"><a href="#没有隐藏层" class="headerlink" title="没有隐藏层"></a>没有隐藏层</h2><p>再修改模型部分代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_model</span>():<br>    model = nn.Sequential(<br>        <span class="hljs-comment"># 没有隐藏层</span><br>        nn.Linear(<span class="hljs-number">28</span> * <span class="hljs-number">28</span>, <span class="hljs-number">10</span>)<br>    ).to(device)<br>    loss_fn = nn.CrossEntropyLoss()<br>    optimizer = SGD(model.parameters(), lr=<span class="hljs-number">1e-2</span>)<br>    <span class="hljs-keyword">return</span> model, loss_fn, optimizer<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/13%E9%9A%90%E8%97%8F%E5%B1%82%E5%A2%9E%E5%8A%A0%E4%B8%80%E5%B1%82%E7%9A%84%E6%95%88%E6%9E%9C.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/13%E9%9A%90%E8%97%8F%E5%B1%82%E5%A2%9E%E5%8A%A0%E4%B8%80%E5%B1%82%E7%9A%84%E6%95%88%E6%9E%9C.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240407170711471"></p>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/14%E6%B2%A1%E6%9C%89%E9%9A%90%E8%97%8F%E5%B1%82%E7%9A%84%E6%95%88%E6%9E%9C.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/14%E6%B2%A1%E6%9C%89%E9%9A%90%E8%97%8F%E5%B1%82%E7%9A%84%E6%95%88%E6%9E%9C.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240407171743670"></p>
<p>比较两幅图，不难发现当使用两层隐藏层时，损失值的下降速度更快，准确率提升的速度也更快</p>
<blockquote>
<p>发现这里与书中的情况不太一样，在书中使用两层隐藏层时出现了严重的过拟合现象</p>
</blockquote>
<h1 id="理解批归一化的影响"><a href="#理解批归一化的影响" class="headerlink" title="理解批归一化的影响"></a>理解批归一化的影响</h1><p>在前面的学习中，我们学习到了通过对输入数据进行归一化之后，能有效地提升模型的训练效果。</p>
<p>在此基础上，我们还可以对隐藏层的输入值进行归一化操作，这样的操作被称为<strong>批归一化</strong>，一种常见的批归一化操作为：</p>
<ol>
<li>计算均值，再计算每个值与均值的差 </li>
<li>再计算均方差</li>
<li>将差除以均方差</li>
</ol>
<p>通过上面的操作，输入值将被归一化到一个较小的范围</p>
<p>接下来，我们对模型进行批归一化操作，与不进行批归一化的操作进行比较。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_model</span>():<br>    model = nn.Sequential(<br>        nn.Linear(<span class="hljs-number">28</span> * <span class="hljs-number">28</span>, <span class="hljs-number">1000</span>),<br>        <span class="hljs-comment"># 对隐藏层输入进行归一化操作</span><br>        nn.BatchNorm1d(<span class="hljs-number">1000</span>),<br>        nn.ReLU(),<br>        nn.Linear(<span class="hljs-number">1000</span>, <span class="hljs-number">10</span>)<br>    ).to(device)<br>    loss_fn = nn.CrossEntropyLoss()<br>    optimizer = Adam(model.parameters(), lr=<span class="hljs-number">1e-2</span>)<br>    <span class="hljs-keyword">return</span> model, loss_fn, optimizer<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/15%E4%BD%BF%E7%94%A8%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E6%95%88%E6%9E%9C.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/15%E4%BD%BF%E7%94%A8%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E6%95%88%E6%9E%9C.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240407181945432"></p>
<p>相比之下，没有进行批归一化的曲线如下：</p>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/16%E6%B2%A1%E6%9C%89%E4%BD%BF%E7%94%A8%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E6%95%88%E6%9E%9C.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/16%E6%B2%A1%E6%9C%89%E4%BD%BF%E7%94%A8%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E6%95%88%E6%9E%9C.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240407182414763"></p>
<p>很显然，批归一化能较好地提升模型训练效果</p>
<h1 id="过拟合的概念"><a href="#过拟合的概念" class="headerlink" title="过拟合的概念"></a>过拟合的概念</h1><ol>
<li>过拟合（Overfitting）： 过拟合指的是模型在训练数据上表现得过于优秀，但在未见过的测试数据上表现不佳的情况。具体来说，过拟合通常表现为模型对训练数据中的噪声和随机变化过于敏感，导致模型过度地“记忆”了训练数据的特性，而未能学习到泛化到新数据的规律。过拟合的原因通常是模型过于复杂，参数数量过多，导致模型的学习能力过强，容易过度拟合训练数据。</li>
<li>欠拟合（Underfitting）： 欠拟合指的是模型在训练数据和测试数据上的表现都不佳的情况。具体来说，欠拟合通常表现为模型不能很好地拟合训练数据中的真实关系，表现出的拟合程度不足，无法捕捉到数据的一般规律。欠拟合的原因通常是模型过于简单，学习能力不足，或者特征量不足，无法很好地描述数据的复杂性。</li>
<li>解决过拟合和欠拟合问题的方法包括：<ul>
<li>过拟合：增加训练数据量、简化模型复杂度（如减少参数数量、增加正则化）、使用更多的特征工程、使用集成学习方法等。</li>
<li>欠拟合：增加模型复杂度（如增加参数数量、增加模型的层数）、增加更多的特征、改进模型算法等。</li>
</ul>
</li>
</ol>
<h2 id="添加dropout的影响"><a href="#添加dropout的影响" class="headerlink" title="添加dropout的影响"></a>添加dropout的影响</h2><p>Dropout是一种解决过拟合问题的方法，比如，在每个训练批次中，通过忽略一半的特征检测器（让一半的隐层节点值为0），可以明显地减少过拟合现象。</p>
<blockquote>
<p>注意！在模型训练和模型验证的过程中，我们应该注意通过 model.train() 和 model.eval() 转换模型的模式，当使用model.eval() 将模型转化为评估模式，模型会抑制dropout的使用</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_model</span>():<br>    model = nn.Sequential(<br>        nn.Linear(<span class="hljs-number">28</span> * <span class="hljs-number">28</span>, <span class="hljs-number">1000</span>),<br>        nn.ReLU(),<br>        <span class="hljs-comment"># 随机屏蔽一半的节点</span><br>        nn.Dropout(<span class="hljs-number">0.5</span>),<br>        nn.Linear(<span class="hljs-number">1000</span>, <span class="hljs-number">10</span>)<br>    ).to(device)<br>    loss_fn = nn.CrossEntropyLoss()<br>    optimizer = SGD(model.parameters(), lr=<span class="hljs-number">1e-2</span>)<br>    <span class="hljs-keyword">return</span> model, loss_fn, optimizer<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/17%E4%BD%BF%E7%94%A8dropout%E7%9A%84%E6%95%88%E6%9E%9C.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/17%E4%BD%BF%E7%94%A8dropout%E7%9A%84%E6%95%88%E6%9E%9C.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240407183903783"></p>
<p>对比前面<code>没有进行批归一化的曲线</code>，可以发现训练集和验证集曲线更接近了，这证明dropout确实降低了过拟合。不过，理所当然，模型的训练速度也变慢了少许</p>
<h2 id="正则化的影响"><a href="#正则化的影响" class="headerlink" title="正则化的影响"></a>正则化的影响</h2><p>除了模型的训练准确度远高于验证准确度之外，过拟合的另一个特征是，某些权重值会远远高于其他权重值。而在探讨不同学习率对数据集的影响那一节，我们发现学习效果越好，其权重分布越接近正态分布。</p>
<p>正则化是一种基于对模型中高权重值进行惩罚的技术。</p>
<h3 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h3><p>L1正则化项结合交叉熵损失函数的计算公式如下：</p>
<ul>
<li>$L1loss &#x3D; -\frac{1}{n}(\sum_{i&#x3D;1}^{n}(y_ilog(p_i) + (1-y_i)log(1-p_i))) + \Lambda\sum_{j&#x3D;1}^{m}|w_j|$</li>
</ul>
<p>上述公式的前半部分正是二元交叉熵损失函数，后半部分中$\Lambda$表示一个权重。整体意思为：每次计算损失值时将损失值增加一部分，以减少较高权重出现的可能。</p>
<p>这次需要修改的地方在训练函数中，同时将训练轮次改到30轮：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_batch</span>(<span class="hljs-params">x, y, model, optimizer, loss_fn</span>):<br>    model.train() <br>    prediction = model(x)<br>    <br>    <span class="hljs-comment"># batch_loss = loss_fn(prediction, y)</span><br>    <span class="hljs-comment"># L1 正则化，计算</span><br>    l1_regularization = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> model.parameters():<br>        <span class="hljs-comment"># norm(parm, 1)表示对parm中所有值1次方（绝对值）再求和再开1次方</span><br>        <span class="hljs-comment"># 结果等同于绝对值之和</span><br>        l1_regularization += torch.norm(param, <span class="hljs-number">1</span>)<br>    <span class="hljs-comment"># 权重设为0.0001</span><br>    batch_loss = loss_fn(prediction, y) + <span class="hljs-number">0.0001</span>*l1_regularization<br>    <br>    batch_loss.backward()<br>    optimizer.step()<br>    optimizer.zero_grad()<br>    <span class="hljs-keyword">return</span> batch_loss.item()<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/18%E4%BD%BF%E7%94%A8L1%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E6%95%88%E6%9E%9C.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/18%E4%BD%BF%E7%94%A8L1%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E6%95%88%E6%9E%9C.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240407193126580"></p>
<blockquote>
<p>奇怪的是使用L1正则化并没有明显减小过拟合，即使其参数分布已经变成一条竖杠</p>
</blockquote>
<blockquote>
<p>更新，正则化减小过拟合效果不明显可能是迭代次数不够，在后续学习中发现当使用正则化能明显减小过拟合</p>
</blockquote>
<h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><p>L2正则化项结合交叉熵损失函数的计算公式如下：</p>
<ul>
<li>$L1loss &#x3D; -\frac{1}{n}(\sum_{i&#x3D;1}^{n}(y_ilog(p_i) + (1-y_i)log(1-p_i))) + \Lambda\sum_{j&#x3D;1}^{m}w_j^2$</li>
</ul>
<p>与L1的区别为，将元素的绝对值之和改为了平方和，因为使用了平方，所以权重也要相应的增加一些：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_batch</span>(<span class="hljs-params">x, y, model, optimizer, loss_fn</span>):<br>    model.train() <br>    prediction = model(x)<br>    <br>    <span class="hljs-comment"># batch_loss = loss_fn(prediction, y)</span><br>    <span class="hljs-comment"># L2 正则化，计算</span><br>    l2_regularization = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> model.parameters():<br>        <span class="hljs-comment"># norm(parm, 2)表示对parm中所有值2次方再求和再开2次方</span><br>        <span class="hljs-comment"># 结果等同于绝对值之和</span><br>        l2_regularization += torch.norm(param, <span class="hljs-number">2</span>)<br>    <span class="hljs-comment"># 权重设为0.01</span><br>    batch_loss = loss_fn(prediction, y) + <span class="hljs-number">0.01</span>*l2_regularization<br>    <br>    batch_loss.backward()<br>    optimizer.step()<br>    optimizer.zero_grad()<br>    <span class="hljs-keyword">return</span> batch_loss.item()<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/20%E4%BD%BF%E7%94%A8L2%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E6%95%88%E6%9E%9C.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/20%E4%BD%BF%E7%94%A8L2%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E6%95%88%E6%9E%9C.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240407200525539"></p>
<p>如果仔细观察，可以认为正则化对过拟合有一点点效果，但不明显，也许是我的其他参数选择不正确从而影响了结果。</p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title>03.使用pytorch构建深度神经网络（上）</title>
    <url>/Deep%20Learning/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/</url>
    <content><![CDATA[<p>[TOC]</p>
<h1 id="表示图像"><a href="#表示图像" class="headerlink" title="表示图像"></a>表示图像</h1><ul>
<li>图像文件是由像素组成的，像素是构成图像的最小元素</li>
<li>在灰度图像上，每个像素灰度取值为0-255，0表示黑，255表示白</li>
<li>彩色像素中每个像素是一个三位向量，每个分量分别表示rgb</li>
</ul>
<h2 id="将图像转化为结构化数组和标量"><a href="#将图像转化为结构化数组和标量" class="headerlink" title="将图像转化为结构化数组和标量"></a>将图像转化为结构化数组和标量</h2><p>Python中很容易将图像转化为数组，接下来使用cv2从磁盘读取图像，使用matplotlib将图像绘制出，原始图像如下<img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/01%E6%B5%8B%E8%AF%95%E5%9B%BE%E5%83%8F.jpeg" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/01%E6%B5%8B%E8%AF%95%E5%9B%BE%E5%83%8F.jpeg" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="Hemanvi.jpeg"></p>
<h3 id="灰度图像"><a href="#灰度图像" class="headerlink" title="灰度图像"></a>灰度图像</h3><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> cv2, matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-comment"># 使用 cv2.imread 方法，返回像素数组</span><br>img = cv2.imread(<span class="hljs-string">&#x27;test.jpeg&#x27;</span>)<br><span class="hljs-comment"># 数组切片，也可以理解为图像裁切</span><br>img = img[<span class="hljs-number">50</span>:<span class="hljs-number">250</span>,<span class="hljs-number">40</span>:<span class="hljs-number">240</span>]<br><span class="hljs-comment"># 将彩色图像转换为灰度图像（三通道变为单通道）</span><br>img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br>plt.imshow(img_gray, cmap=<span class="hljs-string">&#x27;gray&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/02%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/02%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406090502891"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 将图片压缩至 25 x 25 像素</span><br>img_gray_small = cv2.resize(img_gray, (<span class="hljs-number">25</span>, <span class="hljs-number">25</span>))<br>plt.imshow(img_gray_small, cmap=<span class="hljs-string">&#x27;gray&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/03%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F%E5%8E%8B%E7%BC%A9.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/03%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F%E5%8E%8B%E7%BC%A9.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406090601993"></p>
<h3 id="彩色图像"><a href="#彩色图像" class="headerlink" title="彩色图像"></a>彩色图像</h3><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 将颜色通道从 BGR转为RGB，使图像能正常显示</span><br>img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)<br>plt.imshow(img)<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/04%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/04%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406093836583"></p>
<h3 id="问题记录"><a href="#问题记录" class="headerlink" title="问题记录"></a>问题记录</h3><p>opencv 编码问题不能识别中文路径，可以使用下面两个方法解决</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 读取图像，解决imread不能读取中文路径的问题</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">cv_imread</span>(<span class="hljs-params">path</span>):<br>    <span class="hljs-comment"># 通过np将文件读为一维的二进制数组</span><br>    byte_img = np.fromfile(path, dtype=np.uint8)<br>    <span class="hljs-comment"># 再通过cv解码为二维的图像数组</span><br>    cv_img = cv2.imdecode(byte_img, -<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> cv_img<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">cv_img_rgb</span>(<span class="hljs-params">path</span>):<br>    <span class="hljs-comment"># 用matplotlib读取图像数组</span><br>    img = plt.imread(path)<br>    <span class="hljs-comment"># 转为cv2数组，因为opencv读取是按照BGR的顺序，所以这里需要调换顺序</span><br>    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)<br>    <span class="hljs-keyword">return</span> img_rgb<br></code></pre></td></tr></table></figure>

<h1 id="为什么要用神经网络进行图像分析"><a href="#为什么要用神经网络进行图像分析" class="headerlink" title="为什么要用神经网络进行图像分析"></a>为什么要用神经网络进行图像分析</h1><p>在图像分析中，如果不使用神经网络，也可以利用传统的计算机视觉技术来提取各种特征。这些特征通常是基于图像的局部结构、纹理、颜色等属性而提取出来的。以下是一些常见的特征：</p>
<ol>
<li><strong>颜色特征：</strong> 提取图像中的颜色信息，包括颜色直方图、颜色均值、颜色矩等。</li>
<li><strong>纹理特征：</strong> 描述图像中的纹理信息，包括灰度共生矩阵（GLCM）、局部二值模式（LBP）、方向梯度直方图（HOG）等。</li>
<li><strong>形状特征：</strong> 描述图像中的形状信息，包括边缘检测、轮廓提取、形状描述符等。</li>
<li><strong>局部特征：</strong> 描述图像中的局部结构信息，包括尺度不变特征变换（SIFT）、加速稳定特征变换（SURF）、尺度空间极值检测（DoG）等。</li>
<li><strong>密集特征：</strong> 提取图像中的密集特征点，包括稠密光流、稠密角点等。</li>
</ol>
<p>这些传统的特征提取方法通常需要设计特定的算法来实现，并且对图像的预处理、参数选择等方面有一定的依赖。尽管这些特征提取方法在一些场景下仍然具有一定的效果，但是相比于神经网络，它们通常需要更多的人工设计和调优，并且在处理复杂的图像任务时可能会受到限制。因此，随着深度学习和神经网络的发展，越来越多的图像分析任务倾向于使用神经网络来提取特征和解决问题。</p>
<h1 id="为图像分类准备数据"><a href="#为图像分类准备数据" class="headerlink" title="为图像分类准备数据"></a>为图像分类准备数据</h1><h2 id="FashionMNIST数据集"><a href="#FashionMNIST数据集" class="headerlink" title="FashionMNIST数据集"></a><strong>FashionMNIST数据集</strong></h2><ul>
<li>数据集：FashionMNIST 数据集是一个包含服装和配件图像的数据集，共有 60000 个训练样本和 10000 个测试样本。图像尺寸为 28x28 像素，灰度图像。</li>
<li>训练集标签：训练集标签是一个长度为 60000 的一维数组，每个元素表示对应图像的服装或配件类别，取值范围为 0 到 9，对应的类别包括 T-shirt&#x2F;top、Trouser、Pullover、Dress、Coat、Sandal、Shirt、Sneaker、Bag 和 Ankle boot。</li>
</ul>
<p>你可以在 TensorFlow、PyTorch 等深度学习框架中直接加载 FashionMNIST 数据集，并使用它来训练和评估模型。下面准备这个数据集：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-comment"># This can be any directory you want </span><br>data_folder = <span class="hljs-string">&#x27;dataset&#x27;</span><br><span class="hljs-comment"># 从文件夹加载训练集对象，download=True表示若文件不存在，则自动从网络下载，指定 train=True 来加载训练集，train=False 来加载测试集</span><br>fmnist_train = datasets.FashionMNIST(data_folder, download=<span class="hljs-literal">True</span>, train=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 获取训练集和测试集数据</span><br>train_data = fmnist_train.data  <span class="hljs-comment"># 训练集图像数据 shape([60000, 28, 28])</span><br>train_labels = fmnist_train.targets  <span class="hljs-comment"># 训练集标签 shape([60000])</span><br></code></pre></td></tr></table></figure>

<p>FashionMNIST 数据集对象通常还具有其他一些有用的属性和方法，例如：</p>
<ol>
<li><strong><code>fmnist_train.transform</code>：</strong> 该属性存储了应用于数据集中图像的转换方法。在训练模型时，通常会对图像数据进行预处理和增强，例如归一化、随机翻转等操作。<code>transform</code> 属性可以指定这些预处理和增强操作。</li>
<li><strong><code>fmnist_train.classes</code></strong> 是一个包含数据集中所有类别标签的列表，每个元素对应一个类别标签。</li>
<li><strong><code>fmnist_train.dataset</code>：</strong> 该属性存储了数据集的原始数据。通常情况下，你不需要直接使用这个属性，而是通过 <code>fmnist_train.data</code> 和 <code>fmnist_train.targets</code> 来访问图像数据和标签。</li>
<li><strong><code>fmnist_train.loader</code>：</strong> 该属性存储了数据集的数据加载器。数据加载器用于在训练模型时批量加载数据，并提供了数据的迭代器接口，方便训练模型。</li>
</ol>
<h2 id="使用数据集中的数据"><a href="#使用数据集中的数据" class="headerlink" title="使用数据集中的数据"></a>使用数据集中的数据</h2><p>从所有的10个类别中分别随机拿出10个样本：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>R, C = <span class="hljs-built_in">len</span>(train_labels.unique()), <span class="hljs-number">10</span><br><span class="hljs-comment"># 创建一个 RxC 的子图网格，返回一个包含所有子图的 Figure 对象和 Axes 对象的数组。</span><br>fig, ax = plt.subplots(R, C, figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>))<br><span class="hljs-comment"># 使用 Axes 对象来绘制每个子图的内容</span><br><span class="hljs-keyword">for</span> label_class, plot_row <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(ax):<br>    <span class="hljs-comment"># 筛选train_labels中标签为指定标签元素的索引位置</span><br>    label_x_rows = np.where(train_labels == label_class)[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">for</span> plot_cell <span class="hljs-keyword">in</span> plot_row:<br>        plot_cell.grid(<span class="hljs-literal">False</span>); plot_cell.axis(<span class="hljs-string">&#x27;off&#x27;</span>)<br>        <span class="hljs-comment"># 随机选择一个索引</span><br>        ix = np.random.choice(label_x_rows)<br>        <span class="hljs-comment"># 拿取图片（实际为一个 28 * 28 数组）</span><br>        x, y = train_data[ix], train_labels[ix]<br>        plot_cell.imshow(x, cmap=<span class="hljs-string">&#x27;gray&#x27;</span>)<br>plt.tight_layout()<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/05%E9%9A%8F%E6%9C%BA%E5%B1%95%E7%A4%BA100%E5%BC%A0%E6%A0%B7%E6%9C%AC.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/05%E9%9A%8F%E6%9C%BA%E5%B1%95%E7%A4%BA100%E5%BC%A0%E6%A0%B7%E6%9C%AC.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406113435044"></p>
<h1 id="训练神经网络"><a href="#训练神经网络" class="headerlink" title="训练神经网络"></a>训练神经网络</h1><p>前面的章节中我们已经学习过如何构建一个神经网络模型，现在我们又获得了准备好的数据集，接下来我们可以正式开始训练一个神经网络了，训练一个神经网络的步骤为：</p>
<ul>
<li><p>准备好环境，并导入相关程序包</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset, DataLoader<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span><br></code></pre></td></tr></table></figure>
</li>
<li><p>构建一个数据集，要求该数据集能一次获取一个数据点（能用于Dataset类）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python">data_folder = <span class="hljs-string">&#x27;dataset&#x27;</span><br>fmnist_train = datasets.FashionMNIST(data_folder, download=<span class="hljs-literal">True</span>, train=<span class="hljs-literal">True</span>)<br><br>train_data = fmnist_train.data<br>train_labels = fmnist_train.targets<br></code></pre></td></tr></table></figure>
</li>
<li><p>将DataLoader包装到数据集中以满足分批训练的需求</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FMNISTDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, y</span>):<br>        x = x.<span class="hljs-built_in">float</span>()<br>        x = x.view(-<span class="hljs-number">1</span>,<span class="hljs-number">28</span>*<span class="hljs-number">28</span>)<br>        self.x, self.y = x, y <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, ix</span>):<br>        x, y = self.x[ix], self.y[ix] <br>        <span class="hljs-keyword">return</span> x.to(device), y.to(device)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>): <br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.x)<br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_data</span>(): <br>    train = FMNISTDataset(train_data, train_labels) <br>    <span class="hljs-comment"># 指定批大小为 32；shuffle=True 表示对数据随机洗牌，用于训练阶段</span><br>    trn_dl = DataLoader(train, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">return</span> trn_dl<br><br>train_dataloader = get_data()<br><span class="hljs-comment"># for data in train_dataloader:</span><br></code></pre></td></tr></table></figure>
</li>
<li><p>构建一个训练模型，定义损失函数和优化器</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> SGD<br><br><span class="hljs-comment"># 自定义模型</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_model</span>():<br>    <span class="hljs-comment"># 使用Sequential设定模型结构（前向传播过程）</span><br>    model = nn.Sequential(<br>        <span class="hljs-comment"># 一个隐藏层，隐藏层包含1000个神经元</span><br>        nn.Linear(<span class="hljs-number">28</span> * <span class="hljs-number">28</span>, <span class="hljs-number">1000</span>),<br>        nn.ReLU(),<br>        <span class="hljs-comment"># 输出结果为长度等于10的张量</span><br>        nn.Linear(<span class="hljs-number">1000</span>, <span class="hljs-number">10</span>)<br>    ).to(device)<br>    <span class="hljs-comment"># 交叉熵损失函数</span><br>    loss_fn = nn.CrossEntropyLoss()<br>    <span class="hljs-comment"># 优化器，随机梯度下降算法，学习率为0.01</span><br>    optimizer = SGD(model.parameters(), lr=<span class="hljs-number">1e-2</span>)<br>    <span class="hljs-keyword">return</span> model, loss_fn, optimizer<br><br>model, loss_fn, optimizer = get_model()<br></code></pre></td></tr></table></figure>
</li>
<li><p>定义两个函数用来训练和验证数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_batch</span>(<span class="hljs-params">x, y, model, opt, loss_fn</span>):<br>    <span class="hljs-comment"># 设置模型为训练模式，会启用 Dropout 层</span><br>    model.train() <br>    <span class="hljs-comment"># 前向传播</span><br>    prediction = model(x)<br>    <span class="hljs-comment"># 计算损失值</span><br>    batch_loss = loss_fn(prediction, y)<br>    <span class="hljs-comment"># 反向传播</span><br>    batch_loss.backward()<br>    <span class="hljs-comment"># 更新权重</span><br>    optimizer.step()<br>    <span class="hljs-comment"># 梯度值置零以准备下一轮</span><br>    optimizer.zero_grad()<br>    <span class="hljs-keyword">return</span> batch_loss.item()<br><br><span class="hljs-comment"># 不需要计算梯度</span><br><span class="hljs-meta">@torch.no_grad()</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">accuracy</span>(<span class="hljs-params">x, y, model</span>):<br>    <span class="hljs-comment"># 设置模型为评估模式</span><br>    model.<span class="hljs-built_in">eval</span>() <br>    prediction = model(x)<br>    <span class="hljs-comment"># argmaxes为最大值所在的索引</span><br>    max_values, argmaxes = prediction.<span class="hljs-built_in">max</span>(-<span class="hljs-number">1</span>)<br>	<span class="hljs-comment"># 将预测的类别索引与真实标签进行比较，得到一个布尔型张量，表示每个样本的预测结果是否正确。</span><br>    is_correct = argmaxes == y<br>    <span class="hljs-keyword">return</span> is_correct.cpu().numpy().tolist()<br></code></pre></td></tr></table></figure>
</li>
<li><p>记录每轮的损失值和精确度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 记录准确度和损失值</span><br>losses, accuracies = [], []<br><span class="hljs-comment"># 定义轮数</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>    <span class="hljs-built_in">print</span>(epoch)<br>    <span class="hljs-comment"># 记录本轮损失值和准确度</span><br>    epoch_losses, epoch_accuracies = [], []<br>	<span class="hljs-comment"># 迭代数据加载器</span><br>    <span class="hljs-keyword">for</span> ix, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">iter</span>(train_dataloader)):<br>        x, y = batch<br>        batch_loss = train_batch(x, y, model, optimizer, loss_fn)<br>        epoch_losses.append(batch_loss)<br>    <span class="hljs-comment"># 训练完一轮，计算本轮平均损失值</span><br>    epoch_loss = np.array(epoch_losses).mean()<br>    <span class="hljs-comment"># 再来一轮计算准确度</span><br>    <span class="hljs-keyword">for</span> ix, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">iter</span>(train_dataloader)):<br>        x, y = batch<br>        is_correct = accuracy(x, y, model)<br>        epoch_accuracies.extend(is_correct)<br>    <span class="hljs-comment"># 计算这一轮平均准确度</span><br>    epoch_accuracy = np.mean(epoch_accuracies)<br>    losses.append(epoch_loss)<br>    accuracies.append(epoch_accuracy)<br></code></pre></td></tr></table></figure>
</li>
<li><p>随着轮数增加，根据每批数据的训练情况调整权重</p>
</li>
</ul>
<p>使用以下代码显示关于轮数的曲线变化</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python">epochs = np.arange(<span class="hljs-number">5</span>)+<span class="hljs-number">1</span><br>plt.figure(figsize=(<span class="hljs-number">20</span>,<span class="hljs-number">5</span>))<br>plt.subplot(<span class="hljs-number">121</span>)<br>plt.title(<span class="hljs-string">&#x27;Loss value over increasing epochs&#x27;</span>)<br>plt.plot(epochs, losses, label=<span class="hljs-string">&#x27;Training Loss&#x27;</span>)<br>plt.legend()<br>plt.subplot(<span class="hljs-number">122</span>)<br>plt.title(<span class="hljs-string">&#x27;Accuracy value over increasing epochs&#x27;</span>)<br>plt.plot(epochs, accuracies, label=<span class="hljs-string">&#x27;Training Accuracy&#x27;</span>)<br>plt.gca().set_yticklabels([<span class="hljs-string">&#x27;&#123;:.0f&#125;%&#x27;</span>.<span class="hljs-built_in">format</span>(x*<span class="hljs-number">100</span>) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> plt.gca().get_yticks()]) <br>plt.legend()<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/06%E6%8D%9F%E5%A4%B1%E6%9B%B2%E7%BA%BF.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/06%E6%8D%9F%E5%A4%B1%E6%9B%B2%E7%BA%BF.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406155529253"></p>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/07%E5%87%86%E7%A1%AE%E5%BA%A6.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/07%E5%87%86%E7%A1%AE%E5%BA%A6.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406155542459"></p>
<p>可以发现，五轮训练后准确度依然维持在一个较低的水平，而损失值的变化早已趋近平滑。换句话说，无论再进行多少轮训练，模型都不太可能达到一个理想的准确度，接下来就需要我们进行对各种超参数的调整来完善该模型。</p>
<h1 id="归一化处理以提升模型准确度"><a href="#归一化处理以提升模型准确度" class="headerlink" title="归一化处理以提升模型准确度"></a>归一化处理以提升模型准确度</h1><p>归一化处理是确保变量被限制在某个有限范围内的过程，通常是0到1或者-1到1之间。在上面的数据集中，我们可以通过对所有自变量的值除以已知的最大值——255，将自变量的值限制在 0-1 之间。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 修改 FMNISTDataset 类，获取自变量时将所有像素点除以255</span><br><span class="hljs-keyword">class</span>  <span class="hljs-title class_">FMNISTDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, y</span>):<br>        <span class="hljs-comment"># x = x.float()</span><br>        x = x.<span class="hljs-built_in">float</span>() / <span class="hljs-number">255</span> <span class="hljs-comment"># 缩放像素点，保证值在 0-1 之间</span><br>        x = x.view(-<span class="hljs-number">1</span>,<span class="hljs-number">28</span>*<span class="hljs-number">28</span>)<br>        self.x, self.y = x, y <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, ix</span>):<br>        x, y = self.x[ix], self.y[ix] <br>        <span class="hljs-keyword">return</span> x.to(device), y.to(device)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>): <br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.x)<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/08%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E6%8D%9F%E5%A4%B1%E6%9B%B2%E7%BA%BF.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/08%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E6%8D%9F%E5%A4%B1%E6%9B%B2%E7%BA%BF.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406162038810"></p>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/09%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E7%A1%AE%E5%BA%A6.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/09%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E7%A1%AE%E5%BA%A6.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406162047439"></p>
<p>在通过对数据集进行缩放后，损失值的减小放缓，同时准确度也达到了一个相对理想的状态。</p>
<p>归一化处理在深度学习中具有多方面的好处，下面我将详细介绍其主要优点：</p>
<ol>
<li><strong>加速模型收敛速度：</strong> 归一化处理使得输入数据的数值范围被缩放到一个较小的区间内，这有助于避免梯度消失或梯度爆炸问题，从而加速模型的收敛速度。当输入数据的尺度较小时，梯度下降的更新步长也会更加合适，模型参数可以更快地收敛到最优解。</li>
<li><strong>提高模型泛化能力：</strong> 归一化处理有助于减少不同特征之间的尺度差异，使得模型更加关注数据中重要的模式和特征。这有助于提高模型的泛化能力，使其在未见过的数据上表现更好，减少过拟合的风险。</li>
<li><strong>增加模型的稳定性：</strong> 归一化处理可以减少不同批次数据之间的方差，使得模型在训练过程中更加稳定。这可以降低训练过程中参数更新的波动，防止模型陷入局部最优解。</li>
<li><strong>提高梯度下降的效率：</strong> 归一化处理可以使得优化算法更加高效，因为它能够更快地找到全局最优解。当输入数据的尺度被缩放到一个合适的范围后，梯度下降算法可以更快地收敛到最优解，从而提高了训练的效率。</li>
<li><strong>抵抗不同输入分布的影响：</strong> 归一化处理使得模型更加鲁棒，能够适应不同的输入数据分布。无论是标准正态分布还是其他分布，归一化处理都能够使得模型更容易学习到数据的模式，提高了模型的通用性。</li>
</ol>
<h1 id="理解不同批大小的影响"><a href="#理解不同批大小的影响" class="headerlink" title="理解不同批大小的影响"></a>理解不同批大小的影响</h1><p>在一批中会更新一个权重，前面的代码中每轮更新了 60000 &#x2F; 32 次权重，下面我们来通过修改批大小来展现不同批大小时对训练效果的影响。</p>
<p>不过在此之前，我们需要使用验证数据来观察模型对未知数据的准确度，以下对验证模型准确度相关的代码进行修改：</p>
<ol>
<li><p>使用验证数据集验证而不是原始训练集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 使用验证集</span><br>fmnist_validation = datasets.FashionMNIST(data_folder, download=<span class="hljs-literal">True</span>, train=<span class="hljs-literal">False</span>)<br><br>validation_data = fmnist_validation.data<br>validation_labels = fmnist_validation.targets<br></code></pre></td></tr></table></figure>
</li>
<li><p>获取验证数据集时不进行洗牌，并且一次加载完所有数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_data</span>(): <br>    train = FMNISTDataset(train_data, train_labels)<br>    <span class="hljs-comment"># shuffle=True 表示对数据随机洗牌，用于训练阶段</span><br>    trn_dl = DataLoader(train, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-literal">True</span>)<br>    <br>    validation = FMNISTDataset(validation_data, validation_labels)<br>    <span class="hljs-comment"># 指定批大小为数据集长度 shuffle=False 表示不进行洗牌，用于验证阶段</span><br>    val_dl = DataLoader(validation, batch_size=<span class="hljs-built_in">len</span>(validation_data), shuffle=<span class="hljs-literal">False</span>)<br>    <span class="hljs-keyword">return</span> trn_dl, val_dl<br><br>train_dataloader, validation_dataloader = get_data()<br><span class="hljs-comment"># for data in train_dataloader:</span><br><span class="hljs-comment"># for data in validation_dataloader:</span><br></code></pre></td></tr></table></figure>
</li>
<li><p>给验证集一个新的损失值计算函数，因为训练集中的损失值由模型自动计算</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 验证集需要单独的损失值计算</span><br><span class="hljs-meta">@torch.no_grad()</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">val_loss</span>(<span class="hljs-params">x, y, model</span>):<br>    model.<span class="hljs-built_in">eval</span>()<br>    prediction = model(x)<br>    val_loss = loss_fn(prediction, y)<br>    <span class="hljs-keyword">return</span> val_loss.item()<br></code></pre></td></tr></table></figure>
</li>
<li><p>将上面的更新应用于循环逻辑中</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 记录验证集数据</span><br>validation_losses, validation_accuracies = [], []<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>    <span class="hljs-built_in">print</span>(epoch)<br>    epoch_losses, epoch_accuracies = [], []<br>    <span class="hljs-keyword">for</span> ix, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">iter</span>(train_dataloader)):<br>        x, y = batch<br>        batch_loss = train_batch(x, y, model, optimizer, loss_fn)<br>        epoch_losses.append(batch_loss)<br>    epoch_loss = np.array(epoch_losses).mean()<br>    <span class="hljs-keyword">for</span> ix, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">iter</span>(train_dataloader)):<br>        x, y = batch<br>        is_correct = accuracy(x, y, model)<br>        epoch_accuracies.extend(is_correct)<br>    epoch_accuracy = np.mean(epoch_accuracies)<br>    losses.append(epoch_loss)<br>    accuracies.append(epoch_accuracy)<br>    <br>    <span class="hljs-comment"># 每轮结束再对验证集计算准确度和损失值</span><br>    <span class="hljs-keyword">for</span> ix, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">iter</span>(validation_dataloader)):<br>        x, y = batch<br>        val_is_correct = accuracy(x, y, model)<br>        validation_loss = val_loss(x, y, model)<br>        val_epoch_accuracy = np.mean(val_is_correct)<br>        validation_losses.append(validation_loss)<br>        validation_accuracies.append(val_epoch_accuracy)<br></code></pre></td></tr></table></figure></li>
</ol>
<p>下面是训练结果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python">epochs = np.arange(<span class="hljs-number">5</span>)+<span class="hljs-number">1</span><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> matplotlib.ticker <span class="hljs-keyword">as</span> mticker<br>%matplotlib inline<br>plt.subplot(<span class="hljs-number">211</span>)<br>plt.plot(epochs, losses, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training loss&#x27;</span>)<br>plt.plot(epochs, validation_losses, <span class="hljs-string">&#x27;r&#x27;</span>, label=<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(<span class="hljs-number">1</span>))<br>plt.title(<span class="hljs-string">&#x27;Training and validation loss when batch size is 32&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Loss&#x27;</span>)<br>plt.legend()<br>plt.grid(<span class="hljs-string">&#x27;off&#x27;</span>)<br>plt.show()<br>plt.subplot(<span class="hljs-number">212</span>)<br>plt.plot(epochs, accuracies, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training accuracy&#x27;</span>)<br>plt.plot(epochs, validation_accuracies, <span class="hljs-string">&#x27;r&#x27;</span>, label=<span class="hljs-string">&#x27;Validation accuracy&#x27;</span>)<br>plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(<span class="hljs-number">1</span>))<br>plt.title(<span class="hljs-string">&#x27;Training and validation accuracy when batch size is 32&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Accuracy&#x27;</span>)<br>plt.gca().set_yticks(plt.gca().get_yticks())<br>plt.gca().set_yticklabels([<span class="hljs-string">&#x27;&#123;:.0f&#125;%&#x27;</span>.<span class="hljs-built_in">format</span>(x*<span class="hljs-number">100</span>) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> plt.gca().get_yticks()]) <br>plt.legend()<br>plt.grid(<span class="hljs-string">&#x27;off&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/10%E6%89%B9%E5%A4%A7%E5%B0%8F32%E7%9A%84%E6%8D%9F%E5%A4%B1%E6%9B%B2%E7%BA%BF.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/10%E6%89%B9%E5%A4%A7%E5%B0%8F32%E7%9A%84%E6%8D%9F%E5%A4%B1%E6%9B%B2%E7%BA%BF.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406181505934"></p>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/11%E6%89%B9%E5%A4%A7%E5%B0%8F32%E7%9A%84%E5%87%86%E7%A1%AE%E5%BA%A6%E6%9B%B2%E7%BA%BF.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/11%E6%89%B9%E5%A4%A7%E5%B0%8F32%E7%9A%84%E5%87%86%E7%A1%AE%E5%BA%A6%E6%9B%B2%E7%BA%BF.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406181518383"></p>
<h2 id="批大小改为10000"><a href="#批大小改为10000" class="headerlink" title="批大小改为10000"></a>批大小改为10000</h2><p>只需修改以下函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_data</span>(): <br>    train = FMNISTDataset(train_data, train_labels)<br>    <span class="hljs-comment"># 将批大小改为10000</span><br>    trn_dl = DataLoader(train, batch_size=<span class="hljs-number">10000</span>, shuffle=<span class="hljs-literal">True</span>)<br>    <br>    validation = FMNISTDataset(validation_data, validation_labels)<br>    val_dl = DataLoader(validation, batch_size=<span class="hljs-built_in">len</span>(validation_data), shuffle=<span class="hljs-literal">False</span>)<br>    <span class="hljs-keyword">return</span> trn_dl, val_dl<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/12%E6%89%B9%E5%A4%A7%E5%B0%8F10000%E7%9A%84%E6%8D%9F%E5%A4%B1%E6%9B%B2%E7%BA%BF.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/12%E6%89%B9%E5%A4%A7%E5%B0%8F10000%E7%9A%84%E6%8D%9F%E5%A4%B1%E6%9B%B2%E7%BA%BF.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406181918016"></p>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/13%E6%89%B9%E5%A4%A7%E5%B0%8F10000%E7%9A%84%E5%87%86%E7%A1%AE%E5%BA%A6%E6%9B%B2%E7%BA%BF.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/13%E6%89%B9%E5%A4%A7%E5%B0%8F10000%E7%9A%84%E5%87%86%E7%A1%AE%E5%BA%A6%E6%9B%B2%E7%BA%BF.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406181946292"></p>
<p>可以发现批大小为10000时准确度远远低于批大小为32时的准确度，这是因为批大小32时每轮进行了1875次权重更新，而批大小10000时每轮只能进行6次权重更新。</p>
<h1 id="理解不同损失优化器的影响"><a href="#理解不同损失优化器的影响" class="headerlink" title="理解不同损失优化器的影响"></a>理解不同损失优化器的影响</h1><p>在此前，我们一直使用<strong>随机梯度下降法（Stochastic Gradient Descent, SGD）</strong>作为优化方法，在本节，我们将使用亚当优化器或者说自适应矩估计算法（Adaptive Moment Estimation, Adam） 作为优化算法，测试不同优化器的影响，我们接下来需要：</p>
<ol>
<li><p>导入并修改优化器为 Adam</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> SGD, Adam<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_model</span>():<br>    model = nn.Sequential(<br>        nn.Linear(<span class="hljs-number">28</span> * <span class="hljs-number">28</span>, <span class="hljs-number">1000</span>),<br>        nn.ReLU(),<br>        nn.Linear(<span class="hljs-number">1000</span>, <span class="hljs-number">10</span>)<br>    ).to(device)<br>    loss_fn = nn.CrossEntropyLoss()<br>    <span class="hljs-comment"># 使用Adam优化器</span><br>    optimizer = Adam(model.parameters(), lr=<span class="hljs-number">1e-2</span>)<br>    <span class="hljs-keyword">return</span> model, loss_fn, optimizer<br></code></pre></td></tr></table></figure>
</li>
<li><p>将训练时的批大小恢复为32</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_data</span>(): <br>    train = FMNISTDataset(train_data, train_labels)<br>    <span class="hljs-comment"># 恢复批大小为32</span><br>    trn_dl = DataLoader(train, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-literal">True</span>)<br>    <br>    validation = FMNISTDataset(validation_data, validation_labels)<br>    val_dl = DataLoader(validation, batch_size=<span class="hljs-built_in">len</span>(validation_data), shuffle=<span class="hljs-literal">False</span>)<br>    <span class="hljs-keyword">return</span> trn_dl, val_dl<br></code></pre></td></tr></table></figure>
</li>
<li><p>增加轮数到10以更好地比较</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>	...<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/01%E4%BD%BF%E7%94%A8SGD%E4%BC%98%E5%8C%96%E5%99%A8.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/01%E4%BD%BF%E7%94%A8SGD%E4%BC%98%E5%8C%96%E5%99%A8.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406190505255"></p>
<p><img src="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/02%E4%BD%BF%E7%94%A8%E4%BA%9A%E5%BD%93%E4%BC%98%E5%8C%96%E5%99%A8.png" class="lazy" data-srcset="/pictures/03.%E4%BD%BF%E7%94%A8pytorch%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89/02%E4%BD%BF%E7%94%A8%E4%BA%9A%E5%BD%93%E4%BC%98%E5%8C%96%E5%99%A8.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240406190116099"></p>
</li>
</ol>
<p>可以发现，虽然Adam优化器在一开始就取得较好的成绩，但在后面的几轮训练中波动较大，似乎不如SGD优化器稳定</p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title>04.卷积神经网络（下）</title>
    <url>/Deep%20Learning/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/</url>
    <content><![CDATA[<h2 id="进一步理解滤波器"><a href="#进一步理解滤波器" class="headerlink" title="进一步理解滤波器"></a>进一步理解滤波器</h2><p>前面提到：<strong>通过多个不同滤波器的组合，卷积层可以提取出输入图像的不同特征，例如边缘、纹理、形状等</strong>。</p>
<h3 id="图像处理时用的滤波器"><a href="#图像处理时用的滤波器" class="headerlink" title="图像处理时用的滤波器"></a>图像处理时用的滤波器</h3><p>滤波器为什么能提取出图像的信息呢？这个问题我们可以从传统的图像处理入手。下面介绍一些常用的滤波器以理解这个过程，需要注意的一个细节是图像使用滤波器会在图像四周填充一些值（一般为0）以保持卷积后的图像大小不变化。</p>
<h4 id="平滑"><a href="#平滑" class="headerlink" title="平滑"></a>平滑</h4><ol>
<li><p><strong>均值滤波</strong>：以核大小3*3为例，$\begin{bmatrix} 1 &amp; 1 &amp; 1 \ 1 &amp; 1 &amp; 1 \ 1 &amp; 1 &amp; 1 \end{bmatrix}$，使像素获得与其相邻像素的值。在图像的平滑区域、基本不会改变像素值，但在像素差距较大的边缘区域可以使像素之间差距减小。</p>
</li>
<li><p><strong>高斯滤波</strong>：一个可能的核为$\begin{bmatrix} 3 &amp; 5 &amp; 3 \ 5 &amp; 10 &amp; 5 \ 3 &amp; 5 &amp; 3 \end{bmatrix}$。高斯滤波采用了正态分布的思想，越靠近中心的像素权重越高。相比均值滤波，高斯滤波在平滑的同时能较好的保存图像细节。</p>
</li>
<li><p><strong>双边滤波</strong>：核需要根据灰度值实时计算。双边滤波在高斯滤波考虑距离的同时，还考虑其余像素和中心像素之间的灰度值差，灰度值相差越大，权重越小。这使双边滤波在去除小噪声的同时，还能较好的保留图像的边缘细节，但运算速度较慢。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py">blurred = cv2.blur(image, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>))<br><span class="hljs-comment"># cv2.GaussianBlur(src, ksize, sigmaX[, dst[, sigmaY[, borderType]]])， sigmaX表示水平方向的标准差，0表示自动计算</span><br>blurred = cv2.GaussianBlur(image, (<span class="hljs-number">5</span>, <span class="hljs-number">5</span>), <span class="hljs-number">0</span>)<br><span class="hljs-comment"># cv2.bilateralFilter(src, d, sigmaColor, sigmaSpace[, dst[, borderType]])，d表示核边长，其余两位分别表示颜色和空间的标准差</span><br>blurred = cv2.bilateralFilter(image, <span class="hljs-number">9</span>, <span class="hljs-number">60</span>, <span class="hljs-number">60</span>)<br></code></pre></td></tr></table></figure></li>
</ol>
<p>下图按顺序依次为原图、均值滤波、高斯滤波和双边滤波。</p>
<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/01%E5%B9%B3%E6%BB%91%E6%BB%A4%E6%B3%A2%E5%AF%B9%E6%AF%94.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/01%E5%B9%B3%E6%BB%91%E6%BB%A4%E6%B3%A2%E5%AF%B9%E6%AF%94.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="平滑滤波对比"></p>
<h4 id="边缘检测"><a href="#边缘检测" class="headerlink" title="边缘检测"></a>边缘检测</h4><ol>
<li><p><strong>Sobel</strong>：一种效率很高的检测算法，结合了微分与高斯平滑的思想，由导数的定义可知，导数大小能反映图像变化的剧烈程度，由此实现边缘检测。在水平方向上，当前点的导数取决于前后列的变化量，记x的变化量dx为1，则$ f’(x) &#x3D; f(x+1) - f(x) &#x3D; f(x) - f(x-1) &#x3D; \frac{f(x+1) - f(x-1)}{2}$。这是Sobel算法检测水平方向上的边缘时的示例$\begin{bmatrix} -1 &amp; 0 &amp; +1 \ -2 &amp; 0 &amp; +2 \ -1 &amp; 0 &amp; +1 \end{bmatrix}$。当图像在水平方向有明显的变化时，导数值较大，当前像素便会被高亮，反之图像平坦时导数趋近于0，图像便为黑色，由此实现水平方向上的边缘检测。下图为Sobel边缘检测实例，下图依次为原图、x、y和xy</p>
<figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># Sobel(src: cv2.typing.MatLike, ddepth: int, dx: int, dy: int)</span><br>image = cv2.imread(file_path)<br>X = cv2.Sobel(image, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)<br>Y = cv2.Sobel(image, -<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>XY = cv2.Sobel(image, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/02Sobel%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/02Sobel%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240416143655504"></p>
</li>
<li><p>其他边缘检测算法还包括 <strong>Laplacian</strong>算法、<strong>canny</strong>算法等，限于篇幅这里就不展开了</p>
</li>
</ol>
<h3 id="滤波器在卷积神经网络的效果"><a href="#滤波器在卷积神经网络的效果" class="headerlink" title="滤波器在卷积神经网络的效果"></a>滤波器在卷积神经网络的效果</h3><p>接下来我们通过实践来深入的理解滤波器在神经网络中的效果。</p>
<ol>
<li><p>随便选一张图片，看看其原本的样子：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># 按照训练时的处理方法处理图像</span><br>idx = <span class="hljs-number">8</span><br>img = train_data[idx] / <span class="hljs-number">255</span><br><br>plt.imshow(img.cpu().numpy(), cmap=<span class="hljs-string">&#x27;gray&#x27;</span>)<br>plt.title(fmnist_train.classes[train_labels[idx]])<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/05%E9%9A%8F%E4%BE%BF%E9%80%89%E5%8F%96%E4%B8%80%E5%BC%A0%E5%9B%BE%E7%89%87.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/05%E9%9A%8F%E4%BE%BF%E9%80%89%E5%8F%96%E4%B8%80%E5%BC%A0%E5%9B%BE%E7%89%87.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240411102717597"></p>
</li>
<li><p>绘出得到的中间输出，可以看出，一些滤波器绘制出了图像的边缘，有些滤波器注重于亮度的关系，还有滤波器反转了图像：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py">model.<span class="hljs-built_in">eval</span>()<br><span class="hljs-comment"># 创建了一个新的Sequential层，其中只包含了模型的第一层。</span><br>first_layer = nn.Sequential(*<span class="hljs-built_in">list</span>(model.children())[:<span class="hljs-number">1</span>])<br><span class="hljs-comment"># 将一张图片通过这个第一层，得到中间输出</span><br>intermediate_output = first_layer(img[<span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, :, :].to(device))[<span class="hljs-number">0</span>].detach()<br><br>n = <span class="hljs-number">8</span><br><span class="hljs-comment"># 8 * 8</span><br>fig, ax = plt.subplots(n, n, figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>))<br><span class="hljs-keyword">for</span> ix, axis <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(ax.flat):<br>    axis.set_title(<span class="hljs-string">&#x27;Filter: &#x27;</span>+<span class="hljs-built_in">str</span>(ix))<br>    axis.imshow(intermediate_output[ix].cpu())<br>plt.tight_layout()<br>plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/06%E7%AC%AC%E4%B8%80%E5%B1%82%E6%BB%A4%E6%B3%A2%E5%99%A8%E7%9A%84%E8%BE%93%E5%87%BA.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/06%E7%AC%AC%E4%B8%80%E5%B1%82%E6%BB%A4%E6%B3%A2%E5%99%A8%E7%9A%84%E8%BE%93%E5%87%BA.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240411101448289"></p>
</li>
<li><p>接下来选择0号滤波器，研究该滤波器对其他图像的效果。可以看出，该滤波器加强了图像的边缘。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># 筛选凉鞋</span><br>ims = train_data[train_labels==<span class="hljs-number">5</span>].to(device)<br>n = <span class="hljs-number">4</span><br>fig, ax = plt.subplots(n, n, figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>))<br><span class="hljs-keyword">for</span> ix, axis <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(ax.flat):<br>    output = first_layer(ims[ix][<span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, :, :] / <span class="hljs-number">255</span>)[<span class="hljs-number">0</span>].detach()<br>    axis.grid(<span class="hljs-literal">False</span>)<br>    axis.set_title(<span class="hljs-string">&quot;image: &quot;</span> + <span class="hljs-built_in">str</span>(ix))<br>    axis.imshow(output[<span class="hljs-number">0</span>].cpu())<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/07%E7%89%B9%E5%AE%9A%E6%BB%A4%E6%B3%A2%E5%99%A8%E7%9A%84%E6%95%88%E6%9E%9C.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/07%E7%89%B9%E5%AE%9A%E6%BB%A4%E6%B3%A2%E5%99%A8%E7%9A%84%E6%95%88%E6%9E%9C.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240411101448289"></p>
</li>
<li><p>接下来再绘制这些图像再第二层滤波器输出的图像，第二层有128个输出，我只绘制出64个以便于与前面做对比。通过这次滤波器，图像变的较为抽象</p>
<figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># 模型的第四层是第二层滤波器</span><br>second_layer = nn.Sequential(*<span class="hljs-built_in">list</span>(model.children())[:<span class="hljs-number">4</span>])<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/08%E7%AC%AC%E4%BA%8C%E5%B1%82%E6%BB%A4%E6%B3%A2%E5%99%A8%E7%9A%84%E8%BE%93%E5%87%BA.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/08%E7%AC%AC%E4%BA%8C%E5%B1%82%E6%BB%A4%E6%B3%A2%E5%99%A8%E7%9A%84%E8%BE%93%E5%87%BA.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240411120540224"></p>
</li>
<li><p>看到第二层的3号滤波器的输出比较有特点，对其再试试上面的图像，结果如下。对于人类来说，基本已经无法判断其身为凉鞋的特征，要解释模型如何提取出特征还得继续向下探索</p>
<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/09%E7%AC%AC%E4%BA%8C%E5%B1%82%E6%BB%A4%E6%B3%A2%E5%99%A8%E7%9A%84%E6%95%88%E6%9E%9C.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/09%E7%AC%AC%E4%BA%8C%E5%B1%82%E6%BB%A4%E6%B3%A2%E5%99%A8%E7%9A%84%E6%95%88%E6%9E%9C.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240411121345591"></p>
</li>
<li><p>绘制扁平层的图像。这一次我们就能直观的看到发生了什么，这张图中每个点表示一个大于零的激活值。这些条纹可以视为卷积操作提取出的<strong>特征</strong></p>
<figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># 筛选所有label为x的样本，共6000个</span><br>ims = train_data[train_labels==<span class="hljs-number">5</span>].to(device) / <span class="hljs-number">255</span><br><span class="hljs-comment"># 为方便展示，这里将样本筛为 1000 个</span><br>ims = ims[:<span class="hljs-number">1000</span>]<br><span class="hljs-comment"># 扁平层的输出</span><br>flatten_layer = nn.Sequential(*<span class="hljs-built_in">list</span>(model.children())[:<span class="hljs-number">8</span>])<br><span class="hljs-comment"># 获取这1000个样本在扁平层的输出，返回结果shape ([1000, 3200])</span><br>flatten_layer_output = flatten_layer(ims[:, <span class="hljs-literal">None</span>, :, :].to(device)).detach()<br><span class="hljs-comment"># 指定绘制大小以便能看清楚</span><br>plt.figure(figsize=(<span class="hljs-number">20</span>,<span class="hljs-number">20</span>))<br>plt.grid(<span class="hljs-literal">False</span>)<br>plt.imshow(flatten_layer_output.cpu())<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/10%E6%8C%87%E5%AE%9A%E5%88%86%E7%B1%BB%E5%9C%A8%E6%89%81%E5%B9%B3%E5%B1%82%E7%9A%84%E8%BE%93%E5%87%BA.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/10%E6%8C%87%E5%AE%9A%E5%88%86%E7%B1%BB%E5%9C%A8%E6%89%81%E5%B9%B3%E5%B1%82%E7%9A%84%E8%BE%93%E5%87%BA.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240411124139162"></p>
</li>
<li><p>为了对比，再随机在取数据集取1000个图像，以展示不同分类样本在扁平层的输出。这次虽然也出现很多条纹，但其混乱程度（熵）明显大于上一张图。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># 训练集中样本已打乱，直接拿训练集的前1000个样本</span><br>ims = train_data[:<span class="hljs-number">1000</span>].to(device) / <span class="hljs-number">255</span><br></code></pre></td></tr></table></figure>

<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/11%E9%9A%8F%E6%9C%BA%E5%88%86%E7%B1%BB%E5%9C%A8%E6%89%81%E5%B9%B3%E5%B1%82%E7%9A%84%E8%BE%93%E5%87%BA.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/11%E9%9A%8F%E6%9C%BA%E5%88%86%E7%B1%BB%E5%9C%A8%E6%89%81%E5%B9%B3%E5%B1%82%E7%9A%84%E8%BE%93%E5%87%BA.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240411125400178"></p>
</li>
</ol>
<h3 id="迁移学习的概念"><a href="#迁移学习的概念" class="headerlink" title="迁移学习的概念"></a>迁移学习的概念</h3><p>通过前面这两张图，我相信你已经了解了卷积的真正作用，通过一系列的卷积操作，便能激活图像的各种特征。</p>
<p>于是，迁移学习的概念也就不难理解了，想象一下，一个训练好的模型是在一个有着上千种分类、数百万张图片的数据集上训练的。这意味着其提取特征的能力已经非常出色了，这时我们就可以直接使用该模型的卷积网络来提取特征，而不用自己去慢慢调整。</p>
<p>下面我们从迁移学习的角度看看卷积网络的作用。</p>
<h4 id="VGG16模型"><a href="#VGG16模型" class="headerlink" title="VGG16模型"></a>VGG16模型</h4><p><strong>VGG模型</strong>由牛津大学工程科学系视觉几何组（Visual Geometry Group, Department of Engineering Science, University of Oxford）提出，并在2014年的<strong>ImageNet</strong>竞赛中获得亚军（ImageNet是一个将大约1400万个图像分类为1000个不同类别的竞赛），相关论文<a href="https://arxiv.org/abs/1409.1556"> 《Very Deep Convolutional Networks for Large-Scale Image Recognition》</a>在2015年发布。VGG16中的16表示模型的层数。</p>
<ol>
<li><p>使用torch导入VGG16模型：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> models<br><br>device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span><br><span class="hljs-comment"># vgg16 模型已集成在PyTorch中，可以直接使用，其输入为224*224的彩色图像(3, 224, 224)，即shape(batach_size, 3, 224, 224)</span><br>models.vgg16().to(device)<br></code></pre></td></tr></table></figure>

<p>以下是模型的具体信息，可以看到模型中有三个模块，分别为features、avgpool和classifier：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py">VGG(<br>  (features): Sequential(<br>    (<span class="hljs-number">0</span>): Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">64</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">1</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">2</span>): Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">3</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">4</span>): MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, ceil_mode=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">5</span>): Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">6</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">7</span>): Conv2d(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">8</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">9</span>): MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, ceil_mode=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">10</span>): Conv2d(<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">11</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">12</span>): Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">256</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">13</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">14</span>): Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">256</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">15</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">16</span>): MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, ceil_mode=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">17</span>): Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">18</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">19</span>): Conv2d(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">20</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">21</span>): Conv2d(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">22</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">23</span>): MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, ceil_mode=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">24</span>): Conv2d(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">25</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">26</span>): Conv2d(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">27</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">28</span>): Conv2d(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">29</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">30</span>): MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, ceil_mode=<span class="hljs-literal">False</span>)<br>  )<br>  (avgpool): AdaptiveAvgPool2d(output_size=(<span class="hljs-number">7</span>, <span class="hljs-number">7</span>)) <span class="hljs-comment"># 自适应池化层，指定输出图像尺寸为7*7</span><br>  (classifier): Sequential(<br>    (<span class="hljs-number">0</span>): Linear(in_features=<span class="hljs-number">25088</span>, out_features=<span class="hljs-number">4096</span>, bias=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">1</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">2</span>): Dropout(p=<span class="hljs-number">0.5</span>, inplace=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">3</span>): Linear(in_features=<span class="hljs-number">4096</span>, out_features=<span class="hljs-number">4096</span>, bias=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">4</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">5</span>): Dropout(p=<span class="hljs-number">0.5</span>, inplace=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">6</span>): Linear(in_features=<span class="hljs-number">4096</span>, out_features=<span class="hljs-number">1000</span>, bias=<span class="hljs-literal">True</span>)<br>  )<br>)<br></code></pre></td></tr></table></figure>
</li>
<li><p>数据预处理，VGG16模型有特定的的数据预处理方式，我们需要将尺寸重置为244*244、将图像设置为三通道、再将其归一化为vgg16需要的格式</p>
<figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># 将图像调整为三通道</span><br><span class="hljs-comment"># train_images_rgb = np.repeat(train_data[:, :, :, np.newaxis], 3, axis=-1)</span><br><br>vgg16_preprocess = transforms.Compose([<br>    transforms.ToPILImage(),<br>    transforms.Resize(<span class="hljs-number">224</span>),  <span class="hljs-comment"># 调整图像大小到 224x224</span><br>    transforms.ToTensor(),  <span class="hljs-comment"># 转换为张量</span><br>    <span class="hljs-comment"># transforms.Lambda(lambda x: x.flip(0)),  # 交换颜色通道 (RGB to BGR)</span><br>    transforms.Normalize(mean=[<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>], std=[<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>]),  <span class="hljs-comment"># 归一化</span><br>]) <br></code></pre></td></tr></table></figure>
</li>
<li><p>下载VGG16模型的参数（参数文件约为550M），并指定使用其features模块</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 指定 weights=models.VGG16_Weights.DEFAULT，torch会自动寻找预训练好的权重</span><br>model = models.vgg16(weights=models.VGG16_Weights.DEFAULT, progress=<span class="hljs-literal">True</span>).to(device)<br>conv_layer = nn.Sequential(<br>    *<span class="hljs-built_in">list</span>(model.children())[:<span class="hljs-number">2</span>],<br>    nn.Flatten(), <span class="hljs-comment"># 展平数据，25088</span><br>)<br></code></pre></td></tr></table></figure>
</li>
<li><p>进行测试，为了更好的图像比例，这次我使用了6000条数据，并截取了扁平层的前20000个参数</p>
<figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># 指定train_labels=5的6000个样本</span><br>ims = train_data[train_labels==<span class="hljs-number">5</span>][:<span class="hljs-number">6000</span>] / <span class="hljs-number">255</span><br>rgb_ims = np.repeat(ims[:, :, :, np.newaxis], <span class="hljs-number">3</span>, axis=-<span class="hljs-number">1</span>)<br>conv_layer.to(device)<br><br><span class="hljs-comment"># 因为数据有点多，我们一个一个计算</span><br>result = []<br><span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> rgb_ims:<br>    processed_img = vgg16_preprocess(image.numpy())<br>    res = conv_layer(processed_img[<span class="hljs-literal">None</span>, :, :, :].to(device))<br>    result.append(torch.squeeze(res).detach().cpu()[:<span class="hljs-number">20000</span>])<br></code></pre></td></tr></table></figure>

<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/12vgg16%E6%8C%87%E5%AE%9A%E5%88%86%E7%B1%BB%E5%9C%A8%E6%89%81%E5%B9%B3%E5%B1%82%E7%9A%84%E6%95%88%E6%9E%9C.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/12vgg16%E6%8C%87%E5%AE%9A%E5%88%86%E7%B1%BB%E5%9C%A8%E6%89%81%E5%B9%B3%E5%B1%82%E7%9A%84%E6%95%88%E6%9E%9C.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240417220956432"></p>
<figure class="highlight py"><table><tr><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># 拿前6000条</span><br>ims = train_data[:<span class="hljs-number">6000</span>] / <span class="hljs-number">255</span><br></code></pre></td></tr></table></figure>

<p><img src="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/13vgg16%E6%A8%A1%E5%9E%8B%E5%9C%A8%E9%9A%8F%E6%9C%BA%E5%88%86%E7%B1%BB%E7%9A%84%E6%95%88%E6%9E%9C.png" class="lazy" data-srcset="/pictures/04.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89/13vgg16%E6%A8%A1%E5%9E%8B%E5%9C%A8%E9%9A%8F%E6%9C%BA%E5%88%86%E7%B1%BB%E7%9A%84%E6%95%88%E6%9E%9C.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20240417221238719"></p>
</li>
<li><p>结论</p>
<ul>
<li><p>上面这两张图乍一看很相似，但仔细观察，就会得到与之前相同的结论，多种类的图像混乱程度更高</p>
</li>
<li><p>两张图相似的原因是该数据库中的所有图像比较简单，通过vgg16模型的features模块处理后找出了这些图像的很多相同特征</p>
</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
</search>
